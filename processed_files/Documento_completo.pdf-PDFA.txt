generacin texto estructurado espaol batalla freestyle utilizar modelo deep learning dal bianco pedro alejandro dr ronchetti franco licenciaturar informtica presencia inteligencia artificial variedad rea tcnica aprendizaje automtico deep learning propsito creativo tambin aumentar significativamente ltimos ao tipo trabajo rea procesamiento lenguaje natural tpicamente tomar forma modelo neuronal generador ficcin lrica encontrndo él mayora caso idioma ingls lidad adaptar él fcilmente idioma trabajo desarroll sistema generador texto enmarcar estilo estructura subgnero rap conocido batalla freestyle adems puramente creativo artstico incluir factor competitivo ejecutant tesinar grado aprendizaje profundo procesamiento lenguaje natural generacin lenguaje natural generacin texto estructurado modelo lenguaje desarrollado logr efectivamente captar estilo gnero entrenamiento base dato generado as correcto estructuracin estrofa verso correspondiente rima go algoritmo garantizar texto generado cumplir caracterstica generar texto corresponder estilo tambin estructura correspondiente gnero batalla freestyle identificar posible trabajo futuro mentacin sistema concordancia gnero batalla freestyle capaz generar texto gnero tambin responder estrofa producido potencial contrincante profundizar utilizacin tcnica learning estudiar aplicabilidad problema desarrollado experimentar arquitectura red neuronal definir modelo lenguaje utilizar incrementar cantidad dato disponible entrenamiento recoleccin transcripción travs data augentation lugar definir base dato texto letra canción gnero rap similar compuesto cripción batalla freestyle exclusivamente encontrar momento base dato pblica dominio dato mentar distinto tcnica preprocesamiento representacin utilizar entrenar modelo lenguaje neuronal generacin texto corresponder gnero ltimo rroll sistema generacin freestyle incluir algoritmo trabajar resultado modelo lenguaje garantizar texto generado respetar estructura rima requerido gnero febrero generacion texto estructurado espanol batalla freestyle utilizar modelo deep learning director dr ronchetti franco alumno dal bianco pedro alejandro facultad informatica universidad nacional plata obtener grado licenciado informatica febrero indice general introduccion resumen motivacion objetivo organizacion documento deep learning modelo lineal clasicacion binario clasicacion multiclase funcion error entropa cruzada categorico descenso gradiente sobreajuste red neuronal función activacion entrenamiento red neuronal dropout transfer learning red neuronal recurrente rnns bidireccional arquitectura compuerta procesamiento lenguaje natural preprocesamiento tokenizacion byte pair encoding representacion documento representacion caracterstica estatica representacion caracterstica dinamica representacion caracterstica aprendido word embeddings obtencion vector embedding indice general modelo lenguaje generacion lenguaje natural modelo lenguaje modelo lenguaje neuronal ventana ja modelo lenguaje red neuronal recurrente generacion texto estructurado batalla freestyle metrica evaluacion trabajo relacionado desarrollo generacion base dato preprocesamiento analisis dato analisis vocabulario tokenizacion modelo generacion texto entrenamiento modelo lenguaje generacion texto estructurado organizacion texto estrofas generacion texto estructura rima algoritmo generacion conclusión trabajo futuro conclusión general linea trabajo futuro bibliografa muestra texto generado captulo introduccion resumir presencia inteligencia articial variedad area tecnica aprendizaje automatico deep learning proposito creativo aumentar signicativamente ultir ano tipo trabajo area procesamiento lenguaje natural tpicamente tomar forma modelo neuronal dor ccion lrica encontrar él mayora caso idioma ingl posibilidad adaptar él facilmente idioma trabajo desarrollo sistema generador texto enmarcar estilo estructura subgenero rap conocido batalla freestyle componente puramente creativo artstico incluir factor competitivo ejecutant totalidad desarrollo consistio lugar generacion base dato compuesto transcripción batalla freestyle encontrar momento realizacion trabajo base dato publicar contener tipo material contener letra canción genero similar denicion modelo lenguaje neuronal entrenar texto contenido base dato nalmente algoritmos utilizar modelo entrenado generacion texto corresponder estilo buscado respetar rima metrica correspondiente modelo utilizado mostro capaz captar estilo genero particular incluido particularidad utilizacion expresión ingl vocab él caracterstico conjunto algoritmo generacion desarrollado lograr generar texto realista espanol acorde genero mencionado respetar estructura estrofas rima motivacion procesamiento lenguaje natural especcamente generacion lenguaje natural area resultar inter tiempo principio tecnica capitulo introducci on utilizado plantilla templat similar surgimiento tecnica deep dearning aplicado campo particular permitir avance actual arte deng and liu disponibilidad base dato permitir generacion modelo robusto respecta generacion lenguaje natural brown et modelo reciente entrenado version base dato common constar billon palabra ejemplo fenomeno prueba artculo original obtener resultado campo generacion texto entrenar base dato mencionado realizar texto especco estilo buscar generar ejemplo branwen utilizar modelo generacion distinto genero particular poesa dialogo texto humorstico generacion texto estructurado poesa lrico combinar necesidad exibilidad brindar tecnica deep dearning regla especca acerca metrica acentuacion rima texto generado ejemplo encontrar ghazvininejad et combinar red lstm generacion texto poetico maquina nito aceptar texto cumplir estructura deseado lau et utilizar embeddings red lstm generacion lnea candidata formar soneto elegir adapta estructura deseado funcion metrica rima encontrar trabajo tratar generacion texto poetico ingl yi et utilizar red neuronal recurrente entrenado poesa clasico chino zugarini et entrenar modelo basado slaba poema italiano dante alighieri unico trabajo encontrar utilizar lenguaje espanol ghazvininejad et referenciado anteriormente trabajo enfocar principalmente generacion texto ingl ultimo seccion mostrar prueba generacion texto espanol utilizar modelo desarrollado entrenado banco letra canción espanol texto extrado wikipedia generacion poesa trabajo tratar generacion texto genero similar freestyle ver potash et generar letra rap ingl batalla freestyle agreguir componente resultar inter generacion automatico texto objetivo texto improvisar tiempo real respuesta rima competidor generacion texto s respetar rima metrica adecuado necesario tiempo inmediato caracterstico proporcionar modelo generacion automatico objetivo objetivo objetivo tesina desarrollar modelo generador texto estructurado logre generar texto espanol respetar caracterstica utilizado marco competencia freestyle rima metrica utilizar tecnica deep learning presentar objetivo especco generar base dato contengar transcripción batalla freestyle mitar entrenamiento modelo generacion lenguaje natural nlg sigla ingl estudiar implementar comparar distinto tecnica procesamiento texto entrenamiento modelo deep learning tal word embeddings sentacion palabra vector numero real forma cion estudiar modelo neuronal utilizado generacion texto particularmente red neuronal recurrente rnn tal lstm denir arquitectura modelo lenguaje entrenir base dato generado implementar sistema utilize modelo lenguaje entrenado generacion texto responder forma estilo utilizado batalla freestyle utilizar tecnica permitir garantizar texto generado cumplir estructura rima acorde organizacion documento presente tesinar estructurado forma captulo presentar marco teorico reerar aprendizaje automatico dizaje profundo necesario cabo desarrollo trabajo describir concepto modelo aprendizaje automatico particular red neuronal entrenamiento utilizar él tarea especca clasicacion binario multiclase describir arquitectura particular red comunmente utilizado ro procesamiento secuencia texto tal red neuronal recurrente especcamente lstm captulo profundizar area procesamiento lenguaje natural desarrollar concepto clave campo tal tokenizacion representacion documento caso especco word embeddings modelo lenguaje problematica especca trabajar texto estructurado puntualmente genero batalla freestyle captulo presentar desarrollo proceso llevar cabo encontrar confeccion base dato texto enmarcado capitulo introducci on genero rap freestyle respectivamente analisis texto obtenido prueba forma tokenizacion nalmente entrenamiento modelo lenguaje neuronal utilizar generacion tipo texto describir programa desarrollado garantizar texto generado cumplir estructura rima correspondiente genero captulo presentar conclusión obtenido listar posible linea trabajo futuro continuar desarrollado tesina captulo deep learning inteligencia articial ai sigla ingl campo ciencia computacion buscar entender principio comportamiento inteligente sistema natural articial hipotesis razonamiento proceso computable poo él et desafo inteligencia articial encontrar resolver problema humano poder resultar facil cabo difcil describir formalmente tal reconocer palabra escucha identicar rostro imagen goodfellow et aprendizaje automatico machine learning ml rama inteligencia articial buscar lugar computadora actue funcion instrucción explicitada previamente humano aprenda forma correcto tarea conjunto dato lugar paradigma inteligencia articial inteligencia articial tradicional humano ingresar regla programa conjunto dato procesar nalmente obtener respuesta procesamiento dato reerar aprendizaje automatico ingresar dato conjunto respuesta esperado procesamiento esperar obtener regla programa permitir procesamiento regla obtenido aplicar dato obtener respuesta original chollet et gura buscar ilustrar diferencia descrito paradigma sistema aprendizaje automatico entrenar lugar explcitamente programado proceso entrenamiento cabo presentar él sistema distinto ejemplo resultir relevante tarea busca capaz encontrar estructura estadstico permitir obtener regla automatizar dicho tarea chollet et llamado aprendizaje profundo deep learning subrama aprendizaje automatico poner enfasis aprender capa representación signicativa dato forma sucesivo capa representación aprender aprendizaje tomatico dato entrenamiento subrama dicho representación capitulo deep learning figurar diferencia paradigma inteligencia articial clasico aprendizaje tomatico aprender trav modelo conocido red neuronal estructurado capa literal apilada chollet et modelo desarrollar forma extenso seccion gura ilustrar trav diagrama venn relacionar concepto inteligencia articial aprendizaje automatico aprendizaje profundo descrito figurar diagrama venn relacionar concepto inteligencia articial aprendizaje automatico aprendizaje profundo modelo lineal mencionar anteriormente trav aprendizaje automatico buscar obtener dato constituir entrada salida esperado tarea especca programa capaz dicho tarea obtener programa buscado conjunto modelo lineal totalidad posible programa posible función resultar problema difcil necesario restringir él familia especca programa función conocido clase hipotesis restringir él clase hipotesis particular establecer conocer sesgo inductivo conjunto asunción acerca forma solucion facilitar proceso encontrar él clase hipotesis determín representar programa resultante goldberg and hirst mohri et clase hipotesis común aprendizaje automatico función lineal dimensión dar cantidad entrada cantidad salida ds función forma xw b x rde w rdeds b rds caso vector x entrada funcion matriz w vector b llamado parametro entrenamiento modelo clase hipotesis consistirar ajustar valor parametro w b funcion comportar forma esperado conjunto dato compuesto valor entrada xn correspondiente valor salida esperado yn clase hipotesis función lineal restringida representar mente relación lineal modelo clase principal enfoque reerar procesamiento lenguaje natural estadstico servir unidad basica construccion modelo lineal poderoso red neuronal rrollada seccion goldberg and hirst resultar util introducir concepto utilizado modelo complejo aprendizaje profundo razón decidir incluir él captulo clasicacion binario problema llamado clasicacion binario consistar determinar entrada pertenecer clase particular tpicamente representado valor llamado clase negativo positivo respectivamente ejemplo tipo problema ser él clasicacion resena producto positiva negativa necesitar modelo capaz tomar entrada representacion adecuado resena forma representacion texto procesamiento lenguaje natural desarrollar captulo cuyo salida representar clase pertenencia resena entrada corresponder unico salida perteneciente conjunto modelar tipo problema utilizar version restringido ecuacion ds w vector b escalar capitulo deep learning xw b problema clasicacion binario salida modelo indicar correspondiente entrada pertenecer clase determinado representado valor salida modelo denido rango resultar necesario obtener clasicacion binario deneir valor conocer tpicamente umbral decision redeneir modelo xw b xw b forma entrenar modelo lineal permitir clasicar valor entrada clase conjunto separar conjunto dato problema efectivamente usar modelo lineal conjunto dato considerar linealmente separable hiperplano separar dato conjunto pertenecer clase forma perfecto mohri et modelo lineal logstico funcion sigmoidear modelo clasicacion binario descrito permitir clasicar efectivamente entrada clase conjunto brindar informacion acerca conanza dicha cacion modelo predicar probabilidad evento suceder probabilidad pertenencia clase conocer modelo logstico forma obtener modelo logstico utilizar problema clasicacion binario trav composicion funcion correspondiente modelo lineal ecuacion funcion sigmoidea gura graco funcion sigmoidear cuyo imagen intervalo utilizar modelo conjunto funcion error adecuado desarrollar seccion interpretar salida modelo entrada x dar probabilidad pertenencia x clase llamado clase positivo obtener probabilidad pertenencia x clase negativo clasicacion multiclase caso problema representar clasicacion binario común naturaleza clasicacion multiclase goldberg and hirst modelo lineal figura graca funcion sigmoidea mohri et problema clasicacion multiclase k clase distinto entrada asociar etiqueta conjunto k representar clase pertenecer ejemplo tipo problema clasicacion artculo periodstico distinto categora poltico deporte dar secuencia palabra palabra deber continuacion palabra representar clase distinto modelo clasicar entrada ejemplo desarrollado detalle seccion forma implementar clasicador multiclar k clase modelo lineal descrito ecuacion denir dimension salida modelo ds w rdek columna serar peso utilizado predecir pertenencia entrada k clases particular salida modelo pertenecer conjunto rk considerar ndice valor alto vector clase asignado modelo entrada particular modelo clasicacion multiclar forma b unico enfoque modelar problema clasicacion multiclase desarrollar totalidad quedar alcance trabajo encontrar acercamiento allwein et modelo lineal logstico funcion softmax as clasicacion binario utilizar funcion sigmoidea modelo lineal obtener estimacion probabilidad forma analogo utilizar clasicacion multiclase capitulo deep learning funcion softmax exi p j exj transformacion fuerza valor vector resultante positivo suma permitir interpretarlo distribucion probabilidad modelo utilizado prediccion multiclase resultar aplicacion funcion softmax resultado modelo denido b funcion error mencionar anteriormente algoritmo aprendizaje automatico recibir entrada conjunto entrenamiento denir par vector x tamano n representar ejemplo entrenamiento correspondiente valor esperado objetivo algoritmo encontrar funcion f mapeir forma preciso ejemplo correspondiente valor esperado predicción ejemplo conjunto entrenamiento x correcto obtener medicion formal certero predicción delo denir funcion error loss function asignar valor numerico prediccion funcion proximidad valor esperado conjunto entrenamiento determinado jar valor calcular error modelo funcion parametro comunmente denominado modelo lineal visto w b error l modelo f parametro conjunto prueba x denir l n n x yi objetivo algoritmo aprendizaje automatico encontrar valor parametro minimicir error modelo entropa cruzada categorico problema clasicacion multiclase comun utilizacion funcion conocido entropa cruzado categorico categorical funcion error mite medir disimilitud distribución probabilidad goldberg and hirst resultar relevante desarrollo trabajo usar funcion error descenso gradiente tipo problema utilizar medir similitud distribucion dad generado modelo entrada clase real interpretar clase real ejemplo distribucion probabilidad utilizar llamado sentacion consistir problema k clase distinto representar etiqueta clase i vector dimension k valor posicion i valor posición restante denicion general entropa cruzado categorico x i i distribucion probabilidad real forma vector simplicar t t ndice clase real ejemplo funcion penalizara ción probabilidad dar probabilidad asignado clase real ende probabilidad sumar diferiran probabilidad asignado demas posible clase descenso gradiente distinto metodo encontrar valor parametro minimicir error modelo pensar lugar solución analtica modelo sencillo lineal descrito metodo soler mente costoso medida crecer cantidad parametro generalizab él modelo complejo metodo ampliamente utilizado algoritmo aprendizaje automatico general particularmente algoritmo aprendizaje profundo conocido descenso gradiente goodfellow et descenso gradiente metodo efectivo entrenar modelo aprendizaje automatico recibir funcion f respectivo parametro funcion error l conjunto entrenamiento x buscar establecer valor minimizar error modelo goldberg and hirst montavon et presentar ventaja algoritmo iterativo generalizable utilizar modelo lineal maquina vector soporte red neuronal rapido metodo analtico utilizar cantidad dato metodo consistir iterativamente calcular vector gradiente funcion error l parametro haber jar valor conjunto entrenamiento actualizar valor sustraer él valor vector gradiente calculado punto capitulo deep learning multiplicado valor conocido factor aprendizaje learning ratar equivaler desplazar él l direccion maximo decrecimiento punto ilustrar gura factor aprendizaje valor escalar positivo determinar serar actualización parametro salto desplazamiento funcion error valor factor aprendizaje implicar necesitar paso llegar converger mnimo l valor alto podra algoritmo divergir ilustrar gura valor ideal denido seleccion factor aprendizaje practica común probar valor evaluar desempeno entrenamiento disminuir valor iteración goodfellow et goldberg and hirst criterio corte algoritmo soler denir él cantidad ja iteración conjunto entrenamiento alcanzar valor determinado error modelo figurar ilustracion proceso descenso gradiente espacio tridimensional algoritmo mostrar pseudocodigo correspondiente descenso gradiente camente version conocido descenso gradiente estocastico mini lot diferencia ambos versión momento calcular gradient descenso gradiente tradicional conjunto entrenamiento version aqu presentado calcular funcion subconjunto llamado mini lote obtener forma aleatorio iteracion permitir trabajar conjunto dato tamano resultar practico procesar totalidad descenso gradiente factor aprendizaje pequeno canzar mnimo paso b factor aprendizaje censo gradiente divergir figurar graco error funcion parametro iteración descenso gradiente factor aprendizaje chico iteracion algoritmo entrenamiento trav descenso gradiente estocastico datar funcion respectivo parametro conjunto entrenamiento compuesto vector x entrada correspondiente valor esperado funcion error l while cumplir criterio nalizar do tomar forma aleatorio mini lote minibatch m ejemplo conjunto entrenamiento computar estimador vector gradiente g m p i yi actualizar parametro g end sobreajuste factor relevante aprendizaje automatico relacion optimizacion generalizacion optimizacion reerar ajustar modelo conjunto dato entrenar generalizacion reerar desempenar modelo entrenado dato ver chollet et practica comun evaluar factor separacion conjunto dato subconjunto utilizado conjunto entrenamiento ejecutar capitulo deep learning algoritmo entrenamiento correspondiente descenso gradiente conjunto prueba utilizar evaluar desempeno modelo dato entrenar tpicamente etapa entrenamiento optimizacion generalizacion cuentrar correlacionada medida disminuir error conjunto entrenamiento prueba etapa modelo subajustar undertting modelo capaz modelar patrón relevante dato disponible medida avanzar entrenamiento comun  capacidad generalizacion modelo evaluado desempeno conjunto prueba estancar empiecir disminuir desempeno conjunto miento continuo mejorar punto modelo sobreajustar overtting aprender patrón especco dato presente conjunto entrenamiento resultar enganoso irrelevante analisis dato chollet et solucion problema simplemente obtencion dato naturalmente modelo entrenado cantidad dato capaz generalizar caso opcion utilizar modelo sencillo generar capaz identicar menor cantidad patrón conjunto entrenamiento forzar proceso optimizacion enfocar él relevante posibilidad generalizar correctamente metodo evitar sobreajuste conocer regularizacion metodo consistir forzar peso red tomar valor pequeno resultar distribucion regular traducir él modelo simple lograr agregar funcion error utilizado entrenamiento costo asociado peso valor red forma común regularizacion conocer diferenciar forma computar costo obtener valor absoluto peso red obtener cuadrado red neuronal mencionar seccion modelo lineal aprendizaje automatico ten representar exclusivamente relación lineal caso buscar representar relación complejo resultar necesario utilizar modelo lineal caso red pensar él composicion modelo lineal función lineal llamado función activacion red neuronal simple conocido perceptron equivalente modelo lineal descrito ecuacion xw b x rde w rdeds b rds red neuronal forma comun representar red neuronal trav grafo nodo llamado neurona unidad computo recibir valor escalar entrada salida neurona computar producto entrada respectivo valor llamado peso representado arista red corresponder parametro modelo valor buscar optimizar valor obtenido aplicar funcion lineal cuyo resultado salida neurona gura mostrar vista perceptron descrito ecuacion representado figurar representacion perceptron forma grafo ds gura neurona organizar capa reegir ujo informacion corresponder caso salida transformacion lineal capa resultar efectivamente transformación lineal conocer capa totalmente conectado layers capa denso unico arquitectura tipo capa llamada convolucional pooling quedar alcance trabajo capa representar valor entrada modelo ultimo salida capa intermedio denominar capa oculto cantidad capa modelo conocer comunmente profundidad red goldberg and hirst modelo descrito informacion uye capa suma operacion comun caso utilizacion función tal maximo capitulo deep learning entrada salida trav capa intermedio ningun tipo tacion denominar red neuronal goodfellow et red incorporo conexión retroalimentacion red neuronal recurrente descriptas detalle seccion perceptron corresponder modelo lineal obtener modelo lineal resultar necesario incluir capa oculto incorporar precisamente funcion lineal dar lugar llamado perceptron multicapa capa oculto tipo red neuronal ilustrado forma grafo gura forma x rde rds figurar representacion perceptron multicapa forma grafo ds capa oculto dimension describir red neuronal especicar cantidad capa conjunto respectivo dimension dimension entrada dimension salida ds ds salida escalar utilizar red modelar problema regresion clasicacion binario forma modelo lineal describio seccion as ds k modelar problema clasicacion red neuronal k clase diferencia modelo lineal red neuronal terminos representacion capacidad aproximador universal teora capaz aproximar función continuo subconjunto cerrado rn hornik et cybenko aspecto red neuronal tal cantidad capa respectivo cantidad neurona entrar conocer hiperparametro red parametro utilizar controlar proceso aprendizaje red aprender denir manualmente ejemplo factor aprendizaje tamano mini lote utilizado descenso gradiente función activacion distinto función lineal tpicamente usado función activacion red neuronal funcion utilizar depender mayormente prueba emprica problema particular goldberg and hirst utilizado funcion sigmoidea descrito seccion funcion tangente hiperbolico funcion relu rectier linear unit funcion tangente hiperbolico denir graca gura transformar entrada x valor intervalo funcion relu glorot et denir x x x casocontrario funcion sencillo demostrar lograr desempeno distinto tarea funcion tangente hiperbolico cuyo graca observar gura superar ampliamente rendimiento funcion sigmoidea entrenamiento red neuronal entrenamiento red neuronal funcion conjunto entrenamiento cabo mencionar utilizar algoritmo descenso descenso gradiente presentado seccion utilizar tpicamente función error utilizado modelo lineal descrita seccion principal diferencia entrenamiento red neuronal modelo lineal reerar forma capitulo deep learning b figurar graco función tanh relu ejecucion algoritmo descenso gradiente calcular evaluan gradient parametro red algoritmo conocido rumelhart et permitir cabo proceso calcular evaluar gradient parametro red forma simple computacionalmente costoso diferencia metodo analtico quedar alcance trabajo detallar profundamente funcionamiento algoritmo rasgo utilizar regla cadena calculo variable z depender variable depender variable x dz dx dz dy dy dx computar sucesivamente gradient capa red orden especco permitir ejecutar él forma altamente eciente goodfellow et dropout tecnica conocido dropout forma conocido usado gularizacion red neuronal prevenir sobreajuste discutido seccion chollet et aplicado capa red consistir aleatoriamente entrenamiento apagar neurona capa jar valor salida cero proporcion neurona apagar hiperparametro denir soler valor común etapa evaluacion apagar neurona valor capa escalar factor cion neurona apagado entrenamiento compensar habrar unidad activo entrenamiento tecnico evitar predicción red dependar peso neurona especca ruido introducido red aleatoriedad neurona apagado traducir descartir patrón red neuronal recurrente casual red memorizar caso ruido presente chollet et goldberg and hirst transfer learning modelo neuronal medianamente complejo caso cantidad dato escaso soler modelo generalice forma pobre sobreajuste dato disponible afrontar problematica recurrir sencillo comun tecnica conocido campo aprendizaje profundo transfer learning consistir modelo neuronal entrenado conjunto dato corresponder exactamente problema original presentar formato caracterstica similar tamano signicativamente congelar valor peso capa resultante entrenamiento disminuir factor aprendizaje entrenar modelo s conjunto dato correspondiente problema especco ultimo etapa miento conjunto dato especco tamano conocer esperar entrenamiento conjunto dato tamano permitir modelo extraer caracterstica general resultir util momento trabajar dato especco problema zhang et forma entrenamiento disminuir sobreajuste entrenamiento modelo complejo conjunto dato pequeno permitir peso red estar valor cercano optimo alcanzar convergencia rapido evitar caer mnir local facilitar namiento etapa tecnica ampliamente utilizado distinto area aprendizaje profundo campo procesamiento lenguaje natural especicamente comunmente utilizado entrenamiento vector representacion palabra conocido word embeddings profundizar seccion red neuronal recurrente red neuronal recurrente rnns sigla ingl rumelhart et familia red neuronal especializado procesamiento dato mayora red capacidad procesar entrada longitud variable diferencia red permitir entrada tamano jo goodfellow et momento procesar entrada xi formar secuencia xn rnns acceso valor capa oculto red resultante procesar entrada anterior secuencia valor capa oculto procesar entrada xi conocido red tpicamente denido h referencia valor capa oculto hiddir layers red denir recursivamente mostrar ecuacion as funcion representar red neuronal funcion involucrir recurrencia capitulo deep learning representar rnn goodfellow et ht xt forma comun representar red neuronal recurrente conocer forma grafo desplegado consistir representar elemento red paso temporal elemento red representar tanto secuencia procesar ejemplo rnn representado forma gura figurar representacion rnn forma grafo desplegado entrenamiento red neuronal cabo forma similar red aplicar algoritmo descenso gradiente función error descrita error gradient calcular grafo desplegado rnn procesamiento totalidad secuencia variante algoritmo conocer trav tiempo through time bptt werbos rnns bidireccional rnns presentado momento considerar unicamente informacion entrada pasado procesar entrada xn caso resultar necesario totalidad secuencia ejemplo tipo problema reconocimiento dgito escrito informacion acerca fonema red neuronal recurrente dgito continuacion previo procesar permitir clasicacion ejemplo servir desambiguar posible termino red neuronal recurrente bidireccional crear justamente abordar sidad schuster and paliwal resultar exitosa grave aplicación surgir tal reconocimiento escritura mano grave et nocimiento grave and schmidhuber grave et bioinformatico baldi et rasgo rnn bidireccional componer subred recurrente procesarar secuencia orden comienzo procesara forma inverso nal forma modelo permitir computar representacion valor xt secuencia depender informacion futura resultar sensible valor cercano valor necesidad utilizar ventana tamano jo rededor valor resultar necesario caso utilizar red neuronal goodfellow et arquitectura compuerta entrenamiento rnns presentar problema particular conocido problema desvanecimiento gradiente vanishing gradient pascanu et consiste gradient paso tardo procesamiento secuencia disminuir rapidamente proceso causar valor pequeno momento alcanzar valor correspondiente principio secuencia resultar dicil red captar dependencia plazo secuencia goldberg and hirst considerar rnn dispositivo computo proposito general pensar aplicacion modelo leer entrada xi memoria representado red instante operar forma resultar viendolo forma considerar problema rnn control acceso memoria paso lee escribir totalidad memoria disponible llamado arquitectura compuerta gated architectur familia rnns utilizar llamado compuerta gates controlar acceso memoria compuerta basicamente vector binario realizar producto punto entrada memoria respectivamente permitir red discriminar valor entrada serar tenido descartado as valor memoria almacenar serar sobrescrito detalladamente dar compuerta g entrada x rn memoria s rn denir memoria g x g almacenar valor entrada compuerta permitir cuyo posicion sobrescribeir valor haber memoria posicion descartar valor cuyo posicion compuerta contener capitulo deep learning mantener valor almacenado memoria vector correspondiente compuerta utilizar componente red neuronal utilizar controlar acceso memoria red aprender resultar importante retener olvidar determinado informacion obtenido comportamiento compuerta estatico aprender él conjunto resto parametro red requerir vector diferenciabl as utilizar correctamente algoritmo razon lugar utilizar vector binario representar compuerta utilizar vector numero real pasar trav funcion sigmoidea cercana valor resultante indicar dejar pasar valor entrada corresponder lstm figurar graco red lstm forma grafo desplegado arquitectura red neuronal conocido long memory lstm troducido hochreiter and schmidhuber tipo rnn compuerta disenado especcamente resolver problema desvanecimiento diente rasgo arquitectura dividir vector vector denominado celda memoria c correspondiente oculto h do memorioa trabajo celda memoria utilizar preservar memoria gradient tiempo controlar compuerta compuerta conocido entrada input olvido forget salida output valor obtener trav computar combinacion lineal vector entrada actual xi oculto pasar trav funcion sigmoidea compuerta olvido f determinar memoria previo mantener f compuerta entrada determín entrada i x x finalmente compuerta salida determín red neuronal recurrente valor mencionada celda memoria tener memoria trabajo procesamiento entrada hj visualizacion graco arquitectura descrito observar gura red lstm forma exitoso arquitectura red recurrente goldberg and hirst demostrar poseer capacidad der dependencia plazo facilmente arquitectura principio conjunto dato disenado especcamente tipo tarea bengio et hochreiter and schmidhuber hochreiter et problema procesamiento secuencia lograr desempeno alcanzo arte grave grave et sutskever et capitulo deep learning captulo procesamiento lenguaje natural procesamiento lenguaje natural nlp sigla ingl rama ciencia computacion inteligencia articial cubrir conjunto tecnica analizar representar texto ocurrar naturalmente nivel lingustico proposito alcanzar escala procesamiento similar humano rango tarea aplicación liddy desarrollar profundidad punto denicion texto ocurrar naturalmente referir yo texto idioma genero condicion utilizar humano comunicar él s generar especcamente proposito analisis considerar denicion reerar objetivo nlp rango tarea aplicación senalo n concretar n especco traduccion texto generacion resumen respuesta pregunta interaccion trav dialogo eje trabajo generacion lenguaje natural liddy trabajo nlp surgir ano acercamiento inteligencia articial simbolico regla denida mano rul enfoque resultar util analisis sintactico texto mostrar insucient tratar signicado semantica lidiar caracterstica lenguaje natural ambiguedad texto cuyo prós respetar necesariamente regla sintactica idioma as resultar comprensible lector humano nadkarni et acrecentar disponibilidad cantidad texto electronico surgimiento internet duda principal fuente texto bird et razón principal impulsora anos haber giro pronunciado metodo estadstico nlp encontrar forma aprendizaje automatico liddy et proceso descubrimiento conocimiento kdd sigla ingl texto enmarcar proceso descrito sección siguiente capitulo procesamiento lenguaje natural tomo referencia ordenar contenido captulo proceso constar lugar etapa preparacion dato dato inicial obtener vista minable etapa minera dato detectar patrón resultir relevante vista minable obtenido etapa evaluacion interpretacion visualizacion dato obtenido gura ilustran etapa mencionado listar proceso desarrollado trabajo enmarcar etapa figurar etapa proceso descubrimiento conocimiento kdd captulo desarrollar tarea principal especca campo nlp proceso kdd tecnica utilizado llevar él cabo punto especco area conciernar trabajo particular seccion tratar tecnica preprocesamiento texto tokenizacion subseccion algoritmo byte pair encoding subseccion seccion describir distinto acercamiento representacion texto representacion atributo estatico subseccion dinamico subseccion aprendido subseccion caso especico word embeddings cabo entrenamiento subseccion seccion denir modelo lenguaje detallar utilizado modelo modelo neuronal ventana ja modelo neuronal recurrente subsección ilustrar generacion lenguaje natural seccion profundizar acerca problema generacion texto estructurado subseccion describir dominio especco trabajo batalla freestyle caracterstica particular trabajar generacion texto preprocesamiento agrupan etapa preprocesamiento texto tarea encargado convertir texto formato resultar analizable proposito especco eleccion combinacion correcto tarea resultar mejora signicativo resultado trabajo nlp uysal and gunal preprocesamiento seccion desarrollar principal tarea preprocesamiento necesario trabajo nlp tokenizacion particion texto subseccion algoritmo permitir cabo forma especca tokenizacion conocido byte pair encoding subseccion nombrar tarea pertenecer etapa ltrado caracter palabra remocion signo puntuacion palabra funcion bajo frecuencia normalizacion ejemplo conversion letra mayuscula minuscula lematizacion conversion palabra forma basica conversion verbo innitivo truncado palabra soler resultar necesario tarea limpieza texto tal deteccion remocion error ortograco caracter invalido tokenizacion tokenizacion particion proceso separar texto crudo mnima unidad lingustica identicabl llamado tokens dicho tokens considerar unidad indivisibl hora procesamiento principal desafo respectar tokenizacion encontrar forma separacion permitir extraer informacion util token jurafsky and martin continuacion subsección siguiente describir posible enfoque tokenizacion nivel palabra caracter subpalabra tokenizacion nivel palabra tokenizacion nivel palabra enfoque tpico sencillo plementar consiste utilizar separador caracter espacio blanco signo puntuacion computar él trav algoritmo determinstico expresión regular compilar automata nito ecient jurafsky and martin enfoque traer problema utilizacion optima tokenizacion nivel palabra mayora caso implicar tamano vocabulario cantidad tokens utilizado signicativamente pretender incluir distinto palabra encontrar base dato conllevar deciente memoria baja performance modelo trabajar tipo tokenizacion ejemplo modelo arte utilizar enfoque transformer xl dai et vocabulario tokens entrenar modelo texto tokenizar nivel palabra principio modelo capaz procesar ejemplo palabra gurar vocabulario tratar problema soler incorporar diccionario capitulo procesamiento lenguaje natural modelo token representar palabra desconocido traducir obtener informacion dicho palabra problema surgir tokenizacion nivel palabra division elemento resultar util ser unico token caso nombre aires brindar informacion acerca reerar tokens aires separado tipo tokenizacion error escritura tomar relevancia token eror guardar principio ningun tipo relacion token error tokens contengar tipo error resultar inutil modelo entrenado texto tokenizado tokenizacion nivel caracter busca solucionar problema mencionado anteriormente presentar tokenizacion nivel caracter forma tokenizacion inmediatamente resultar vocabulario reducido ejemplo bastar tokens representar caracter espanol utilizado representar caracter especial comun encontrar tipo tokenizacion idioma chino caracter unidad semantico razonable jurafsky and martin forma tokenizacion presentar ventaja modelo entrenado nivel caracter capaz procesar entrada encontrar ningun token pertenecer vocabulario denido originalmente retomar caso idioma espanol ingl trabajar tipo tokenizacion necesariamente implicar necesidad trabajar secuencia tokens larga obtener informacion util secuencia palabra resultar suciente obtener estimacion palabra secuencia caracter insuciente inferir palabra palabra formar lograr resultado interesante utilizar enfoque ver él radford et kalchbrenner et permitir alcanzar objetivo inicial tokenizacion obtener unidad valor semantico tokenizacion nivel subpalabra forma intermedio tokenizacion nivel palabra cion nivel caracter tokenizacion nivel subpalabra tipo tokenizacion generar tokens unico palabra común dividir palabra común subpalabra valor semantico ejemplo palabra invertebrado descomponer él tokens in vertebr ado obtener nocion signicado palabra forma tokenizacion principal utilizado modelo arte respectar generacion guaje natural brown et bert devlin et algoritmo utilizado lograr él wordpiecir unigram lm byte pair encoding preprocesamiento utilizado trabajo desarrollado continuacion subseccion byte pair encoding algoritmo conocido byte pair encoding traducir codicacion par byt algoritmo compresion dato descrito originalmente gage proposito original tokenizacion texto tarea nlp ampliamente extendido proponer tipo tarea sennrich et convertir tokenizacion nivel subpalabra norma modelo arte respectar nlp mencionado bert algoritmo consistir etapa inicializacion vocabulario palabra tokenizar nivel caracter adjuntar token especial indicar nal palabra tpicamente denominado generar vocabulario forma diccionario palabra tokenizado asociar cantidad ocurrencia texto ejemplo simplicado diccionario generado forma descrito c n t r r b l v l r caso ejemplo diccionario obtener texto contener palabra cantar palabra arbol volar algoritmo continuo forma iterativo iteracion frecuencia par tokens consecutivo unir token caso ejemplo aparición par frecuente compuesto tokens r unir token ar diccionario quedar forma c n t ar ar b l v l ar iteracion par frecuente tokens ar concatenar token ejemplo necesidad utilizacion token indicar nal palabra token probable indicar presencia verbo innitivo dierar inferir presencia token ar arbol unir tokens mencionado diccionario pasar forma c n t ar b l v l llegar sexto iteracion formar tokens cant ol diccionario presentar forma cant ar b ol v ol forma considerar tokenizacion nivel subpalabra util blema determinado logicamente base dato mundo real presentar cantidad palabra aparición presentado ejemplo capitulo procesamiento lenguaje natural observar iteración palabra estar representado token unico diccionario estarar compuesto tokens union tokens cantidad iteración denir él utilizar valor jo tener cantidad tokens desear compuesto vocabulario dicho cantidad iteracion escenario posible numero tokens crementar él iteracion ejemplo aumentar iteracion mantener él practica medida cantidad iteración incrementar tpicamente cantidad tokens incrementar principio decrementar él representacion documento extraccion caracterstica etapa importante problema aprendizaje automatico inuencia directo resultado obtengar alla modelo utilizar reerar nlp extraccion caracterstica consistir convertir texto crudo legible forma numerico adecuado modelo aprendizaje automatico conocer representacion texto vajjala et clasicar distinto forma representacion texto categora utilizar caracterstica estatica utilizar caracterstica dinamica lizar caracterstica aprendido seccion describir categora presentar ejemplo forma representacion encontrar jurafsky and martin resultar necesario mencionar ejemplo extraccion caracterstica forma representacion mencionado seccion principalmente palabra realidad aplicar tokens palabra subpalabra caracter metodo representacion caso previo popularizar utilizacion forma tokenizacion palabra aplicar igualmente nivel tokenizacion descrito seccion representacion caracterstica estatica representacion texto caracterstica estatica consistir utilizar caracterstica gida previamente procesamiento documento tpicamente grupo experto dominio problema especco representar documento funcion dicho racterstica llamado featur ejemplo caracterstica cantidad palabra longitud promedio palabra cantidad promedio tipo palabra terstica dominio ser él cantidad cita parrafo texto academico tipo representación resultar util problema clasicacion texto representaci on documento informacion expertar acerca caracterstica resultar relevante clasicar texto correctamente ejemplo trabajo utilacer enfoque ferretti et hassen et buscar identicar artculo wikipedia calidad deciente utilizar caracterstica estatica tal cantidad texto referencia buscar clasicar software malicioso funcion frecuencia codigo operacion mnemonico representacion caracterstica dinamica representación caracterstica dinamica utilizar caracterstica gir procesamiento dato tpicamente forma representacion generar lugar vocabulario contener totalidad tokens texto representar funcion vocabulario jurafsky and martin ejemplo representacion tipo llamado bolsa palabra bow sigla ingl lugar mencionar generar vocabulario texto texto representar tabla indicar palabra vocabulario presente texto opcionalmente cantidad aparición forma representacion orden dispuesto palabra texto comun encontrar él problema clasicacion especcamente sentiment analysis analisis sentimiento texto jurafsky and martin caso identicar resena producto positivo negativo ejemplo aplicacion tipo representacion resena producto ilustrar gura figurar ejemplo representacion forma bolsa palabra texto corresponder resena producto forma representacion texto utilizar caracterstica dinamica incluir indexacion vocabulario texto posterior representacion palabra correspondiente ndice vocabulario ejemplo querer representar frase perro gato casa vocabulario conjunto v perro gato casa palabra correspondera ndice i frase quedar representado lista representacion utilizar ndiz soler optima modelo texto inferencia equivocada caso ejemplo palabra perro capitulo procesamiento lenguaje natural comun  palabra perro gato ndiz proximos necesariamente modelo buscar predecir palabra texto posibilidad alto palabra perro gato optar palabra ndice intermedio anterior solucion problema mencionado comun  representacion texto llamado codicacion consistir representar palabra vector tamano vocabulario contener posicion correspondiente ndice palabra resto posición vector querer codicar frase ejemplo utilizar dicho representacion palabra representar perro gato casa logicamente texto representado forma requerira espacio almacenamiento utilizar ndiz palabra resultar problema trabajar vocabulario tamano eliminar problema surgido utilizar codicacion resultar importante notar representacion guardar ningun tipo valor semantico palabra representado goldberg and hirst ejemplo mostrado perro gato distinto perro casa necesariamente corresponder signicado palabra representacion caracterstica aprendido word bedding llamado representación aprendido presentar mayor salto reerar representacion palabra modelo reciente zaje profundo nlp goldberg and hirst idea consistir extender aprendizaje automatico usualmente utilizado etapa minera dato etapa representacion aprender representar palabra funcion contexto forma principal tipo representacion caracterstica llamado word embeddings subseccion desarrollar concepto embeddings ventaja brindar tipo representacion asumir vector embeddings adecuado subsecccion describir metodo utilizado obtener dicho vector utilizacion word embeddings consistir representar palabra vector denso longitud ja numero real palabrar signicado similar tener representación similar lograr contexto palabra aparecer frecuentemente goldberg and hirst forma representacion presentar ventaja enfoque anterior representacion primero computacional obtener representacion palabra dimension vector representacion tamano vocabulario valor signicativamente general diferencia representaci on documento tamano respectivo vector embedding palabra valor arbitrario jo tamano ideal problema abierto generalmente menor tamano vocabulario valor común utilizado dimension vector embeddings estudio yin and shir red neuronal soler funcionar vector disperso alto dimension goldberg and hirst ver ejemplo representacion permitir obtener informacion semantico palabra representado dar vector hot representar palabra distinto encontrar relacionado diferencia representacion trav embeddings permitir cercano semanticamente palabra trav llamado similitud coseno utilizar embedding bidimensional interpretar gracamente observar grado angulo vector palabra signicado similar cercano palabra cuyo semanticar relacionado ejemplo ver gura resultar importante mencionar embedding dimensionalidad perder capacidad gracar él similitud coseno funcionar figurar ejemplo posible vector representar palabra perro gato casa angulo primero menor casa valor ejemplo meramente ilustrativo obtener algoritmo calcular embedding obtencion vector embeddings subseccion describir proceso obtencion vector embedding representacion palabra token vocabulario texto presentado mikolov et proceso constar razgo capitulo procesamiento lenguaje natural etapa etapa conformar conjunto dato adecuado texto utilizar concepto ventana deslizante skipgram muestreo negativo descrito continuacion etapa aprender efectivamente vector embeddings conjunto dato generado ventana deslizante mencionar embedding buscar captar informacion acerca contexto palabra generar dato poder captar contexto realizar entrenamiento ejecutar texto conocer algoritmo ventana deslizante tamano denir formar conjunto dato iterar texto tomar inicialmente palabra desplazar ventana posicion tomar palabra wn procesar texto completo tpicamente dato entrenar modelo lenguaje modelo computar probabilidad ocurrencia enesimo palabra funcion n palabra anterior seccion desarrollar profundidad acerca modelo lenguaje cadena obtenido ventana deslizante soler separar par contener posicion primero n palabra cadena utilizar entrada modelo enesimo palabra representar objetivo predecir modelo ejemplo conjunto dato generado trav algoritmo texto observar gura figurar conjunto dato generado aplicar ventana deslizante tamano texto interesante notar tamano ventana hiperparametro inuir nalmente representar embedding entrenado tamano ventana pequeno tre palabra soler captar similitud funcional palabra llevar representaci on documento palabra cuyo vector similar poder utilizar contexto poder considerar intercambiabl tener palabra rodear goldberg and hirst ejemplo soler antonir palabra malo soler rodeado inmediato palabra plato malo malo deporte skipgram mencionar subseccion conjunto dato utilizado miento modelo lenguaje consistir par relacionar palabra n n enesimo palabra arquitectura conocido skipgram utilizado entrenamiento embeddings denir estructura distinto conjunto dato generado lugar utilizar ventana deslizante tamano n particularidad n palabra previo entrada prediccion enesimo palabra contemplar n palabra encontrar continuacion diferencia formato dato relacionar palabra anterior palabra buscar predecir relacionar separado ejemplo aplicacion gura figurar conjunto dato generado aplicar ventana deslizante tamano texto utilizar arquitectura skipgram ultimo modicacion estructura conjunto dato razon convertir problema forma problema clasicacion binario resultar capitulo procesamiento lenguaje natural simple rapido calcular lugar modelar dato forma palabra entrada salida estructuran forma par palabra correspondera salida caso relacionado caso estar él gura ilustrar ejemplo aplicacion modicacion descrito conjunto dato figurar ejemplo conjunto dato formato par palabra respectivo conjunto formato clasicacion binario muestreo negativo resultar necesario notar proceso descrito subsección anterior obtener conjunto contener unicamente par palabra relacionado probable modelo entrenado conjunto aprendar inferir par palabra recibir entrada encontrar relacionado logicamente error solucion problema utilizar llamado muestreo negativo consistir incluir conjunto dato muestra palabra encontrar relacionado cabo tomar ejemplo conjunto dato generar relacionir palabra par palabra aleatorio vocabulario encontrar previamente relacionado correspondera salida idea baso propuesto gutmann and hyvarinir conjunto proceso mencionado subsección anterior conocer skipgram muestreo gativo skipgram with negativir sampling sgns  gura ilustrar ejemplo representaci on documento conjunto dato obtenido texto utilizar sgns figurar conjunto dato generado utilizar sgns  texto aprendizaje embedding denir estructura conjunto dato utilizar proceder desarrollar proceso entrenamiento embedding lugar necesario denir tamano vocabulario v cantidad palabra especcamente tokens buscar entrenar vector embeddings tamano dicho vector mencionar anteriormente valor comun considerar valor denir modelo neuronal buscar predecir par palabra encontrar relacionado contar matriz embeddings llamada contexto ambos tamano v xe trav entrenamiento modelo buscar ajustar matriz embeddings servir representacion adecuado entrenamiento proceso iterativo iteracion conjunto dato ejemplo positivo palabra entrada x respectivo ejemplo negativo tomar par palabra x encontrar relacionado conjunto dato cantidad n depender cantidad muestra negativo muestra positivo haber insertar par palabra x w x z estir relacionado correspondera conjunto dato prediccion modelo palabra par tomado relacionado corresponder aplicacion funcion sigmoidea producto punto capitulo procesamiento lenguaje natural vector embeddings palabra entrada x vector contexto palabra par tomado respectivamente considerarar error modelo diferencia resultado esperado par palabra ejemplo positivo negativo resultado prediccion par error buscar minimizar actualizar parametro modelo valor matriz embeddings iteracion proceso entrenamiento ilustrar gura figurar iteracion proceso entrenamiento descrito avanzar entrenamiento modelo alcanzar error mnimo sariamente descartar matriz contexto esperar matriz embeddings modelo contener vector corresponder adecuado representacion token vocabulario modelo lenguaje generacion lenguaje tural conocer modelo lenguaje sistema dar secuencia palabra capaz determinar distribucion probabilidad palabra modelo lenguaje generaci on lenguaje natural palabra xi token vocabulario v goldberg and hirst modelo encontrar sistema teclado predictivo sugerencia buscador sistema generacion texto sistema generador texto construido modelo lenguaje funcionar general denir sentencia inicial s comunmente llamado semilla seed utilizar s entrada modelo lenguaje devolver distribucion probabilidad elemento vocabulario indicar respectivo bilidad token actualizar s agregar él token vocabulario probabilidad token repetir paso ultimo n elemento repetir cantidad ja s alcanzar longitud deseado s contendra texto generado sencencia semillo aclaracion punto caso tomar nuar texto generado token estrictamente presente probabilidad tomar token azar distribucion probabilidad generado modelo goldberg and hirst distribucion probabilidad soler cocientar valor conocido temperatura aumentar disminuir distancia probabilidad token as controlar azarosidad modelo menor tura implicar diferencia probabilidad serar mayor caso factible seleccionir token probabilidad inicialmente alto raro seleccionir token probabilidad bajo resultar sistema generacion conservador temperatura alto suavizar diferencia posibilidad token resultar sistema utilizar frecuentemente kens probable generar texto diversidad propenso equivocar él hinton et seccion detallar modelo lenguaje utilizado modelo lenguaje subseccion modelo neuronal ventana ja subseccion modelo neuronal recurrente subseccion modelo lenguaje llamado modelo modelo lenguaje sencillos surgir idea obtener probabilidad enesimo palabra cadena wn utilizar simplemente regla cadena probabilidad capitulo procesamiento lenguaje natural n cantidad posible sentencia querer considerar palabra previo dada calcular probabilidad seguramente variado dato sucient estimación conabl modelo proponer utilizacion secuencia n palabras modelo utilizar predecir palabra n ejemplo común tipo modelo llamado modelo markov asuncion probabilidad ocurrencia palabra determinado unicamente numero n jo palabra anterior conocer asuncion markov probabilidad ocurrencia palabra m dar anterior asumir similar ocurrencia dado unicamente n palabra anterior general modelo resultar modelo insucient lenguaje principalmente soler existir dependencia distancia posible trabajar tamano jo modelo costoso termino almacenamiento texto vocabulario v posible texto serar ocurrirar serar valido cantidad crecer multiplicativamente aumentar tamano trabajar tamano implicara tamano general signicativo vocabulario naturaleza lenguaje natural estadstica obtenido dispersa goldberg and hirst modelo lenguaje neuronal ventana ja modelo utilizar arquitectura red neuronal descrito detalle captulo prediccion modelo lenguaje dar secuencia palabra obtener palabra salvedad modelo contemplar cantidad ja n palabra tpicamente modelo tomar entrada representacion palabra contar capa inicial dimension nxv v tamano vocabulario inmediatamente conectado capa embeddings asignar palabra vector embeddings correspondiente entrenar previamente entrenar él resto red tendrar capa oculto modelo lenguaje generaci on lenguaje natural capa salida tamano v aplicar funcion softmax obtener prediccion modelo acerca probabilidad ocurrencia palabra vocabulario representacion graco tipo modelo observar gura figurar ejemplo arquitectura red neuronal correspondiente modelo lenguaje descrito principal ventaja modelo modelo necesidad almacenar prediccion resultar necesario unicamente etapa entrenamiento modelo reducir eliminar problema relacionado almacenamiento dispersion dato persistir limitación tamano jo ventana modelo anterior capaz dependencia palabra encontrar distancia n tamano tpico ventana utilizado dicho dependencia soler abundar lenguaje natural modelo lenguaje red neuronal recurrente red neuonal recurrente rnn sigla ingl familia red ronal caracterstica capacidad procesar entrada longitud solucionar principal problema modelo mencionado teriormente tarea nlp lidiar dependencia palabra distancia arquitectura especicar forma detallado captulo factor diferenciar tipo red arquitectura tpica momento procesar capitulo procesamiento lenguaje natural secuencia salida depender elemento n resultado procesar elemento resultado soler corresponder él version basico oculto red momento procesar elemento forma procesamiento secuencial elemento caso nlp tokens permitir capturar naturaleza secuencial inherentemente presente lenguaje young et gura mostrar forma graco ejemplo arquitectura figurar ejemplo modelo lenguaje utilizar arquitectura red neuronal rrente principal ventaja tipo modelo mencionar anteriormente lidad utilizar entrada longitud variable capacidad teora utilizar informacion palabra procesado paso practica modelo simple discriminar correctamente informacion relevante cuentra distancia palabra procesado versión simple sufrir conocer desvanecimiento gradiente vanishing gradient gradiente tomar valor pequeno medida propagar impedir entrenamiento eciente young et superar problema desarrollar tectura especca rnn buscar solucionar tipo problema llamada long memory encontrar desarrollado detalle captulo desventaja tipo red mencionar entrenamiento red recurrente resultar computacionalmente costoso modelo neuronal anterior generaci on texto estructurado generacion texto estructurado hablar modelo capaz generar texto resultar sucient captar texto respetar estructura especca caso generacion verso contexto poesa caso presente trabajo texto adecuar genero freestyle estructura reerar caracterstica poseer texto genero cantidad slaba linea determinado forma acentuacion palabra cantidad linea verso estructura rima esperado caracterstica estructural resultar factible lograr trav trenamiento modelo lenguaje reerar linea verso comun inclusion tokens especial indicar n linea n verso modelo aprendar insertar tokens texto generado ejemplo potash et zugarini et optar jar cantidad tamano linea verso modelo lenguaje responsable unicamente generar tokens ir formar dicho estructura ejemplo ver yi et modelo lenguaje generar linea linea determinar anteriormente verso estar compuesto linea ghazvininejad et ver dar estructura rima ja ende cantidad ja linea generar posible linea candidata utilizar rnn denir adecuar buscar generar caracterstica mencionada comienzo seccion difcil captar modelo lenguaje rima metrica razon dicha caracterstica soler incorporar trav regla ja afectar explcitamente texto generado acorde estructura deseado afectar negativamente signicado yi et lugar rima denir igualdad completo caso rima nante parcial caso rima asonante sonido palabra generalidad texto poetico caso puntual letra batalla freestyle compuesto estrofa cuyo linea conocido verso seguir estructura particular dar verso nalizar palabra rimar estructura describir tpicamente utilizar letra mayuscula representar verso verso representado letra corresponder nalicir palabra rimar estructura rima descrito aaaa reerar verso nalizar palabra rimar s estructura abab representar estrofa verso rimar cuarto mencionada estructura tpica abbar aabb gura mostrar ejemplo verso respectivo estructura rima mencionar tpicamente difcil modelo lenguaje capaz capitulo procesamiento lenguaje natural figurar verso extrado poema martn fierro estructura rima abbccb caracterstica deber caso tokenizacion nivel palabra subpalabra resultar imposible modelo identicar tokens rimar s representacion contemplar letra componer identicar patrón rima descrito requerir modelo capaz identicar dencia mediana largo distancia palabra nal verso rimar razón caso buscar generar texto cumplir estructura rima comun lugar jar estructura adecuar texto generado lograr denir patron rima mencionado aaaa abab caso genero buscar generar denar patron rima co ver yi et buscar generar forma particular poesa chino utilizar patron rima denido soler elegir forma aleatorio estrofa buscar adaptar texto generado dicho estructura sario modicar mnimo ultimo palabra generado verso presentar problema caso jo verso modelo generara ultimo palabra modicar ultimo palabra verso adapte estructura rima particular probable ocasionir dano valor semantico verso ultimo palabra modicado necesariamente adecuado encontrar él continuacion predecesora verso lugar generacion texto incoherente razón generar texto cumplir patrón especco rima soler util generacion inverso texto dar secuencia palabra buscar predecir palabra generar texto forma lugar antemano ultimo palabra linea modicar efectivamente adapte estructura buscado rimar palabra necesario verso siguiente modelo generaro ultimo palabra palabra antecesora verso obtener verso generaci on texto estructurado coherente yi et lau et encontrar ejemplo trabajo utilizar recurso apartado mencion problema especco identicar palabra efectivamente rimar variar idioma tpicamente afrontar problema trabajo generar texto idioma ingles utilizar diccionario rima disponible librera lenguaje programacion cosa ocurrir generacion texto espanol idioma encontrar literatura describir posible forma cabo tarea especco batalla freestyle freestyle subgenero rap cabo forma improvisado forma conocido batalla freestyle ejecutant llamado freestylers mcs competir s medir capaz ejecutar mejor verso edwards registro batalla freestyle datar decada unidos trazar paralelismo contrapunto forma payada tpico cultura gauchesco ro plata ejecutant medir capacidad improvisar verso registro principio siglo xix genero crecer particularmente pase hispano disputar nivel internacional llamado red bull batalla gallo reune maximo exponente freestyle espanol ultimos ano establecer liga nacional espana argentina mexico chile peru formato variado tiempo participante medir encuentro rounds regla particular jurado evaluarar estrofa puntaje punto funcion calidad rima adecuar regla round particular punto extra factor tal puesta escena formato clasico round llamado easy mode hard mode freestyler minuto elaborar estrofa incluir palabra generar aleatoriamente segundo respectivamente conocido deluxe consistir segundo participante elaborar estrofa tematica libre adversario responder inmediato estrofa dicultad generacion texto estructurado mencionada seccion generacion texto batalla freestyle espanol dicultad particular encontrar presente genero base dato contener transcripción batalla freestyle ta letra improvisado momento frecuente ejemplo contener poema letra canción genero ultir texto soler publicar obra batalla freestyle pensado ver vivo disponible mayora formato audiovisual capitulo procesamiento lenguaje natural generacion texto espanol implicar solución implementada marco generacion lenguaje natural ingl trasladar él trabajo facilmente caso ejemplo representacion palabra usar embeddings texto ingl utilizacion diccionario rima ingl identicacion subgenero surgido inicialmente ingl resultar incluir anglicismo palabra idioma freestyle espanol deformación espanolización dicho palabra perteneciente jerga especca comunidad implicar siquiera solución implementada generacion texto espanol poder trasladar él facilmente trabajo caso querer utilizar embedding probable realizar base palabra contemplir caso mencionado palabra espanol tradicional diseno algoritmo identicar rima espanol har tener vocal palabra fonetica palabra corresponder escritura incluir palabra ingl dejar suceder ejemplo palabra ingl beat utilizado contexto batalla freestyle pronunciar bit esperar algoritmo mencionado identique correctamente rima palabra formato round tomado referencia generacion texto genero particular implicar estrofa generado responder estrofa utilizar palabra especca formular respuesta adecuado estrofa responder as forma incluir palabra particular resultar problema trivial as genero presentar dicultad particular mencionado genero competitivo conjunto llevar cabo utilizar improvisacion tiempo real constituir motivación resultar interesante desarrollo sistema capaz generar texto metrica evaluacion metrica utilizado evaluar modelo lenguaje dierir signicativamente lizada modelo tpico machine learning tpicamente determinar forma binario numerico correctamente modelo eligio palabra frase razon generacion lenguaje natural continuo campo evaluacion experto representar factor clave surgir metodo buscar evaluar forma automatico modelo generacion lenguaje popular conocido generaci on texto estructurado bleu presentado papineni et metodo pensar inicialmente evaluar modelo dedicado traduccion texto contemplar texto generado texto referencia caso traducción traduccion exacto texto generado similar caso generacion texto nes artstico verso poetico caso trabajo estilo freestyle contar texto referencia buscar generar texto distinto cualquiera contenido conjunto dato tipo trabajo soler factor evaluacion humano ver zugarini et buscar generar verso similar libro divina comedia evaluan resultado funcion to tratar distinguir verso original generado mencionado yi et entrenar modelo generar poesa tpico chino realizar evaluacion experto puntuan verso generado funcion distinto criterio uidez coherencia trabajo relacionado seccion analizar trabajo relacionado presente reerar generacion texto estructurado revisar trabajo reciente enmarcar area lugar resultar necesario mencionar arte actual reerar generacion lenguaje natural alcanzar modelo presentado brown et modelo general lenguaje disenar resolver tarea especca poder traduccion texto interactuar modelo trav consola texto describir utilizar lenguaje natural sumo ejemplo tarea esperar realizar as ejemplo especicar él generara artcu él noticia resultar difcil distinguir evaluador humano artculo real brown et encontrar trabajo branwen modelo generacion texto estructurado tal dialogo poés gura mostrar poema generado forma caso modelo capaz reproducir forma aceptable estructura necesariamente respetar esquema rima resultar importante mencionar entrenar base dato common base dato billon palabra mayo base dato texto estructurado disponible momento desarrollo trabajo acceso modelo libre poder evaluar capacidad dominio especco trabajo batalla freestlye continuacion describir trabajo desarrollar especcamente generacion texto estructurado listar lau et buscar generar texto poetico especcamente formato soneto ingl texto generado consistir linea ajustar esquema rima especco esquema capitulo procesamiento lenguaje natural figura poema extrado brown et generado modelo acentuacion llamado pentametro lugar diseno modelo lenguaje compuesto red neuronal recurrente entreno soneto extrado base dato proyecto gutenberg modelo llamado modelo pentametro puntuar linea funcion ajustar esquema acentuacion modelo lenguaje entrenar generacion inversa texto proceso descrito seccion proceso generacion consistir utilizar modelo lenguaje generar linea candidata utilizar resultar adecuado modelo pentametro continuacion seleccionar esquema rima azar aaaa abab utilizar modelo lenguaje generar ultima palabra linea caso corresponder él esquema rima buscado generar palabra palabra nal cumplir esquema buscado modelo lenguaje completo generacion verso poema generado modelo presentar evaluador experto valorar positivamente metrica rimo soneto escrito humanos resultar facil distinguir poema generado modelo presentado utilizado entrenamiento debio principalmente falta impacto emocional legibilidad lau et idioma distinto ingles encontrar trabajo yi et utilizar modelo red neuronal recurrente generar poesa clasico chino camente quatrain cuarteto compuesto verso compuesto caracter chino cuarto opcionalmente rimar caso utilizar modelo distinto compuesto red nal recurrente entrenar poema clasico chino llamado word poetry module wpm entrenar generar verso palabra sentence poetry module spm buscar generar verso verso context poetry module cpm objetivo generar verso anterior buscar captar patrón rima cuarteto proceso generaci on texto estructurado generacion consistir utilizar palabra ingresado usuario entrada wpm encargar generar linea cuarteto utilizar entrada spm generar linea line utilizar entrada cpm generar ingresar cpm generar cuarto ultima line poema generado modelo someter evaluacion humano experto der puntuar él poema escrito humano funcion criterio tal coherencia signicado uidez caracterstica poetica evaluacion obtener resultado positivo verso generado obtener general menor puntuacion escrito humanos distancia signicativo modelo obtener puntaje generado modelo propuesto anteriormente gura incluir par poema generado modelo extrado yi et figura figurar extrado yi et par verso generado modelo presentado trabajo zugarini et buscar generar poesa italiano replicar trabajo autor divina comedia dante alighieri entrenar modelo neuronal recurrente especcamente red lstm trabajar tokenizacion nivel slaba agregar tokens especial representar comienzo nal terceto conjunto verso lugar preentrenar modelo utilizar conjunto texto publico italiano busca modelo aprendiero sintaxis gramatico idioma entrenar él utilizar especcamente divina comedia proceso generacion terceto consistir generar slaba token comienzo terceto generar token nal terceto alcanzar lmite slaba as generar terceto puntuan funcion caracterstica conocido autor cantidad slaba terceto particular compuesto endecaslabo capitulo procesamiento lenguaje natural verso slaba esquema rima deber encadenar ar utilizacion palabra encontrar vocabulario divina comedia trabajo mencionado texto generado sometio evaluacion humano buscar distinguir terceto generado modelo autora dante caso terceto generado percibir escrito humano mitad realmente escrito dante unico trabajo encontrar generacion texto estructurado espanol ghazvininejad et desarrollado ingl mencionar enfoque utilizado trasladable idioma espanol trabajo presentar sistema capaz generar poesa tematica particular ingresado usuario componer vocabulario palabra relaciono patron acentuacion indicar slaba acentuado dar tematica particular obtener lista palabra relacionado utilizar modelo mikolov et entrenado millar millon caracter wikipedia separar palabra clase rima conjunto palabra rimar seleccionar par rima forma aleatorio posible par utilizar automata nito aceptar posible secuencia palabra vocabulario ajustar estructura deseado nalicen palabra par rima obtenido secuencia cumplir estructura estrofa buscado mayora sentido nivel semantico seleccionar verso efectivamente sentido utilizar modelo lenguaje compuesto red neuronal recurrente entrenado letra canción ingl utilizar puntuar posible camino encontrar trabajo tratar generacion texto especcamente batalla freestyle potash et presentar modelo lenguaje compuesto red neuronal lstm generacion letra rap entrenar canción palabra caso aplicar ningun tipo restricción especca esquema rima metrico deber seguir texto generado modelo denir verso estrofa trav generacion tokens especial indicar ambos cosa respectivamente trabajo evalua lograr generar forma efectivo letra replicar estilo artista distinto trabajo listado notar desarrollar trabajo respectar generacion texto estructurado utilizar modelo neuronal mayora apuntar generacion texto ingl genero clasico disponer cantidad texto caso zugarini et yi et diferencia trabajo actual centrar generacion espanol genero escaso cantidad texto disponible caso listado n proyecto centrar replicar efectivamente genero artstico elemento s resultar necesario considerarlo hora batalla freestyle necesidad responder verso ultimo instancia posibilidad medir él ejecutante captulo desarrollo captulo describir desarrollo trabajo consistir sistema generacion lrico enmarcar genero freestyle respetar estructura rima principal aporte trabajo siguiente base dato texto contener letra canción genero rap similar compuesto transcripción batalla freestyle exclusivamente descrita seccion modelo lenguaje neuronal entrenado base dato listada racion texto genero detallado seccion sistema generacion lrica freestyle incluir algoritmo trabajar resultado modelo lenguaje garantizar texto generado respetar estructura rima requerido genero presentado seccion seccion describir trabajo preprocesamiento necesario dato obtenido usar él entrada modelo neuronal utilizado conjunto tarea analisis contenido dicho dato generacion base dato competencia freestyle fenomeno relativamente reciente competencia internacional ocial espanol realacer ano sumado naturaleza improvisacion genero conjunto ejecucion soler acompanado puesta escena formar gestualidad pronunciacion interaccion publico resultar esperable mayora material disponible genero encontrar formato audio comunmente audiovisual cantidad tipo material grabación audio video batalla freestyle acceder trav internet material disponible formato capitulo desarrollo texto transcripción necesario entrenar modelo lenguaje capaz generar tipo texto buscado principio disponar base dato texto contener mente material dominio buscado mencionar parrafo momento realizacion trabajo encontrar material internet forma abundante considerar apropiado utilizar tecnica preentrenar modelo material similar facil acceso cantidad desarrollo trabajo generar base dato contener letra canción rap genero similar consistir unicamente transcripción batalla freestyle trav entrenamiento modelo capaz obtener noción acerca estructura general lenguaje espanol acerca cuestión especca estructuracion texto estrofa posiblemente rima corresponder exactamente transcripción batalla freestyle letra genero similar compartir medida aspecto tal vocabulario rima utilizado considerar patrón aprendido aspecto resultar utilidad aspecto dierir lrica utilizado batalla freestyle plo tpicamente lrica cancion estructuran distinto parte tal estrofa estribillo repetir cosa suceder batalla freestyle lrica improvisado repetirar teora estrofa base dato conformar letra obtenido realizar web scrapping pagina perl artista relevante genero finalmente base dato generado letra canción tas idioma espanol contener line totalidad palabras gura mostrar estrofa extrada base dato figurar porcion letra cancion rapero dani includar base dato contener canción genero base dato consistir unicamente transcripción batalla freestyle utilacer entrenamiento denitivo modelo lenguaje narlo base dato descritar dato utilizado generar base dato generaci on base dato resultar difcil obtener cantidad texto contener cativamente menor base unico fuente transcripción batalla freestyle encontrar momento trabajo wiki dedicado exclusivamente transcribir lrica genero transcripción encontrar incompleto obtener transcripción do tecnica web scrapping completar manualmente material audiovisual respectivo batalla encontrado youtube agregar transcripción obtenido forma manual material audiovisual batalla trada plataforma completar base dato totalidad transcripción compuesto line palabra gura mostrar estrofa extrada base dato figurar porcion transcripcion batalla freestyle rapero wos cachaber marco torneo conocido freestyle master serie fms obtener originalmente trav tabla mostrar descripcion general base dato encontrar publica acceder repositorio presente tesinar minuto capitulo desarrollo bd lrica canción bd batalla freestyle documento linea palabra tabla resumen contenido base dato generado preprocesamiento analisis dato previo utilizar texto obtenido conjunto dato entrenamiento modelo realizar tarea preprocesamiento analisis lugar deber depurar texto remover expresión formar letra tal indicación repeticion algun verso bis especca cancion coro cantante verso particular analisis palabra presente texto validar aspecto batalla freestyle descrito seccion describir continuacion subseccion probar distinto metodo tokenizacion analizar viabilidad conjunto dato disponible analisis vocabulario analisis vocabulario consistio lugar descartar llamado labra vaca conocido ingl stop words luhn palabra tal pronombr artculo preposición soler frecuente texto idioma particular soler tener trabajo nlp rajaramar and ullman realacer tokenizacion nivel palabra tabilizar tokens aparición totalidad conjunto dato especco transcripción batalla freestyle generar histograma recortado correspondiente conjunto dato observar gura respectivamente lista permitir observar har mencionado seccion palabra ingl tal ow freestyle baby deformación palabra as pa reemplazo palabra inclusion onomatopeyas caso yeah ey factor presente letra genero batalla freestyle mente necesariamente momento considerar buscar generar rima fonetica palabra ingl analizar forma distinto palabra espanol momento utilizar tecnica transfer learning embedding preentrenado probablemente considerar tokens correspondiente deformación palabra onomatopeya preprocesamiento an alisis dato b figurar palabra cantidad aparición totalidad conjunto dato exclusivamente transcripción freestyle capitulo desarrollo tokenizacion considerar forma posible tokenizacion nivel palabra nivel subpalabra nivel slaba utilizar algoritmo byte pair encoding descrito seccion tokenizacion nivel caracter motivo resultar complejo construir verso rimar jar caracter nal jar palabra unidad semantica valida resultar inter embeddings ende utilizacion transfer learning trav considerar haber trabajo utilizar tipo tokenizacion mayora trabajo nlp actualmente inclinar tipo tokenizacion nivel palabra tokenizacion nivel palabra primero opción cuestion facilidad implementacion correctamente obtener conjunto dato tokens palabra distinto presento problema aparecer frecuencia conjunto dato tokens distinto aparecer unico equivaler considerar aparecer numero ascender evalua situacion conjunto transcripción batalla freestyle exclusivamente problema conjunto dato tokens palabra distinto aparecer unico aparecer sumo situacion observado cantidad tokens resultar problema signicativo probable modelo lenguaje utilizado obtener informacion util conjunto dato acerca representar utilizar palabra representado tokens frecuente implicar signicativo conjunto dato aprovechar él comun trabajo area nlp generacion conjunto entrenamiento modelo tokens aparecer frecuencia menor umbral determinado permitir reducir tamano vocabulario descartar elemento posiblemente representar informacion inutil porcentaje tokens unico presente conjunto dato mencionado descartarlo signico dejar porcion signicativo conjunto entrenamiento nal buscar generar ejemplo par entrenamiento compuesto sentencia tokens utilizar predecir sexto token objetivo conjunto dato generar par distinto descartar contener tokens aparecer unico resultar descartar par descartar contener tokens aparecer sumo implicar ignorar par cantidad par entrenamiento original exclusivamente base dato compuesto transcripción batalla freestyle generar par forma descrito descartar ignorar contengar tokens aparezcar unico preprocesamiento an alisis dato hacer aparecer sumo mitad conjunto entrenamiento obtenido tabla resumir observación mencionado problema tokens bajo frecuencia tipo tokenizacion permitir concluir tokenizacion nivel palabra podra ideal problema conjunto dato particular bd lrica canción bd batalla freestyle cantidad porcentaje cantidad porcentaje tokens tokens aparecer unico tokens aparecer sumo par to secuencia par descartado poseer kens frecuencia par descartado poseer kens frecuencia menor tabla resumen observación realizado cantidad tokens baja frecuencia sumo cantidad par entrenamiento generado incluir utilizar tokenizacion nivel palabra posible razón considerar causar problema cantidad palabra frecuencia aparicion lugar tamano conjunto dato utilizado considerar base dato contener totalidad canción genero contener cantidad considerable momento comparar él conjunto dato utilizado entrenamiento modelo lenguaje proposito general soler compuesto cantidad texto considerablemente totalidad wikipedia cantidad texto periodstico accesible trav internet base dato exclusivo batalla freestyle mencionar anteriormente obtener dato resultar aspecto trivial signico base dato signicativamente menor cualquiera caso esperar aumentar tamano conjunto dato habrar mejora problema puntual punto posible error ortografa distinto forma escribir expresión onomatopeya listado palabra utilizado mostrado seccion observar presencia token yeah yeh podrar caso rereir él expresion utilizacion marcado token pa reemplazo palabra capitulo desarrollo palabra utilizar texto encontrar token pa apostrofe nal utilizado signicado resultar frecuencia palabra quede repartido tokens distinto necesario considerar naturaleza genero puntualmente batalla freestyle desafo particular consistir incluir verso improvisado palabra dado justamente palabra ajena contexto batalla utilizar frecuentemente resultar complejo ejecutante incluir él podra resultar potencialmente aparicion tokens frecuencia bajo conjunto dato corresponder palabra incluido forzosamente ejecucion tokenizacion nivel slaba forma tokenizacion utilacer nivel slaba forma nizacion complejizar seleccion tokens rimar s podra tokenizacion nivel caracter permitio obtener mejora problematica tokens frecuencia signico solucion problema someter totalidad conjunto dato tipo tokenizacion obtener tokens distinto aparecer aparecer sumo utilizar unicamente conjunto dato transcripción batalla freestyle obtener tokens distinto aparecer unico texto aparecer maximo conjunto dato ver mejora porcentaje tokens bajo frecuencia tokenizacion nivel palabra valor signicativamente alto cantidad par generar descartar tokens numero mejorar signicativamente generar par forma describio seccion dato ventana deslizante tamano generar secuencia logicamente cantidad observada tokenizar nivel palabra doble secuencia contener tokens aparecer unico contener tokens aparecer sumo respectivamente analisis base dato contener exclusivamente transcripción obtener par posible contener tokens aparecer unico contener tokens frecuencia sumo numero obtenido mostrar mejora signicativo considerar utilizar tokenizacion nivel slaba tamano secuencia considerar predecir token deber utilizado tokens representar palabra completo caso brindar informacion tener tokens obtenido tokenizar conjunto dato nivel slar obtenido tokenizar él nivel palabra considerar token representar palabra traducir promedio tokens nivel slar exacto considerar factor preprocesamiento an alisis dato tokenizacion nivel slar utilizar token especial necesario tokenizar nivel palabra espacio blanco resultar adecuado comparacion analizar utilizacion tamano secuencia representar informacion secuencia tokens nivel palabra utilizar secuencia conjunto dato generar par entrenamiento contener tokens unico aparicion contener tokens sumo aparición trabajar exclusivamente transcripción batalla freestyle par contener tokens frecuencia contener tokens frecuencia sumo respectivamente observar resultado utilizar tipo tokenizacion decision adecuado tokens bajo frecuencia secuencia contener generar par entrenamiento modelo tabla resumir observación realizado tipo tokenizacion bd lrica canción bd batalla freestyle cantidad porcentaje cantidad porcentaje tokens tokens aparecer unico tokens aparecer sumo par to secuencia par descartado poseer kens frecuencia par descartado poseer kens frecuencia menor par to secuencia par descartado poseer kens frecuencia par descartado poseer kens frecuencia menor tabla resumen observación realizado cantidad tokens baja frecuencia sumo cantidad par entrenamiento generado incluir utilizar tokenizacion nivel slaba capitulo desarrollo tokenizacion utilizar bpe utilacer algoritmo byte pair encoding seccion permitir tokenizar texto identicar tokens aparecer frecuencia considerar poder resultar utilidad tener problema presentado forma anterior tokenizacion utilizar algoritmo denir tamano vocabulario deseado trav buscar obtener cantidad tokens distinto cercano tamano probar distinto tamano vocabulario valor cercano cantidad tokens obtenido tokenizar nivel palabra valor cercano cantidad tokens obtenido tokenizar nivel slaba caso lograr mejora signicativo reerar cantidad tokens bajo frecuencia notorio utilizar tamano vocabulario menor resultar esperable menor tamano vocabulario tokens soler pequeno cercano nivel caracter vocabulario volver complejo soler corresponder palabra completo gura ilustrar ejemplo resultar distinto nivel tokenizacion frase extrado conjunto dato utilacer token especial sep indicar nal verso objetivo modelo capaz aprender nalizar utilizar longitud ja figurar ejemplo tokenizacion frase extrado conjunto dato utilizar bpe distinto tamano vocabulario evaluacion algoritmo lugar ejecuto totalidad conjunto dato tamano vocabulario mencionado resulto tokenizador distinto conjunto tokens corresponder respectivo tamano vocabulario tokenizador conjunto tokens denido evaluo cantidad tokens aparecer unico aparecer sumo conjunto dato transcripción letra batalla respectivamente tokens denir totalidad conjunto dato razón mencionado anteriormente cantidad preprocesamiento an alisis dato tokens unico aumente evaluar algoritmo unicamente transcripción batalla freestyle resultar razonable gura mostrar porcentaje tokens aparecer unico aparecer sumo totalidad conjunto dato transcripción batalla freestyle respectivamente distinto nivel tokenizacion aplicacion bpe distinto tamano vocabulario mencionar anteriormente resultar esperable porcentaje bajo obtener utilizar bpe tamano vocabulario pequeno medida aumentar tamano valor acercar obtenido tokenizar nivel palabra slaba evaluarlo base dato contener exclusivamente transcripción batalla freestyle analisis par entrenamiento generar utilizar distinto tamano vocabulario forma tokenizacion utilacer criterio tokenizacion nivel slaba utilizar distinto tamano secuencia funcion cantidad tokens resultante tokenizar texto tamano vocabulario cantidad tokens tokenizar él nivel palabra utilacer tamano secuencia resultado analisis tamano vocabulario utilizado observar tabla totalidad conjunto dato transcripción batalla freestyle exclusivamente finalmente gura ilustran porcentaje secuencia contener tokens unico tokens frecuencia menor conjunto dato respectivamente esperar él forma analoga suceder caso porcentaje tokens bajo frecuencia porcentaje bajo cantidad par entrenamiento contener tokens bajo frecuencia obtener tokenizar texto utilizar bpe tamano vocabulario pequeno estadstica tokenizacion nivel slaba permitio obtener resultado porcentaje utilizar tecnica ron similar obtenido utilizar bpe vocabulario tamano comparar él conjunto dato signicativamente mejor obtenido utilizar bpe vocabulario mayor conjunto dato contener unicamente transcripción batalla freestyle capitulo desarrollo b figurar porcentaje tokens aparecer unico tokens sumo aparición tokens distinto totalidad conjunto dato exclusivamente transcripción freestyle aplicar tokenizacion distinto nivel preprocesamiento an alisis dato tamano vocabulario relacion tamano kenizacion texto cion nivel palabra largo secuencia par generado par contener tokens unico par contener tokens frecuencia porcentaje par contener tokens unico porcentaje par contener tokens frecuencia tamano vocabulario relacion tamano kenizacion texto cion nivel palabra largo secuencia par generado par contener tokens unico par contener tokens frecuencia porcentaje par contener tokens unico porcentaje par contener tokens frecuencia b tablar resumen analisis problema tokens bajo frecuencia cantidad par entrenamiento posible utilizar tokenizacion bpe mano vocabulario totalidad conjunto dato transcripción batalla freestyle capitulo desarrollo b figurar porcentaje par entrenamiento contener tokens aparecer unico tokens sumo aparición par entrenamiento totalidad conjunto dato exclusivamente transcripción freestyle aplicar tokenizacion distinto nivel modelo generaci on texto modelo generacion texto generacion texto utilacer modelo lenguaje consistir red neuronal recurrente compuesto capa utilizar concepto desarrollado captulo lugar red recibir entrada vector entero corresponder ndiz secuencia tokens vocabulario secuencia hiperparametro congurable red realizar experimento utilizar distinto valor combinacion distinto forma tokenizacion descrito sección siguiente dicho secuencia procesar entrada capa embeddings mapear ndice respectivo vector embedding descrito detalle seccion dimension hiperparametro red denido valor utilizado valor estandar trabajo nlp valor dicho vector ajustar entrenamiento red neuronal resto peso red figurar representacion graco modelo utilizado tamano secuencia secuencia vector resultante utilizar entrada capa lstm cional unidad cuyo arquitectura detallar seccion considerar importante capa bidireccional desarrollar seccion entrenar modelo utilizar secuencia texto orden inverso encontrar turalmente unidad lstm capa bidireccional procesen secuencia inverso serar realidad capaz captar dependencia orden correcto texto continuacion utilacer capa conocido dilucion dropout descrito capitulo desarrollo evitar sobreajuste modelo respectar modelo generacion texto traducirar generar unicamente texto similar existente conjunto dato lugar generalizar correctamente generacion texto capa salida modelo capa denso poseer tanto neurona tamano vocabulario experimento tamano vocabulario distinto deberar generar modelo acorde capa utilizar funcion activacion funcion softmax detallado seccion permitir intepretar salida red distribucion probabilidad distinto tokens vocabulario gura representacion graco modelo descrito entrenamiento modelo lenguaje utilizar modelo lenguaje descrito realizar experimento distinto tipo tokenizacion largo secuencia nivel palabra secuencia tokens nivel slaba secuencia tamano utilizar bpe vocabulario tamano secuencia tamano respectivamente utilacer bpir tamano vocabulario mayor mostro seccion tamano vocabulario volver signicar problema presencia tokens  bajo frecuencia utilacer tamano vocabulario tokenización obtenido tamano resultar tokens sencillo cercano nivel caracter mostro seccion entrenamiento modelo experimento generar conjunto dato base dato disponible tokenacer totalidad base dato reemplazo token correspondiente ndice ejecuto dicho secuencia ndices algoritmo ventana deslizante generar par entrenamiento x x vector correspondiente secuencia ndices predecir palabra representado forma vector tamano vocabulario conjunto generado utilacer conjunto entrenamiento conjunto prueba preentreno modelo conjunto dato generado canción realacer entrenandolo conjunto compuesto exclusivamente batalla freestyle funcion error utilizado entrenamiento modelo entropa cruzado categorico seccion utilizar lote dato tamano entrenamiento llevar cabo computadora local utilizar tambir plataforma colaboratory permitir ejecucion remoto deep learning hardware potencia hardware disponible localmente gpu nvidia gtx cor gb memoria cpu intel core cor gb memoria ejecutar sistema operativo windows hardware accesible trav google colaboratory entrenamiento modelo lenguaje consistir gpu nvidia gb nvidia gb varar ejecucion disponibilidad cpu cor gb memoria ram logicamente caracterstica harware entrenamiento modelo plataforma collaboratory resultar signicativamente rapido equipo local dicho plataforma presentar lmit tiempo totalidad recurso hardware razon utilizar ambos opción tabla presentar comparativo exactitud tiempo entrenamiento modelo entrenado parametro tokenizacion correspondiente experimento analisis tiempo requerido entrenamiento experimento llevado cabo plataforma google colab mencionar necesariamente menor llevado cabo localmente factor momento comparar caracterstica experimento llevado cabo plataforma distinto resultar interesante notar tiempo requerido entrenamiento modelo utilizado signicativo disponer localmente hardware especco gpu considerar momento cabo entrenamiento mencionar anteriormente plataforma tal colaboratory facilitar acelerar signicativamente proceso poseer lmite recurso ecacia obtenido experimento notar lugar utilizacion resulto efectivo experimento realizado preentrenamiento base dato contener canción rap genero similar mitio alcanzar cantidad epoca entrenamiento menor alto nivel ecacia base dato compuestir unicamente transcripción freestyle preentrenamiento consistio epoca experimento utilizar tokenizacion bpe vocabulario tamano alcanzar ecacia conjunto entrenamiento respectivamente modelo entrenado tokenizacion nivel slaba requerir epocas alcanzar nivel ecacia similar realizar epocas alcanzo ecacia modelo entrenado tokenizacion nivel palabra obtener ecacia entrenado cantidad epocas utilizado experimento tokenizacion utilizar bpe alcanzo ecacia epocas preentrenamiento experimento realizado bastar cuarto quinto cantidad epoca entrenamiento conjunto transcripción freestyle alcanzar ecacia logrado entrenamiento conjunto dato modelo experimento utilacer bpe tamano vocabulario alcanzar ecacia epocas pectivamente experimento utilizo tokenizacion base slaba obtener epoca entrenamiento ecacia modelo entrenado tokenizacion nivel palabra alcanzo ecacia entrenar epocas resultar llamativo diferencia ecacia obtenido conjunto capitulo desarrollo miento conjunto prueba experimento realizado diferencia presente experimento medida entrenamiento conjunto transcripción freestyle preentrenamiento conjunto canción genero similar comportamiento tpicamente asociar sobreajuste overtting modelo obstante situacion descritar subseccion acerca caso tokenizacion nivel palabra existencia cantidad tokens frecuencia aparicion conjunto dato causa directo probable tokens unico bajo frecuencia formar secuencia componer conjunto prueba resultar modelo aprender acerca tokens conjunto entrenamiento caso tokenizacion nivel subpalabra palabra bajo frecuencia corresponderan tokens componer secuencia aparecerar bajo cantidad resultar probablemente fenomeno creer cantidad dato entrenamiento resultar necesario lograr mejora resultado nivel tokenizacion palabra slaba tamano secuencia plataforma utilizado colaboratory colaboratory local local entrenamiento conjunto canción genero rap similar cantidad epocas tiempo promedio epoca s tiempo entrenamiento h ecacia accuracy to entrenamiento ecacia accuracy to prueba entrenamiento conjunto transcripción freestyle cantidad epocas tiempo promedio epoca s tiempo entrenamiento h ecacia accuracy to entrenamiento ecacia accuracy to prueba tabla comparativo exactitud tiempo entrenamiento modelo entrenado parametro tokenizacion generaci on texto estructurado generacion texto estructurado entrenar modelo utilizar sistema generador texto describir seccion texto inicial s llamado semilla repetir tanto tokens generar ingresar s secuencia entrada red obtener token forma aleatorio distribucion probabilidad generado modelo concatenar secuencia experimento generacion texto llevar cabo forma modelo entrenado lograr as generar texto replicar efectivamente estilo letra presente base dato factor capaz captar correctamente plasmar texto generado rima cantidad verso estrofa gura observar porcion texto generado forma descrito anteriormente enunciado modelo aprendio forma organizacion texto estrofas generar cantidad tokens salto linea consecutivo aparecer forma irregular dar lugar estrofas caso mostrado generar estrofa verso respectivamente debierir ja verso eventualmente garantizar estrofa generado cumplir estructura rima texto mostrado ejemplo contrar estructura rima estrofa generado forma abbb claramente caso deber suceder figurar porcion texto generado temperatura utilizar modelo trenado texto tokenizado utilizar bpe tamano vocabulario tamano secuencia texto modico capitalizar letra verso modelo entrenar unicamente caracter minuscula analizar caso puntual estrofa generado encontrar conjunto dato entrenamiento estrofa distinto utilizar palabra frase similar generada observar gura permitir suponer capitulo desarrollo modelo identico palabra encontrar comunmente proxima implicar generalizar aprender patrón rima b figurar estrofa extrada conjunto dato entrenamiento presentar similitud palabra estrofa generado gura tener observación realizado aspecto validar él correctamente texto medida generar decidar modicar algoritmo generacion garantizar texto cumplir modelo neuronal organizacion texto estrofos modicacion realacer nalidad garantizar estrofa tener cantidad verso adecuado cualquiera forma tokenizacion denida implementar control resulto trivial gracias utilizacion tokens indicar nal verso representado token sep tokenizacion utilizar bpe token salto linea forma tokenizacion permitir identicar modelo completo generacion estrofa simplemente contabilizar cantidad tokens n verso generado llegar modico algoritmo racion texto lugar permitir denir cantidad arbitrario tokens generar unicamente permitir denir cantidad estrofas contabilizar descrito gura ver ejemplo texto generado modicacion utilizar hiperparametro descrito obtencion texto gura claramente texto generado estructurado estrofa particularmente tomar generado esperar él aparicion verso rimar continuo situacion esporadico garantizar estrofa adecuar estructura rima particular generacion texto estructura rima garantizar verso cumplir estructura rima pondiente resultar problema trivial estrofa cumplir cantidad verso correspondiente identicar razón particular ilustran dicultad problema lugar identicar palabra rimar tarea trivial contabilizar token particular necesario contemplar caso posible generaci on texto estructurado figura texto generado modicacion algoritmo generacion agrupar adecuadamente estrofa acentuacion palabra conjunto regla particular lengua identicar respectivo vocal tonica tendrar resto caracter analizar tipo rima complejizar considerar aparicion palabra ingl expresión onomatopeya seguir regla correspondiente acentuacion ejemplo expresion yeah tomado ingl deber escribir él espanol yea caso contrario deberar poseer tilde palabra grave terminar n s vocal suceder tpicamente escrito hache nal tilde ingl modicar texto generado ajustar estructura rima particular necesario trabajar generacion ultimo palabra verso conllevar dicultad cantidad palabra slaba ja verso elegir él palabra continuacion forma aleatorio distribucion babilidad predicho modelo identicar ultimo palabra generado palabra token deber corresponder él token n verso sep buscar ajustar él estructura rima modicar ultimo palabra verso generar anterior lugar perdida coherencia texto generado tomar ejemplo verso extrado conjunto dato modelo haber generar frase muerdo leon estructura rima generado deber ajustar rimir verso particular terminacion distinto reemplazar ultimo palabra exclusivamente cumplir estructura rima probablemente generarar verso nal resultar coherente contexto ultimo palabra limito capitulo desarrollo medida posible candidata caso ejemplo deber animal capaz morder tener predicción modelo permitir considerar contexto palabra predecir palabra rime ejemplo elegir palabra rimir unicamente tokens alto probabilidad siguiente tomar vector predicción generado aumentar posibilidad elegir palabra cumplir estructura rima garantizar encontrar palabra adapte contexto rimir correctamente caso utilizar tokenizacion nivel palabra disponer forma computar palabra efectivamente rimar resultar relativamente facil analisis probabilidad aparicion palabra rimir particular simplemente analizar vector salida modelo lenguaje posicion corresponder palabra buscado complejizar utilizar tokenizacion nivel subpalabra trabajar nivel slaba utilizar bpe mayora caso bastar observar token particular disponer probabilidad aparicion vector salida modelo palabra rimir deseada deberan analizar predicción tokens siguiente futuro traducir costo computacional alto siguiente subsección desarrollar herramienta desarrollado modicación realizado algoritmo generacion superar dicultad enumerado efectivamente lograr texto generado adapte estructura rima deseado identicacion palabra rimar punto tratar cabo generacion texto determinado estructura rima identicacion presencia rima asonante consonante palabra mencionar seccion mayora trabajo dominio similar lengua inglés utilizar diccionario rima permitir identicar encontrar momento realacer trabajo herramienta similar lengua espanolo identicacion rima desarrollo script permitir discriminar slar vocal tonico palabra reconocer palabra rima consonante coincidir letra vocal tonico asonante coincidir vocal vocal tonico mencionar seccion expresión utilizado genero directamente palabra ingl expresión ajustar regla acentuacion espanol script desarrollado inicialmente capaz identicar correctamente rima intervengar vocablo subsanar problema denio diccionario expresión ingl encontrado frecuentemente texto asignar correspondiente castellanizacion adaptacion palabra norma idiomatica castellano ejemplo ow asignar ou freestyle fristail castellanización permitir palabra ingl identicar rimar generaci on texto estructurado palabra espanol gura mostrar recorte primero entrada denida diccionario figurar recorte primero entrada denida diccionario rima nalmente identicar palabra vocabulario rimar chequeir lugar ambos poseer entrada diccionario castellanización corresponder caso armativo reemplaza respectivo valor ejecutar script identicacion rima palabra resultante script descrito diccionario castellanización denido encontrar disponible repositorio generacion invertida texto disponer forma identicar palabra rima realacer modicacion algoritmo generacion texto pos generar verso rimir palabra particular modelo entrenado texto tokenizado nivel palabra evitar ma descrito punto almacenar distribución probabilidad producido modelo ultimos tokens generado generar token indicar nal verso dispondrar distribucion probabilidad genero token correspondiente ultimo palabra verso modicar distribucion probabilidad multiplicar factor probabilidad correspondiente tokens rimar palabra deseado utilizar distinto factor rima consonante asonante volver computar funcion softmax resultado corresponder distribucion probabilidad capitulo desarrollo generar palabra nal verso reemplazar distribucion probabilidad generado prioridad palabra rimar palabra deseado gura mostrar estrofas generado forma descrito marcar negrita palabra deber ajustar adapte esquema rima particular mostrar estrofa palabra reemplazar ajustar él esquema palabra original corresponder posicion ilustran estrofas esquema rima aaaa servir vericar enunciado punto verso generado forma cumplir tura rima denido palabra utilizado reemplazo palabra nal verso quedar general contexto apreciar practicamente palabra verso estrofa mostrado frase va ganar resultar coherente frase va afeitar contexto batalla freestyle aplicar verso estrofa dar clase sentir pupitre resultar coherente frase nal dar clase sentir mall frase tercer estrofa mostrado frase secuestrar completa afrodita resultar adecuado secuestrar completa imagina solucion problema descrito intentar generar texto rimir generacion inverso texto lugar entrenar modelo dar secuencia xi prediga palabra entrenar él predecir palabra forma generacion utilizado trabajo dominio similar tal yi et lau et solucionar problema mencionado punto permitir exactamente palabra predecir ultimo palabra verso aparicion token sep permitir redenir palabra riesgo generar incoherencia verso generar palabra elegido logicamente cabo tipo generacion deberar modicar forma generar conjunto dato utilizado entrenamiento modelo utilizar algoritmo ventana deslizante texto tokenizado generar par x x representar secuencia cuyo orden invertido orden texto original pi representar palabra dicho secuencia gura ilustrar quedar formado par entrenamiento texto forma mencionar seccion razon considero importante capa lstm modelo bidireccional secuencia par entrenamiento invertido orden natural lenguaje procesarla forma captarir dependencia secuencial texto capa bidireccional analizar secuencia forma inverso corresponderar orden real tokens texto capaz captar dicho dependencia generaci on texto estructurado figura ejemplo estrofa generado forma descrito modelo entrenado tokens nivel palabra secuencia temperatura gura mostrar forma gura aplicacion algoritmo generacion descrito comienzo seccion texto generado forma invertido observar palabra deber ajustar estrofa cumplir estructura rima representar problema coherencia verso resultar acorde verso palabra previo reemplazo sentido verso construir reemplazar palabra partir observar palabra reemplazar caso guardar relacion estrecho verso estrofa generar inmediatamente palabra tercer verso estrofa reemplazar palabra romano guardar relacion directo tematica verso demanda ajustar correctamente verso guardar relacion verso generar previo palabra caso situacion observar verso estrofa reemplazar muerto territorio computo probabilidad nivel subpalabra diccionario rima resultar util utilizar modelo entrenado texto tokenizado nivel palabra aplicar algoritmo descrito seccion utilizar nivel capitulo desarrollo figura ejecucion algoritmo ventana deslizante tamano generacion conjunto dato invertido orden texto tokenizacion nivel subpalabra tokenizacion slaba utilizar bpe descrito punto unico token correspondiente subpalabra brindar suciente informacion palabra formar rimo palabra dado token componer obtener version algoritmo funcionir nivel subpalabra generar conjunto palabra distinto presente ambos base dato diccionario rima genero computar palabra conjunto mencionado palabra conjunto rimar tipo rima consonante asonante logicamente generacion diccionario rima resultar computacionalmente costoso orden cuadratico palabra conjunto dato palabra distinto cabo unico permitir acelerar obtencion palabra rimar dada generacion texto version modicado algoritmo generar verso rimir palabra dado tener utilizar generacion invertida texto funcionar lugar obtendrar trav diccionario rima palabra rimar palabra dado palabra p tokenizar palabra p nivel tokenizacion utilizar generaci on texto estructurado figura ejemplo estrofa generado forma descrito modelo entrenado tokens nivel palabra secuencia temperatura entrenado secuencia texto invertido resultar secuencia tn obtener probabilidad generacion tn distribucion probabilidad producido modelo procesar secuencia previo obtener probabilidad generacion tokens haber él generar token agregar nal secuencia s token entrada modelo probabilidad mencionado corresponder ubicacion token distribucion probabilidad producido modelo posibilidad generacion palabra p regla cadena probabilidad capitulo desarrollo n j n computar almacenar diccionario utilizar p clave haber él generar diccionario contener palabra rimar palabra dado correspondiente probabilidad generacion computar distribucion probabilidad utilizar funcion softmax elegir palabra forma aleatorio funcion distribucion probabilidad generado palabra elegido efectivamente rimo palabra deseado elegir devuelta diccionario rima algoritmo generacion finalmente tener consideración descrito subsección anterior proceso generacion estrofa ajustar determinado esquema rima consistir ingresar texto utilizar semilla modelo texto tokenizar utilizar tokens denido momento ejecuto algoritmo bpe totalidad texto caso encontrar tokens presente vocabulario asignar token desconocido unk denir estructura rima forma aleatorio estructura tpicamente utilizado trabajo tomar estructura aaaa aabb abab abbar utilizar tokenizacion semilla entrada modelo predicción generar texto obtener token sep indicar nal verso palabra predecir ultimo palabra verso garantizar ajustar esquema rima mencionado caso suceder deber rimar palabra verso utilizar algoritmo descrito subseccion obtener palabra cumplir rima esperado repetirar completar verso acorde esquema rima estrofa gura mostrar estrofa generado usar algoritmo descrito anexo anexo incluir muestra texto generado generaci on texto estructurado figura ejemplo estrofa generado forma descrito modelo entrenado texto tokenizado utilizar bpe tamano vocabulario secuencia temperatura entrenado secuencia texto invertido observar ultimo estrofa generado estructura aaaa seguir estructura abbar abab respectivamente capitulo desarrollo captulo conclusión trabajo futuro captulo exponer principal conclusión obtenido desarrollo tesina presentar posible lnea investigacion futuro pos extender tema estudiado tesina conclusión general trabajo estudiar tecnica existente generacion lenguaje utilizar aprendizaje automatico especcamente deep learning desarrollar concepto esencial acerca metodo alcanzar arte reerar tarea nlp trav arquitectura recurrente tal red lstm descrita tesina profundacer idea fundamental reerar nlp tal tokenizacion algoritmo especco llevar él cabo byte pair encoding tecnica representacion documento word embeddings obtienar denicion modelo lenguaje puntualmente modelo lenguaje neuronal recurrente utilizar generacion tipo texto particular estudiar factor dicultad hora generar texto deber corresponder él estructura particular metrica rimo tener realacer estudio distinto trabajo cuyo problema enmarcar generacion texto estructurado mayora encontrar idioma ingl analizar solución presentar sortear dicultad presentado estudiar dicultad especca surgir trabajar genero freestyle espanol inclusion expresión ingl vocab él jerga genero onomatopeya respetar regla acentuacion idioma espanol utilizar concepto estudiado lograr desarrollar sistema generador texto capitulo conclusión trabajo futuro trav modelo lenguaje neuronal algoritmo generacion texto garantizar respete estructura rima correspondiente genere lrica enmarcar genero batalla freestyle necesario confeccion base dato texto contener especcamente transcripción batalla freestyle encontrar momento realizacion tesina base dato publicar contener tipo material genero base dato tamano contener letra canción genero similar utilizar preentrenar modelo lenguaje utilizado cabo realizar lugar tarea preprocesamiento texto permitir observar caso presente trabajo dato disponible entrenar modelo lenguaje principio escaso vocabulario diverso tokenizacion nivel palabra ideal concluyo observar caso cantidad tokens palabra componer texto aparecer bajo frecuencia caso tokenizacion nivel subpalabra mostro forma adecuado tokenizacion nivel slaba efectivamente reducir cantidad tokens bajo frecuencia texto tokenizado continuar representar porcion signicativo algoritmo byte pair encoding tamano vocabulario pequeno menor medida metodo tokenizacion eciente solucionar problematica resultar cantidad tokens aparición conjunto dato rondir entrenamiento modelo poder conrmar efectividad tecnica base dato generado pre entrenamiento base dato tamano poder lograr porcentaje precision base dato contener transcripción batalla freestyle cantidad epoca entrenamiento signicativamente menor concluir modelo lenguaje utilizado logro efectivamente captar estilo genero entrenamiento base dato desarrollado as correcto estructuracion estrofa verso correcto rima algoritmo garantizar texto generado cumplir caracterstica generar texto corresponder estilo estructura correspondiente genero batalla freestyle linea trabajo futuro identicar posible linea trabajo futuro continuacion presentado tesina implementacion sistema concordancia genero batalla freestyle capaz generar texto enmarcar genero capaz responder estrofa producido potencial contrincante necesario sistema evalue estrofo dada factor linea trabajo futuro clave elaborar respuesta resultar adecuado coherente internamente estrofa responder profundizar utilizacion tecnica conocido aprendizaje plo learning estudiar aplicabilidad problema desarrollado tesina particular permitir modelo aprendizaje generalicir ejemplo entrenamiento resultar adecuado base dato ble contener exclusivamente transcripción batalla freestyle relativamente pequén tecnica incluir ejemplo utilizado trabajo experimentar arquitectura red neuronal denir modelo lenguaje utilizar ejemplo conocido transformer arquitectura red neuronal recurrente utilizado recientemente trabajo area nlp puntualmente tratar problema conocido secuencia secuencia sequence to sequence tal traduccion resumen texto aumentar cantidad dato disponible entrenamiento delo cabo lugar simplemente ampliar tamano base dato desarrollado recoleccion transcripción canción genero similar as estudiar posibilidad cion tecnica generar dato encontrar disponible capitulo conclusión trabajo futuro bibliografa allwein et allwein schapire and singer reducing multiclass to binary unifying approach for margin classiers journal of machine learning research baldi et baldi brunak frasconi soda and pollastri ploiting the past and the future in protein secondary structure prediction bioinformatics bengio et bengio simard and frasconi learning denci with gradient descent is dicult ieee transactions on neural networks bird et bird klein and loper natural language processing with python oreilly medio inc edition branwen branwen creativir ction brown et brown mann ryder subbiah kaplan dhariwal neelakantar shyam sastry askell et language models are learners arxiv preprint chollet et chollet et deep learning with python volumir manning new york cybenko cybenko approximation by superpositions of sigmoidal function mathematics of control signals and systems dai et dai yang yang carbonell and salakhutdinov attentivir language models beyond context arxiv preprint deng and liu deng and liu deep learning in natural language processing springer devlin et devlin chang lee and toutanova bert training of deep bidirectional transformer for language understanding arxiv preprint edwards edwards how to rap chicago review press bibliografia ferretti et ferretti cagnín paiz donne  zacagnini and calde quality aw prediction in spanish wikipedia case of study with bility aws information processing management gage gage new algorithm for data compression c users journal ghazvininejad et ghazvininejad shi choi and knight nerating topical poetry in proceedings of the conference on empirical methods in natural language processing pag glorot et glorot borde and bengio deep spar él rectier neural networks in proceedings of the fourteenth international conference on articial intelligence and statistics pages jmlr workshop and conference proceedings goldberg and hirst goldberg and hirst neural network methods in natural language processing morgar claypool zitiert auf seitar goodfellow et goodfellow bengio and courville deep learning mit press grave grave supervised sequence labelling in supervised sequence lling with recurrent neural networks pag springer grave et grave liwicki fernandez bertolami bunke and midhuber novel connectionist system for unconstrained handwriting recognition ieee transactions on pattern analysis and machine intelligence grave et grave mohamed and hinton speech recognition with deep recurrent neural networks in ieee international conference on acoustics speech and signal processing pag ieee grave and schmidhuber grave and schmidhuber framewise phoneme classication with bidirectional lstm and other neural network architectur neural networks gutmann and hyvarinir gutmann and hyvarinir estimation new estimation principle for unnormalized statistical models in proceedings of the thirteenth international conference on articial intelligence and statistics pages hassen et hassen carvalho and chan malware tion using static analysis based featur in ieee symposium serie on computational intelligence ssci pag ieee hinton et hinton vinyals and dean distilling the knowledge in neural network arxiv preprint bibliografia hochreiter et hochreiter bengio frasconi schmidhuber et gradient ow in recurrent nets the diculty of learning dependenci hochreiter and schmidhuber hochreiter and schmidhuber long term memory neural computation hochreiter and schmidhuber hochreiter and schmidhuber lstm can solvir hard long time lag problems advanz in neural information processing systems pag hornik et hornik stinchcombe and white multilayer ward networks are universal approximators neural networks jurafsky and martin jurafsky and martin speech and language cessing draft chapter hiddir markov models draft of september retrieved march kalchbrenner et kalchbrenner espeholt simonyan oord ves and kavukcuoglu neural machine translation in linear time arxiv preprint lau et lau cohn baldwin brooke and hammond joint neural model of poetic language meter and rhyme arxiv preprint liddy liddy natural language processing luhn luhn key index for technical literature kwic index americar documentation mikolov et mikolov chir corrado and dean ecient mation of word representations in vector space arxiv preprint mohri et mohri rostamizadeh and talwalkar foundations of machine learning mit press montavon et montavon orr and muller neural networks tricks of the trade volumir springer nadkarni et nadkarni and chapman natural language processing an introduction journal of the americar medical informatics association papineni et papineni roukos ward and zhu bleu method for automatic evaluation of machine translation in proceedings of the annual meeting of the association for computational linguistics pag bibliografia pascanu et pascanu mikolov and bengio on the diculty of training recurrent neural networks in international conference on machine learning pag pmlr poo él et poo él mackworth and goebel computational gence potash et potash romanov and rumshisky ghostwriter using an lstm for automatic rap lyric generation in proceedings of the conference on rical methods in natural language processing pag radford et radford jozefowicz and sutskever learning to ratir reviews and discovering sentiment arxiv preprint rajaramar and ullman rajaramar and ullman mining of massivar dataset datar mining min massivir dataset rumelhart et rumelhart hinton and williams ning representations by errors naturir rumelhart et rumelhart smolensky mcclelland and hinton sequential thought process in pdp models parallel distributed processing explorations in the microstructures of cognition schuster and paliwal schuster and paliwal bidirectional recurrent neural networks ieee transactions on signal processing sennrich et sennrich haddow and birch neural machine lation of rare words with subword units arxiv preprint sutskever et sutskever vinyals and sequence to sequence learning with neural networks arxiv preprint uysal and gunal uysal and gunal the impact of preprocessing on text classication information processing management vajjala et vajjala majumder gupto and surán practical natural language processing oreilly medio inc werbo werbos backpropagation through time what it does and how to do it proceedings of the ieee yi et yi li and sun generating chinese classical poems with rnn in chinese computational linguistics and natural language processing based on naturally annotated big datar pages springer yin and shir yin and shir on the dimensionality of word embedding in advanz in neural information processing systems pag bibliografia young et young hazarika poria and cambrio recent trends in deep learning based natural language processing ieee computational intelligence zine zhang et zhang lipton li and smola divir into deep learning zugarini et zugarini melacci and maggini neural poetry ning to generate poems using syllabl in international conference on articial neural networks pag springer bibliografia apendizar muestra texto generado barrio pa montarte negro velar melon matar frase dar desenlace rima improvisacion habran ver color atraar entender pobre porquera pobre porquero pasar pistola metia cazador roncar viento plans ke merecer cara vencedor subir desibelio parecer criterioh amor promesa preferir mc universidad entender hacemir blando ladrón mirar ir amartar fracaso pidiir disculpar yo venir main interesado vo resucitada mirada ah yeah negro llego imperio dejar impero represente acar ofender querer gente as ventilo pa problema resolver apendice muestra texto generado ver jugada movimiento sador elegante salido fuego rap atuendo seguir pie quemar vo rapero seguir salvar rima triunfada idiota intentar nal obviamente hermano juro yendo paladarreado sentir frente desesionir hiphoper eh eh eh eh hiphoper estrenado acuerdate m copiar sentir rapeas decir acordas obvio subir involucra dime dime pasar eh wo ver paseo meloda ma modula sudor fresco recorrer él nuca tipo corrido dejar vida renglon faltar aprender creacion piel oscuro haber as ey permanecer querer billete perno querer querer oír montevideo ver yo encontrarno pelo mente tomo venir hablar ver pistola bala suceder gana amar gana amar bola billar ver aqu uyo monton pasar perd improvisacion ano criticar bajar carrera pensar apuro vacio mil familia dar dinero pa sirio dejar equis todava tanto salir escenario pesar durmiendo creer pobar lateir rima estilo querer ir yo ir vo rima plastica m interesar quieras contar m interesar pellan tocar liga reventar tematico namate s estrella inventar puro real imita vo vivistar contar cuento hip hop subirte tarimo cara roto bull suspiro preparo contenido loco batallar problema trueno banditito ow mo aprieta harar cosa ignorar él empezar asaltar partistir carajo s ensenar salio mote segu hermano batalla soltastar bombo pariante rocker pudistar vato tranquilamente perd poner hardcore distir gorra muestro placa weon as vo rey revuelto as asimilo decir querer querer irme ir improviso rapear junto representar sangre calculo fal tranquilo boludo rapear golpear rilo rapera escuche cabida ow roble rap vendo atuendo apendice muestra texto generado ow muevelo creer sensación estser aca fallar as acaparo estilo comparar warrior haber notar vengo barrio pesado rapero rapear poder sobramir trampa nal envidia mata ah dar ventaja dar caja so migaja serio quitar grada esquina vera ignorar sonabas diferencia poesa yeah entend importar dame rompe porton acertijo talento codiguito critico ocupar concepto invertir pasar sos vo representar chile pelear espartant cancion pastis entro reviento bombo ey piso piso piso trazo oh dem jugar fortnite detrozo s reconocimiento respuesta puesto sudar idiota intentar nal obviamente hermano juro invento agarrar microfono sentir cocodrilo tarimo dilatada preero confundir esperar problema andia hardcore perro tirar comienzo carrera vuelta beat ganar pierda huelo riquelme clavo angulo momento irte pa sirio ah conocer bomba uyo bombo normal esconda ay perdon miamar leyenda sucecer muerto lloron harto pegar patron jimmy neutron auto subo renglon s subir pulgar caro ow extranjero representado canot llorar vas perder él negro retroceder