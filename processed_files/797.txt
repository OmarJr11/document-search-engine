arán carlos working paper red neuronal recurrente anlisis modelo especializado dato secuencial serie documento trabajo 797 provided in cooperation with university of cemo aires suggested citation arán carlos 2021 red neuronal recurrente anlisis modelo especializado dato secuencial serie documento trabajo 797 universidad centro estudio macroeconmico argentina ucema aires this version is available at https://hdl.handle.net/10419/238422 standard-nutzungsbedingungen die dokumente auf econstor drfen zu eigenir wissenschaftlichen zwecken und zum privatgebrauch gespeichert und kopiert werden sie drfen die dokumente nicht fr ffentliche oder kommerzielle zwecke vervielfltigen ffentlich ausstellen ffentlich zugnglich machen vertreibir oder anderweitig nutzen sofern die verfasser die dokumente unter open-content-lizenzen insbesondere cc-lizenzen zur verfgung gestellt haben solltar geltir abweichend von dar nutzungsbedingungen die in der dort genanntir lizenz gewhrten nutzungsrechte terms of usar documents in econstor may be saved and copied for your personal and scholarly purpós you are not to copy documents for public or commercial purpós to exhibit the documents publicly to make them publicly available on the internet or to distribute or otherwise usar the documents in public if the documents have been made available under an opir content licence especially creativir commons licenz you may exercise further usage rights as specified in the indicated licence       universidad cema   aires   argentino             serie    documento trabajo                           rea ingenierar       red neuronal recurrente anlisis   modelo especializado dato secuencial       carlos arán       junio 2021   nro 797                     www.cema.edu.ar/publicaciones/doc_trabajo.html   ucema av crdobar 374 c1054aap aires argentina    issn 1668-4575 impreso issn 1668-4583 lnea   editor jorge m. streb asistente editorial valerio dowding jae@cema.edu.ar      1     red neuronal recurrente anlisis   modelo especializado dato secuencial     carlos arán   mayo 2021       resumir   aprendizaje automtico rama inteligencia artificial ia   especializar inferir patrón conjunto dato utilizar   tcnica estadstica heurstica mtodo tradicional   aprendizaje automtico estn limitado capacidad procesamiento   dato bruto necesidad contar intenso trabajo previo   procesamiento inferencia aprendizaje profundo   clase mtodo aprendizaje automtico mltipl nivel   representacin solucionar dificultad componer mdulo   sencillo lineal generar sucesivamente representación nivel superior   ligeramente ms abstracta caracterstica desarrollado   nivel anterior empezar entrada bruto   artculo   analizar profundidad caracterstica aplicación red   neuronal recurrente rnn clase estructura aprendizaje profundo   especializar tratamiento dato tipo secuencial   caracterizar serie temporal estructura dato texto video   audio artculo presentarn analizarn profundidad   caracterstica aplicacin 3 tipo red neuronal profundo   especializado tipo dato red neuronal recurrente red   memoria corto plazo lstm red recurrente atencin       ingeniero industrial uba doctor ingeniera industrial cand profesor materia mtodo   cuantitativo ciencia dato negocio maestra direccin empresa made   ucema director posgrado ciencia dato big datar facultad ingeniera   universidad aires desempear consultor especializado inteligencia artificial   aplicación negocio industria     punto vista autor representar necesariamente posicin universidad cema    2     1 aprendizaje automtico aprendizaje profundo      algoritmo machine learning aprendizaje automtico basar aprendizaje   muestra dato patrón relación funcional distinto variable   mitchell 1 ofrecer definicin programa ordenador aprender   experiencia clase tarea t medida rendimiento p   rendimiento tarea t medido p mejora experiencia     tarea aprendizaje automtico generalmente describir trmino cmo   sistema aprendizaje automtico deberar procesar ejemplo ejemplo caso jerga     machine learning observacin medida valor variable   tipo cuantitativas relevar caracterstica variable tipo categrico   cualitativo general representar ejemplo vector     entrada   i vector valor variable      1.1 conjunto entrenamiento validacin medida performance     tarea clasificacin generacin caso precisin simplemente   proporcin ejemplo modelo producir salida correcto caso   clasificacin conocer valor variable objetivo caso utilizado entrenar   modelo poder comparar prediccin salida modelo valor   correcto modelo generacin dato momento entrenarlo   estaramos utilizar dato entrenamiento corroborar qu parecido    dato entrenamiento utilizado dato generado      general interesar cmo funcionar algoritmo aprendizaje automtico   dato ver determín cmo funcionar implemente   mundo real evaluar medida rendimiento utilizar conjunto   dato est separado dato utilizado entrenamiento sistema   aprendizaje automtico procedimiento cabo separar set original   subconjunto correr proceso asociado   entrenamiento modelo clasificacin generacin   slo corroboraremo   efectividad medirer performance tarea separacin correspondiente tarea   realizado subconjunto generado entrenamiento validacin   proceso respectivamente fundamental proceso aprendizaje automtico   clave radicar efectividad procedimiento validez capacidad   generalizacin modelo       3       1.2 tipo aprendizaje automtico     algoritmo aprendizaje automtico clasificar él rasgo   supervisado supervisado funcin   tipo proceso permitir   fase entrenamiento aprendizaje      algoritmo aprendizaje supervisado aprender propiedad til estructura   conjunto dato contexto aprendizaje profundo normalmente aprender   distribucin probabilidad gener conjunto dato explcitamente   estimacin densidad implcitamente tarea generacin dato   sinterizado algoritmo aprendizaje supervisado desempean   función clustering consistir dividir conjunto dato grupo   aglutinir ejemplo caracterstica similar separir grupo s   algoritmo aprendizaje supervisado realizar   proceso   conjunto dato caracterstica atributo caso   contar dato fundamental conocer valor variable objetivo jerga   disciplina soler denominar clase     rasgo aprendizaje supervisado consistir observar ejemplo   vector aleatorio intentar aprender implcita explcitamente distribucin probabilidad   propiedad interesante distribucin aprendizaje   supervisado consistir observacin ejemplo vector aleatorio   valor vector asociado aprender predecir normalmente estimar   funcin distribucin pobabildiad condicional   | trmino aprendizaje supervisado   origen idea objetivo clase proporcionar supervisor   mostrar sistema aprendizaje automtico aprendizaje   supervisado supervisor algoritmo aprender sentido dato   gua      forma comn describir conjunto dato observación dataset   matriz diseo matriz diseo matriz contener ejemplo   fila columna matriz corresponder caracterstica algoritmo   aprendizaje diferenciar funcin forma operar procesa dato contenido   matriz diseo          4         2 red neuronal      red neuronal artificial rna inspirar estructura neuronal biolgica   encontrar cerebro humano red contener capa organizado unidad   interconectado nodo rna utilizar principalmente tarea difcil   derivar restricción lgica forma explcito reconocimiento patrón anlisis   predictivo 2 modelo computacional rna desarrollar 1943   mcculloch pitts 3 neurocientfico lgico respectivamente proponer unidad   umbral binario modelo neurona artificial modelo matemtico unidad             funcin activacin caso funcin escaln heaviside umbral   xj seal entrada wj peso asociado j = 1.2..n n corresponder nmero   entrada salida unidad 1 suma est umbral 0 caso   contrario modelo llev rosenblatt 4 desarrollar red neuronal pionerar conocido   perceptrn     modelo actual rna consistir suma ponderado entrada sesgo   bia suma someter funcin lineal producir salida nodo   neurona ver él figura fig. 1       fig. 1 estructura nodo neurona rna     mencionar rna conjunto neurona organizado capas   analizar él grafo dirigido ponderado neurona nodo grafo   arista conexión salida neurona entrada capa   1    5     rna tipo perceptrn capa entrada capa oculto capa salida   ms capa oculto denominacin red cambio   convertir red neuronal profundo figura 2 comprobar organizacin   capa rna interconectado           figurar 2 red neuronal artificial multicapar         3 red neuronal recurrente rnn      red neuronal recurrente rnn clase aprendizaje profundo basado   trabajo david rumelhart 1986 rnn conocer capacidad procesar   obtener informacin dato secuencial anlisis vdeo subtitulacin   imgén procesamiento lenguaje natural pln anlisis msica depender   capacidad red neuronal recurrente diferencia red neuronal artificial   vistas asumir independencia dato entrada rnn capturar activamente   dependencia secuencial temporal     atributo ms definitorio rnn comparticin parmetro   compartir parmetro modelo asignarar parmetro nico representar dato   secuencia   podrar inferencia secuencia longitud variable    6     impacto limitacin observar él forma notorio procesamiento lenguaje   natural red multicapo tradicional red multicapo tradicional fallara crear   interpretacin lenguaje parmetro nico establecido posicin   palabra frase rnn serar ms adecuado tarea   compartir peso dato espaciado secuencialmente ejempl0o lenguaje   palabra frase           fig. 3 diagrama grafo cclico red neuronal recurrente imagen extrado 19         rnn generalmente aumentar arquitectura red multicapo convencional   adicin ciclo conectar nodo adyacente paso tiempo ciclo constituir   memoria interno red utilizar evaluar propiedad dato actual   dato inmediato tambin importante mayora red   neuronal convencional tipo perceptrn tambin llamado feedforward estn limitado   mapeo entrada salida 5 rnn mapeo   ejemplo traducir   ejemplo identificar voz utilizar grafo computacional representar mapeo    7     entrada salida prdida desplegar grfico cadena evento obtener   imagen claro reparto parmetro red ver figura 4             figurar 4 imagen extrado 5     ecuacin generalizado relación recurrencia s(t          indicar sistema depender paso tiempo indicado t -1    ecuacin reescribir él h(t           x(t   representar entrada instancia tiempo particular importancia h(t)es   representacin aspecto relevante secuencia entrada t         3.1 topologa red neuronal recurrente     depender problema tratar resolver red podr trabajar tipo   secuencia entrada salida ejemplo analicemos caso problema anlisis   serie temporal mercado red deber tomar entrada valor   acción ltimo das predecir valor n+1 variante seriar   caso   predecir valor das basndo él valor   ejemplo perspectiva red neuronal   recurrente elegir topologa adecuado definir topologa principal   sequence-vector sequence-sequence vector-sequence encoder-decoder     2   3    8     figura 5 mostrar topologa sequence-vector imagen mostrar capa neurona   recurrente desarrollado tiempo slo salida ltimo time step   tomado salida capa        .figura 5 topologa tipo sequence-vector imagen extrado 5       figura 6 presentar topologa sequence-sequence capa devolver resultado   procesado perodo tiempo usualmente red neuronal recurrente profundo   buscar comportamiento tipo sequence-vector desear acoplar capa tipo sequence- sequence ltimo capa tipo sequence-vector consecuencia estructura   salida red nico elemento calcular funcin resto time steps lote                       9       figurar 6 topologa rnn sequence-sequence imagen extrado 5           ltima arquitectura encoder-decoder 6].deriva unin sequence-vector   vector-sequence ver él figura 7 modelo encoder tomar   entrada secuencia generar ltimo instancia representacin vectorial   modelo sequence-vector modelo decoder tomar dicho   representacin vectorial entrada devolver resultado procesamiento   perodo tiempo modelo global ver él sequence-sequence longitud   secuencia entrada variar secuencia salida til   problema traduccin automtico frase idioma determinado   ms smbolo palabra homloga idioma tipo modelo   estn tener relevancia significativo campo procesamiento lenguaje natural   composicin automtico musico aparicin mecanismo   attention cuyo inclusin modelo result arquitectura transformer 7      10         figurar 7 topologa tipo encode-decoder imagen extrado 5           3.2 problema red neuronal recurrente     momento ver cmo funcionar celda memoria ms bsicas   celda rnn tpica tipo estructura problema llamado short-term   memory 8 problema deriva conocido problema asociado desvanecimiento   gradiente explicar red neuronal recurrente basar   funcionamiento procesado secuencia informacin agregar cmputo   elemento secuencia resultado anlisis elemento resultado   rnn funcin agregar procesamiento elemento secuencia   medida red procés ms elemento cadena problema recordar   informacin     problema surgir consecuencia proceso matemtico red   neuronal recurrente capaz entrenar él aro corpus algoritmo    11     propagacin atrs tiempo back-propagation through time 9 medida rnn   recibir elemento secuencia desarrollar time steps gradiente   error propagar atrs har percepcin multicapa clsico   suceder tipo red medida gradiente alcanzar capa   ms cercano input caso rnn perodo procesado decae   exponencialmente rnn ms sencillo capaz aprender patrón   extendido tiempo eficaz rango corto ejemplo secuencia   10 elemento     mitigar problema short-term memory aparecer tipo celda memoria   s capaz extraer patrón secuencia longitud celda ms complejo   conocer celda memoria corto plazo lstm sigla ingls long- short term memory         4 red memoria corto largo lstm      red neuronal memoria   corto plazo lstm tipo particular   rnn solucionar problema rnn asociado memoria corto plazo   desvanecimiento decaimiento gradiente explosin ver superar   dificultad celda tipo lstm figura 8 recuadro   punteado corresponder unidad red lugar   mostrar componente lstm izquierda   informacin procedente procesamiento dato asociado perodo utilizar   estructura dato caso tipo tensor arreglo dato ms   dimensión inferior entrada informacin unidad   derecha tensor salir informar unidad tiempo   rnn simple informacin diagrama predecir   palabra prdida superior derecho        12         figurar 8 arquitectura celda lstm imagen extrado 19         objetivo mejorar memoria rnn evento pasado entrenndolo   recordar importante olvidir resto lstm procesar versión   memoria selectivo social est superior versin ms local inferior   lneo tiempo memoria superior llamar clula abreviar c. lneo   inferior llamar h.      figura 8 introducir conectiva función activacin lugar   ver lnea memoria modificar lugar pasar unidad   tiempo estn etiquetado tiempo x   +   idea memoria eliminar   unidad tiempo x aadir unidad +   qu decir mirar   incrustacin informacin actual venir inferior izquierdo pasar capa   unidad lineal seguido funcin activacin sigmoidea indicar anotacin w b   s.   w b formar combinacin lineal s funcin sigmoidear neurona clsico     notacin matemtico operacin         4   5    13     utilizar punto central indicar concatenacin vector repetir   inferior izquierda concatenar lneo h ht   dato actual obtener h0   introducir unidad lineal olvido seguido sigmoidea producir f seal   olvido desplazar izquierdo figura salida sigmoidea   multiplicar elemento elemento memorio venir superior izquierdo elemento   elemento ejemplo x[i j elemento matriz multiplicar   y[i j elemento   ecuacin operacin representar as         sigmoid estn limitado cero resultado multiplicacin   reduccin valor absoluto punto memoria principal corresponder   olvido general conjuracin sigmoidear alimentar multiplicacin patrn   comn compuerta suave      contrastar suceder unidad aditivo memoria   dato llegar abajo izquierdo   pasar separado travs   capa lineal activacin sigmoidea funcin activacin tanh   mostrar figura 9   tanh significar tangente hiperblico         importante diferencia funcin sigmoidear tanh salida   valor positivo   negativo arrojar valor lugar slo escalar   dato entrada resultado aadar celda celda etiquetado +           figurar 9 funcin tanh tangente hiperblico   6   7   8)   9    14       despu lnea memoria dividir copia salir derecha copia pasar   tanh combinar transformacin lineal historia valor actual   convertir él lneo h inferior           concatenar entrada proceso repetir punto destacar aqu   lnea memoria celda pasar directamente unidad lineal memoria   desvirtuar olvidar unidad x aadir informacin dato actual   \+         habr   operación   matemtica       combinacin   lineal       escalamiento transformacin lineal        4.1 mecanismo attention     mecanismo attention presentar 2014 dzmitry bahdanauet   extensin mejorar rendimiento modelo encoder decoder tarea traduccin   automticar 11 encoder decoder convencional encoder representar vector   tamao fijo oculto informacin frase entrada palabra   autor rendimiento encoder decoder bsico decae rpidamente medida   longitud cadena entrada crecer mecanismo attention pretender solventar   precisamente problema tcnica conjunto vector generar lugar   vector dimensin fijo dicho vector seleccionar subconjunto cercano   trmino pretender traducir time step     10   11    15         figurar 11 arquitectura attention imagen extrado https://towardsdatascience.com/sequence-2-sequence-model-with- attention-mechanism-9e9ca2a613a       figura 11 ver él esquema arquitectura encoder-decoder   intentar generar t-simo palabra objetivo yt dar frase x1 x2 xt ver lugar   mantener vector oculto celda ltimo step encoder   considerar salida intermedio momento computar suma ponderado   dicho vector intermedio clculo permitir determinar qu palabra sern ms relevant   decoder step     propuesta basado mecanismo      visual attention proponer ao 2015 xuet et 12 objetivo   propuesta consista sistema capaz alinear imagen entrada producir   palabra describir salida arquitectura consista red convolucional   extraa caracterstica imagen continuacin red recurrente mecanismo   atencin salida palabra mecanismo attention focalizar él    16     elemento imagen quelar definir frase salida   corresponder dicho elemento      herarchical attention propuesta llego 2016 mano yanget 13   sistema clasificacin documento aplicar mecanismo attention   nivel arquitectura presentar mdulo encoder decoder + attention   aplicado nivel frase nivel palabra      transformer consecuencia ms interesante aparicin mecanismo   attention aparicin modelo transformer propuesto vaswaniet al.en 2017   14 arquitectura suponer nivel mejora campo procesamiento   lenguaje natural tipo sistema permitir alinear palabra secuencia   calcular representacin dicho secuencia ms preciso eficiente   anterior modelo         5   aplicación lstm   continuacin presentar aplicación   red recurrente   evidenciar manea contundente performance   aplicacin red ms   notorio presentndo él paper cientfico novedoso aplicación red   particular transform ver aplicación ms emblemtica     5.1 generacin automticar subttu él imgén   subtitulado automtico imgén tarea dar imagen sistema   generar pie foto describir contenido imagen 2014 producir explosin   algoritmo deep learning lograr resultado impresionante problema   aprovechar trabajo mejor modelo clasificacin deteccin objeto   fotografas 15     detectar objeto fotografas generar etiqueta objeto   paso convertir etiqueta descripcin frase inteligibl   sistema involucrado entrenir red neuronal convolucional profundo    17     deteccin objeto fotografa utilizar red recurrente lstm convertir   etiqueta frase coherente         5.2 traduccin automtico texto   modelo aprender traduccin palabra contexto modificar   traduccin soportar secuencia entrada salido variar longitud general   s. 16      18           5.3 generacin automticar texto manuscrito   tarea corpus ejemplo escritura generar   palabra frase determinado escritura proporcionar secuencia coordenada   utilizado bolgrafo crear muestra escritura corpus   aprender relacin movimiento bolgrafo letra generar   generar ejemplo aplicacin tcnica aprender   estilo imitarlo 17    19         5.4 generacin automtico partitura musical   literatura encontrar ejemplo tipo arquitectura aplicado caso   composicin musical ejemplo 2017 limet proponer sistema basado lstm   bidireccional blstm generar acompaamiento musical forma progresin   acorde dar melodo 18 arquitectura consista red bidireccional capa   lstm 128 celda red entrenar corpus 2252 partitura msica   moderno rock jazz pop etc. resultar 1802 canción representar nota   meloda tener 12 posible valor cont octava extra   acord nicamente usar triada entrenamiento llev cabo presentar   red grupo tiempo negro meloda comparar salida acord   correspondiente fase generacin iterativamente pas red cadena   tiempo concatenar acord salida progresin completo autor   comparar sistema modelo evaluar cuantitativamente matriz confusin    20     cualitativamente 25 oyente resultado mostrar precisin   modelo as tendencia usuario elegir composición frente   modelo                21       6 conclusión      modelo aprendizaje automtico clsico supervisado   supervisado exigir algn tipo control conocimiento caracterstica atributo medido   relevado intentar desarrollar modelo predictivo descriptivo conocer   representación realidad subyacente entorno anlisis expresado variable   asumir independiente s poder aplicar ms variado tcnica ajuste   estimar parmetro funcin arribar resultado buscado   prediccin agrupamiento etc. obstante caso   representación atributo presentar formato estructurado caso   fragmento texto imgén audio video caso representación   generado algortmica proceso generar abstracción nivel   particularidad consecuencia deseable    estn sumamente correlacionada   pxel imagen palabra texto nota musical sonido   archivo audio      precisar modelo cuyo configuracin encuentre preparado   lidiar representación nivel altamente correlacionada solucin   encontrar modelo redes neuronal artificial especial utilizar tcnica   aprendizaje profundo deep learning construido emular estructura conexión   sinptica cerebro humano arquitectura aprendizaje profundo utilizar forma   ubicua extraccin caracterstica anlisis patrón abstraccin dato   representacin sucesivo concatenado complejidad creciente demostrar   modelo rendir ms rpido actual tcnica anlisis ltima generacin   tarea aprendizaje supervisado supervisad     configuración   arquitectura red neuronal aprendizaje profundo poseer caracterstico   destacar conferir particular destacado utilidad      modelo aprendizaje profundo analizado artculo red neuronal   recurrente rnn sigla ingls especializado tratamiento informacin   presentado forma secuencial variable corte longitudinal definicin encontrar   lenguaje natural simblico seal audio vo hablado msico   video   serie temporal arquitectura red neuronal recurrente presentar   progresivo funcin aparicin escena cientfico mejora   aportar configuración preceder as arribar modelo ms   utilizado actualidad red memoria corto plazo lstm transformer   configuración sofisticado red recurrente sortear dificultad perdido     22     memoria optimizacin funcin objetivo inherente rnn clsica asegurar as   performance posicionar estndar actual arquitectita deep learning   especializado modelados informacin corte longitudinal artculo finalizar   parentacin aplicación emblemtica red recurrente tipo lstm        bibliografa     1 mitchell t. m. 1997 machine learning mcgraw-hill new york   2 b. yegnanarayana artificial neural networks phi learning pvt ltd 2009   3 w. s. mcculloch and w. pitts logical calculus of the ideas immanent in nervous activity the   bulletin of mathematical biophysics vol. 5 4 pp 115133 1943     4 f. rosenblatt  the perceptron probabilistic model for information storage and organization in   the brain psychological review vol. 65 6 p. 386 1958     5 goodfellow i. bengio y. courville a. deep learning mit press http://www   deeplearningbook.org 2016     6 yunghyun cho bart merrinboer caglar gulcehre dzmitry bahdanau fethibougar holger   schwenk and yoshua bengio learning phrar representationsusing rnn encoder-decoder for statistical   machine translation.arxiv preprint ar-xiv:1406.1078 2014     7 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidar ngomez ukasz   kaiser and illia polosukhin attention is all you need inadvancesin neural information processing systems   pag 59986008 2017   8 sepp hochreiter and jrgen schmidhuber long short-term memory.neuralcomputation 9(8):17351780   1997 sepp hochreiter untersuchungen zu dynamischir neuronalen netzen.diploma technische universitt   mnchar 91(1 1991   9 ronald j williams and david zipser gradient-based learning algorithms for recu-rrent.backpropagation   theory architectur and applications 433 1995   10 mike schuster and kuldip k paliwal bidirectional recurrent neural networks.ieeetransactions on   signal processing 45(11):26732681 1997    23     11 dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly   learning to align and translate.arxiv preprint arxiv:1409.0473 2014   12]kelvin xu jimmy ba ryar kiro kyunghyun cho aar courville ruslar sa-lakhudinov rich   zemel and yoshua bengio show attend and tell neural imagecaption generation with visual attention   ininternational conference on machinelearning pages 20482057 2015   13 zichao yang diyi yang chris dyer xiaodong alex smola and eduard hovy.hierarchical   attention networks for document classification inproceedings of the2016 conference of the north americar   chapter of the association for computationallinguistics human language technologies pag 14801489 2016   14 zichao yang diyi yang chris dyer xiaodong alex smola and eduard hovy.hierarchical   attention networks for document classification inproceedings of the2016 conference of the north americar   chapter of the association for computationallinguistics human language technologies pag 14801489 2016   15 show and tell neural image caption generator 2014 https://arxiv.org/abs/1411.4555   16 https://arxiv.org/abs/1409.3215   17 grave a. 2013 generating sequences with recurrent neural networks corr abs/1308.0850   18 hyungui lim seungyeon rhyu and kyogu lee chord generation from symbolicmelody using blstm   networks.arxiv preprint arxiv:1712.01011 2017   19 charniak  e. 2018 introduction to deep learning                