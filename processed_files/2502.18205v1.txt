modelo lenguaje conocido chatgpt inesperado revolucin mbito inteligencia artificial contar multitud aplicación prctica enorme potencial todava explorar tambin objeto debate punto vista cientfico filosfico social duda mecanismo exacto funcionamiento capacidad real comprensin lenguaje aplicación plantear dilema tico captulo describimos cmo llegar tecnologa fundamento funcionamiento permitindono as comprender capacidad limitación introducir principal debate rodear desarrollo palabra clave inteligencia artificial procesamiento lenguaje natural modelo lenguaje captulo iv modelo lenguaje prediccin palabra comprensin carlos parcialmente financiado proyecto financiado miciu gap financiado feder ue latching financiado feder ue financiado ministerio macin digital funcin pblico nextgenerationeu prtr as xunta galicia centro investigacin galicia citic financiado xunta galicia travs colaboracin consellera cultura educacin formacin profesional universidad sidad gallego refuerzo centro investigacin sistema universitario galicia cigus introduccin modelo lenguaje conocido sigla ingls llms large language models precipitar cambio paradigma procesamiento lenguaje natural pln rama inteligencia artificial buscar desarrollar programa poder comprender generar lenguaje humano clave xito modelo aprender directamente texto simple capaz responder mente tipo consulta sta expresabl respondibl texto incluir realizacin tarea traduccin peng et correccin matical fang et resumen texto pu et respuesta pregunta factual brown et escritura creativo williams paradigma anterior manning schtze manning et tarea requero disear entrenar ajustar sistema especfico incluir costoso proceso recogida anotacin dato especializado llms permitir llevarla cabo paragua sistema ajuste fico interfaz natural llevar millón usuario probar tecnologa integrar él vida tiempo rcord obstante modelo importante limitación principal siguiente mejor llms sistema computacionalmente costoso est propiciar oligopolio facto empresa tecnolgica bommasani et enfrentado modelo ms pequeo incluir cdigo abierto momento lograr cerrar brecha modelo respuesta sesgada directamente incorrecto llamar alucinacin fenmeno llm producir respuesta sintcticamente redactado coherencia interno sentido responder consulta alejar realidad sistema opaco contar mecanismo fiable minar qu estn proporcionar respuesta agravar punto modelo proporcionar informacin verificar puesta desaconsejable fiar él salida llm verificacin externo adems deber consciente limitación fruto inmadurez tecnologa incipiente resolver él prximo ao podran intrnseca inevitable xu et banerjee et asimismo funcionar correctamente llms plantear sos reto social cuestión tico derivado malintencionado ejemplo generar informacin falso manipular persona cometer fraude trabajo exmén sobreuso ejemplo sustituir decisión sensible deberar tomar persona captulo iv modelo lenguaje prediccin palabra comprensin comprender oportunidad creado tecnologa desafo plantear captulo explicaremos base funcionamiento llms clave tcnica xito anterior sistema pln ms importante pregunta abierto desafo enfrentar investigacin campo resto captulo estructurar seccin explicar qu llms revolucionario contextualizndolo resumen paradigma suceder historia procesamiento guaje natural posicionar avance suponer seccin explicar cmo funcionar llms seccin basar funcionamiento explicar capacidad limitación funcional llms ltimo seccin resumir conclusión captulo qu revolucionario mencionar modelo lenguaje llms provocar cambio paradigma pln situar adems campo investigacin lero qu revolucionario qu diferencia anterior trabajar lenguaje humano saber él convenir perspectiva histrico distinto paradigma suceder pln pln orgén aos siglo xx hito propuesta test turing sugera evaluar inteligencia mquina vs capacidad mantener conversacin forma indistinguible humano tiempo guerra fra despertar creciente inters posibilidad cir texto automticamente experimento hutchins mostr mundo sistema capaz traducir oración ingls ruso rudimentario titular grandilocuente electronic brain translat difundir pblico general idea traduccin automtico guido suceder predicción optimista segn problema estar resuelto definitivamente ao lingstica computacional definir disciplina cientfico cuyo aplicacin prctico pln empez recibir financiacin consiguiente logro avance tecnologa orgén actualidad podra dividir evolucin tecnolgico pln etapa segn venir ensear mquina trabajar lenguaje humano etapa abarcarar final aos pln basar exclusivamente regla escrito mano experto noam chomsky chomsky describir cmo sistematizar gramtica idioma ingls regla sintctica permitir generar interpretar oración basndo él principio desarrollar analizador sintctico capaz descomponer n componente gramatical facilitar extraer informacin estructurado texto construyen tambin sistema basado regla tarea traducir texto responder pregunta mantener conversacin conocido sistema elizar weizenbaum problema enfoque serio problema escalabilidad resultar costoso escribir regla manualmente problema idioma dominio aplicacin difcil capturar variación excepción lenguaje humano cambiante ambiguo final comenzar aparecer volver dominant tema basado aprendizaje estadstico necesitar regla enseir idioma mquina conjunto ejemplo algoritmo aprendizaje automtico aprender ejemplo entrenar traductor automtico utilizar corpus oración idioma origen traduccin idioma destino brown et tipo enfoque ms escalabl barato basado regla falta involucrar experto diseo sistema contar dato calidad ejemplificar famoso frase fred jelinek every time i fire linguist the performance of the system goes up hirschberg etapa calidad algoritmo aprendizaje automtico rar segn avanzar investigación especialmente irrupcin llamado dizaje profundo deep learning evolucin devolvi lneo olvidada red neuronal mitad dcada sistema aprendizaje automtico basado deep learning representar palabra espacio continuo tor denso mikolov et lugar entidad discreto entrada red neuronal aprender tarea cho et mejora cuantitativo precisin sistema logrado tecnologa cambiar limitacin fundamental etapa sistema propsito especfico llevar cabo tarea concreto entrenar red neuronal oración castellano traducción ingls lograr sistema traduccin automtico efectivo traducir texto castellano ingls intil idioma estn conjunto entrenamiento ms an tarea responder ta resumir texto necesitaramos entrenar sistema distinto conjunto dato ajustado tarea deseado as desear resumir texto castellano tener crear sistema cero entrenndolo corpus texto llano resmén as tarea querer acometer precisamente limitacin venir resolver tecnologa motivar captulo modelo lenguaje inaugurar an incipiente etapa desarrollo pln punto vista tcnico modelo guaje provenir escalado modelo neuronal etapa red ms gracias avance hardware dato entrenamiento ms arquitectura neuronal mejor transformers vaswani et punto vista ms amplio modelo provenir evolucin ruptura anterior suponer cambio paradigma perfectamente resumido captulo iv modelo lenguaje prediccin palabra comprensin ttulo artculo introducir modelo ms transformativo radford et podrar perfectamente considerar hito inaugurar etapa language models are unsupervised multitask learners palabra modelo lenguaje necesitar dato especializado tarea oración traduccin idioma texto versin resumido aprender manejar idioma cantidad gigantesco texto descargado internet fuente necesidad algn humano adapte tarea especfico unsupervised estn restringido tarea concreto sistema cabo variedad tarea pedir multitask suponer autntico revolucin prctica obviar necesidad dato especializado tarea idioma dominio aplicacin querer aplicar pln sistema propsito general chatgpt usuario idioma tipo solicitud dad aprender utilizar software especializado modelo lenguaje ms verstil universalmente accesible pln acercar superacin test turing cambio paradigma dar reciente aparicin vertiginoso velocidad suceder modelo avance suscitar pregunta especialista tema pblico general ejemplo cuestin debate modelo lenguaje realmente capaz entender lenguaje menor grado nicamente simular respuesta coherente entender luto cuestión actualidad modelo llegar creativo podran llegar adquirir consciencia serie debate tico social plantear torno fiabilidad posible malo uso generar desinformacin qu punto podran deberan sustituir humano rol ejemplo cuestión momento respuesta definitivo suscitar consenso entender debate evitar postura simplista necesario prender fundamento cmo funcionar llm describir seccin evitar deliberadamente tpico enfoque tcnico diseccionar componente realmente necesario har explicacin sible contingente garantizar ao tura neuronal funcionar llm radicalmente distinto actual gu dao centrarno esencial qu llm qu tener caracterstica describir ms cmo funcionar esencia llm concepto sencillo modelo dad masivo texto dato entrenamiento predecir continuacin plausible texto ejemplo modelo lenguaje pasar texto romper capaz continuar palabra coherente contexto mvil jarrn corazn llms basar to ver entrenamiento generar texto cadena markov comprender él ms detalle conveniente remontarno orgén pln matemtico ruso andrey markov descubrir modelo describir proceso estocstico conocido cadena markov cadena markov orden k modelo atravesar serie probabilidad depender k anterior ejemplo cadena markov podra asignar probabilidad tiempo har maana funcin tiempo ayer funcin tabla tabla tabla proporcionar probabilidad maana hacer soleado nublado lluvioso funcin tiempo ayer ejemplo ayer nublado est lluvioso tabla sexto fila probabilidad maana soleado nublado lluvioso ayer probabilidad maana soleado probabilidad maana nublado probabilidad maana lluvioso soleado soleado soleado nublado soleado lluvioso nublado soleado nublado nublado nublado lluvioso lluvioso soleado lluvioso nublado lluvioso lluvioso tabla cadena markov orden modelar tiempo atmosfrico captulo iv modelo lenguaje prediccin palabra comprensin mos consultar probabilidad combinacion da ejemplo necesitar da ayer estimar probabilidad tiempo maán cadena markov orden orden podra tambin superior inferior caso ejemplo probabilidad tabla inventado caso real basarir dato histrico ejemplo despu nublado lluvioso vino soleado pondrar correspondiente celda tabla probabilidad correspondern caber esperar dato histrico modelo generar secuencia estocstico abarcar tiempo maana tambin das siguiente ejemplo suponer ayer nublado est lluvioso generar tiempo plausible maán sorteo aleatorio segn probabilidad tabla probabilidad salir soleado nublado lluvioso suponer resultado sorteo lluvioso repetir proceso maán da anterior serar lluvioso lluvioso as probabilidad serar ltimo fila tabla soleado nublado lluvioso proceso repetir él indefinidamente generar das querar medida alejar futuro ms difcil prediccin modelo corresponder suceder realidad continuacin plausible tiempo registrado ltimos da concepto aplicar generar texto lenguaje humano siderir lugar condición meteorolgica corpus texto tabla probabilidad estimar fcilmente corpus contar aparición secuencia k palabras palabra seguir ejemplo buscar bigrama unido corpus bruno pequeo corpus milln palabra espaol interfaz web libre acceso fcil lector querer experimentar spanish concordancer bruno spanish corpus fijar qu palabra venir despu obtener tabla ver bigrama aparecer seguido norteamrica amrico asociado deducir referencia ee uu tambin aparecer palabra trabajar oracin compromiso unido trabajar pase vas desarrollo variedad palabra pequeo tamao corpus tuviser ms dato duda podrar observar ms palabra aparecer despu unidos estimación probabilidad serar tambin ms precisa realidad modelo lenguaje trabajar palabra dividir texto subpalabra tpicamente obtener algoritmo codificacin par bit gage diverso motivo tcnico incluir eficiencia flexibilidad trabajar palabra modelo ver simplicidad claridad exposicin explicacin rarar aspecto supondrer trabajar palabra tambin implementacin ms eficaz eficiente cambiar comprensin concepto fundamental generar texto cadena markov orden recopilaramos dato tabla bigrama corpus tendramos tabla exhaustivo fila bigrama continuacin tabla preferir tabla individual tabla combinacin bigrama proceso generacin sencillo empezar cualquiera corpus constituir pio texto generado repetir desear k ltima palabra generado mirar tabla posible palabra siguiente probabilidad aparezcar continuacin b sorteo elegir palabra siguiente utilizar dicho probabilidad posible palabra siguiente tendr babilidad salir indicar tabla c palabra seleccionado aadir texto generado as ejemplo bigrama inicial ser mujer algoritmo cin consultar palabra aparecer continuacin bigrama conjunto entrenamiento obtener resultado mostrar bloque superior tabla continuacin escogerar aleatoriamente palabra posible continuación probabilidad proporcional estimación tabla caso palabra as escogido ser texto generado mujer iteracin algoritmo consultar palabra aparecer continuacin mujer corpus entrenamiento obtener dato bloque inferior tabla ah wi frecuencia corpus probabilidad estimado unido norteamrico unido amrica unido trabajar tabla frecuencia probabilidad estimado obtener modelar palabra venir despu bigrama unido corpus milln palabra espaol nota notacin wi referencia palabra posición i texto captulo iv modelo lenguaje prediccin palabra comprensin gera palabra azar ejemplo quedar texto mujer proceso seguir iterar generar texto desear proceso estocstico generar texto lenguaje tabla mostrar resultado aplicar proceso biblia conjunto efecto valor texto as generado ciar lxico temtico bblico fuente modelo obtener palabra caracterstica sintctica semntica texto segn valor texto carecer coherencia bastante error sintctico llegu pecado adems sentido lgico aumentar valor k texto cobrar coherencia error sintctico desaparecer vase sintaxis est cerca correcto sentido fragmento texto conjunto carecer sentido lgico consistencia suficiente aumento progresivo coherencia disminucin error sintctico medida aumentar k cadena markov garantizar texto resulte plausible construccin texto aparecer conjunto entrenamiento wi frecuencia corpus probabilidad estimado mujer mujer mujer mujer mujer mujer mujer mujer mayor mujer tal mujer mujer mujer fundado tabla frecuencia probabilidad estimado obtener modelar palabra venir despu bigrama mujer mujer corpus milln palabra espaol santa biblia antiguo versin casiodoro reina revisado cipriano valera sión revisin disponible generator refs heads master podra pensar él alcanzar texto totalmente coherente bastara aumentar k necesario encontrarer problema agotamiento espacio muestral observar texto generado tabla poder efectivamente coherente problema copia literal fragmento conjunto entrenamiento stir suficientemente encontrer l muestra agua mar faltarn ro tabla posible continuación tendr opcin agotar babilidad modelo estocstico reproducir texto bblico lugar generar texto generacin texto cadena markov est lejos apariencia inteligencia atribuir texto generado llms dudoso aplicabilidad prctico limitación k pequeo texto coherencia k ms encontrar barrera escasez dato podrar pensar mitigar ltimo conjunto entrenamiento ms s alcance limitado probabilidad encontrar determinado texto disminuir exponencial k juntser texto escrito historia humanidad podramos ción probabilidad aceptable bastar pensar elegir secuencia k palabra consecutivo texto haber escribir ms probable valor k texto generado jehov har prevaricado animal templo isacar borde dejar dormir llegu pecado ron doble tiempo jehov vida cerrojo anunciar dio har agua salado sirvient jordn ngel parvaim venir m palabra jehov llar tierra crificio alabanza accin gracia luz hijo hija hermano hija sanar hora edific all altar jehov grosura sacrificio paz santificar completo acordar pacto jehov espritu mo est lomos descalzar sandalia pie l poner diestra m dicindome temar hiri levant campo joab est mar galilea ensear da adversidad agua mar faltarn ro agotar secar alejarn ro agotarn secarn corriente foso caa carrizo sern cortado pradera ro ribera ro sementero ro secarn perdern sern ms tabla generacin texto bblico cadena markov distinto valor k captulo iv modelo lenguaje prediccin palabra comprensin escribir caso concreto texto formulaico frase hecho texto legal conjunto entrenamiento seguro modelo limitar copiar literalmente limitación cadena markov atajar mos regla estricto imponer modelo hacindolo ms flexible cubrimiento cadena markov aparecer mejora incremental sentido tcnica suavizado chen goodmar interpolacin jelinek katz avance realmente permitirar desbloquear modelo generativo lenguaje valor k modelo lenguaje neuronal cadena markov modelo neuronal modelo lenguaje neuronal cadena markov modelo generar continuacin plausible texto observar entrenamiento lugar basar él puramente clculo bilidad condicional generacin producir red neuronal modelo poder generar texto ms flexible soportar as longitud contexto k ms larga explicar detalle funcionamiento modelo escapar objetivo captulo s caber destacar principalmente avance posibilitar lograr flexibilidad cadena markov carecar sentación denso lenguaje avance tcnico red neuronal disponibilidad enorme conjunto dato obtenido internet continuacin describir brevemente avance relevancia representación denso lenguaje representación denso lenguaje conocido ingls word embeddings vector numrico capturan significado palabra ubicarla espacio vectorial palabra significado similar estn cerca tpicamente vector representar palabra est formado ciento componente coma flotante aproximacin informtico nmero real popularizacin representación producir raz modelo mikolov et obtener vector palabra sencillo red neuronal ir aparecer alternativa pennington et incluir tación contextualizada vector palabra varar segn contexto aparecer devlin et representación denso lenguaje ventaja considerar palabra entidad discreto red neuronal adaptar trabajar vector denso nmero real entidad discreto principal ventaja flexibilidad aadir modelar forma natural similitud palabra cadena markov vista apartado as estadstico tradicional usado pln palabra representar entidad atmico relacin aparente dems modelo tipo palabra nico avin aeroplano cadena texto traducir interiormente algn tipo identificador numrico modelo tendr constancia ltima parecer ms s as cadena markov continuar texto perdi control aeroplano exacto aparecer conjunto entrenamiento s aparecer perdi control avin poder aprovechar similitud generar continuacin texto cambio modelo neuronal s podr lugar buscar coincidencia exacto palabra tabla modelo neuronal trabajar espacio continuo representación vectorial as aprovechar él fragmento texto similar igual ver entrenamiento proporcionar flexibilidad adicional frente rigidez cadena markov capacidad sacar ms partido dato avance tcnico red neuronal red neuronal modelo computacional inicialmente inspirado estructura funcionamiento cerebro humano diseado aprender relacin entrada salida deseado problema complejo estn formado unidad cesamiento llamado neurona organizado capa conectar red neuronal neurona recibir entrada realizar clculo transmitir resultado neurona capa travs conexión unir proceso entrenamiento conexión ajustar usar algoritmo optimizacin permitir red aprender patrón dato mejore tarea especfico procesamiento texto red neuronal conocido mediados siglo xx final siglo tecnologa olvidado sar solar opcin prctica mayora problema claramente superado alternativa mquina vector soporte sealar lecun et red neuronal medida abandonado comunidad aprendizaje automtico pensar ampliamente forma aprendar dato inviable resurgir ejemplo cmo inversin tigacin bsico aplicación claro vista clave avance cientfico dcada lograr importante mejora red neuronal serie descubrimiento posibilitir llamado aprendizaje profundo goodfellow et enfoque transformar campo inteligencia artificial gracias avance algoritmos capa red neuronal ms profundo implementacin tcnica regularizacin optimizacin as aumento capacidad cmputo captulo iv modelo lenguaje prediccin palabra comprensin facilitado gpu red comenzar alcanzar rendimiento notable tarea complejo incluir procesamiento lenguaje natural goldberg hirst inicialmente red neuronal usado pln arquitectura da problema adaptar procesar lenguaje goldberg hirst avance supondrar pistoletazo salida modelo lenguaje neuronal ms evolucionar modelo lenguaje desarrollo arquitectura neuronal especficamente pensado procesado texto transformers vaswani et arquitectura mecanismo autoatencin permitir relacionar directamente elemento secuencia dato palabra oracin dems capturar as dependencia larga cia caracterizar lenguaje humano modelo basado transformer empezar mostrar especial eficacia tipo tarea pln superar arquitectura neuronal anterior devlin et estn ncleo modelo lenguaje ms conocido tambin estn empezar explorar él arquitectura alternativo podran sustituirlo gu et volmén dato tercer avance clave lograr modelo ms eficaz continuar sible texto entrenamiento cantidad mayor dato tamos apartado s suficiente romper limitación cadena markov tuviser texto jams escribir factor s resultar clave combinacin representación vectorial lenguaje mejora arquitectura neuronal permitir precisamente explotar dato concretamente modelo lenguaje actual utilizar conjunto entrenamiento terabyt liu et contener billón espaol trillón americano palabra ejemplo llamar modelo lenguaje meta entrar billón espaol tokens fragmento palabra dubey et suponer billón espaol palabra hacer él idea enorme cifra estimar libro existente mundo actualidad podran sumar billón espaol prediccin palabra realizacin tarea ingrediente explicado seccin neuronal lenguaje cadena markov generar tinuacin plausible texto ver conjunto entrenamiento probabilidad deduzar dato continuación ms ca común sern ms probable ms atpica sern ms improbable limitación intrnseca cadena markov modelo lenguaje neuronal permitir tamao contexto ciento palabra aparecer problema escasez dato mejorar medida coherencia texto generado as tabla mostrar texto generado versin espaol llm pequeo estndar actual entrenado gb dato texto generar proporcionar entrada torre hrcul permitir modelo predijese palabra ah apreciar objetivo generar texto correcto espaol cumplir creces texto generado sintcticamente correcto semnticamente coherente contenido texto ajustar realidad caso generar continuacin aleatorio sitar torre hrcu él lugar ubicacin real corua adems proporcionar dato veraz deberar sorprenderno reiterar captulo modelo estn entrenado simple exclusivamente generar continuacin plausible texto ningn momento proceso entrenamiento procurar generar texto factualmente realmente sorprendente suponer giro inesperado comunidad investigacin pln wei et simplemente escalar modelo dolo ms dato red neuronal ms ms neurona conexión mejora cualitativo especial empiecir tabla mostrar generación texto principio tabla provenir bloom workshop et modelo ms entrenado millón palabra dems esencialmente distinto tabla resultado texto torre hrcul construcción ms antiguo ciudad alicante cuentra plaza constitucin centro ciudad construir siglo xvi ms antiguo ciudad torre hrcul torre piedra situado ciudad palma mallorca altura metro torre hrcules datar siglo xiv construy seguir modelo inspirado edificio crcega torre hrcul construccin siglo xi ms antiguo conservar catalua estructura original reconstruir mltipl ocasión nicamente ltimo permanecer pie tablar generacin texto cadena torre hrcul captulo iv modelo lenguaje prediccin palabra comprensin sintcticamente correcto semnticamente coherente adems mayor dato fiabilidad ejemplo torre hrcu él est islote utilizar modelo continuar pregunta qu torre hrcul obtener texto tabla ver resultado modelo responder pregunta comportamiento lgico texto contener pregunta continuacin plausible texto responder dicho preguntar pregunta aparecer respuesta sistema habr visto patrn conjunto entrenamiento tambin continuación plausibl implicar responder ejemplo texto examen aparecer pregunta seguido ms pregunta respuesta escalado modelo lenguaje neuronal nir sistema capaz proporcionar informacin veraz dad responder pregunta ms all curiosidad generar texto azar modelo podra realmente til logrrar dotar él torre hrcul faro ms antiguo mundo funcionamiento nico mundo romano dar servicio torre situado pennsula ciudad corua est construido base rocoso metro altura torre hrcul lugar duda smbolo ciudad corua faro funcionamiento ms antiguo mundo historia faro empezar siglo i romano construir ruina faro etrusco torre hrcul faro ms antiguo mundo todava funcionamiento situado ciudad corua galicia islote pennsula torre faro datar siglo tabla generacin texto bloom cadena torre hrcul qu torre hrcul torre hrcul corua faro funcionamiento ms antiguo mundo smbolo ciudad qu torre hrcul torre hrcul monumento ms emblemtico espaa faro construido romano siglo i qu torre hrcul cunto medir qu altura torre hrcul dndir est qu lugar ubicar cunto tiempo necesario subir tabla generacin texto bloom cadena qu torre hrcul tencia ejemplo gustarar pregunta intentar él responder ejemplo tabla lugar generar pregunta ltimo modelo genere continuacin plausible texto veraz posible continuación plausibl potenciar responder pregunta desincentivar ms pregunta ltimo tcnica denominado ajuste instrucción instruction tuning utilizar ajustar modelo preferir generar tipo continuación texto humano variedad tcnica instruction tuning dividir tipo soler aplicar ouyang et ajuste supervisado proporcionar modelo ejemplo tipo respuesta seguir formato modelo imitar aprendizaje refuerzo realimentacin humano ingls reinforcement learning with human feedback rlhf proporcionar modelo realimentacin humano calidad respuesta prefierar respuesta ms deseable alejar deseable ejemplo dar respuesta tabla generado modelo evaluador humano marcara primero deseabl indeseable dato usarar ajustar modelo preferir responder pregunta tcnica cmo obtener llms conocer usuario final sistema chatgpt claude ms modelo neuronal guaje entrenado proporcionar continuacin plausible texto funcionalmente cadena markov desempeo gracias tecnologa neuronal capacidad cmputo dato someter proceso ajuste posible continuación plausibl prefierar aqulla indicar prioritaria qu capacidad limitación ver seccin llm estn entrenado explcitamente responder petición usuario siquiera texto reflejar realidad generar continuación plausibl texto observado dato entrenamiento sentido modelo pequeo limitar generar texto coherente inventado reflejar realidad responder pedir usuario modelo ms ms neurona conexión red entrenar mayor volmén dato espontneamente contenido texto volver ms fiable surgir habilidad responder pregunta importante resaltar ajuste instrucción servir ms consistencia modelo hacerlo ms previsible ejemplo captulo iv modelo lenguaje prediccin palabra comprensin visto responder pregunta lugar tambin mitigar posible sesgo causado dato entrenamiento modelo producir respuesta sesgo indeseab él machismo racismo sto estn presente dato entrenamiento soler caso conjunto dato filtrado descargado internet ningn caso servir aadir capacidad ejemplo visto seccin tabla cmo modelo bloom bruto ningn ajuste capaz responder pregunta ajuste instrucción capacidad aprovechar forma ms consistente usuario noción funcionamiento llms permitir explicar bablemente limitacin tecnolgico conocido fenmeno alucinación ji et llamar situación llm generar texto sonar plausible contener informacin factualmente incorrecto sentido conocer funcionamiento llm deducir punto vista tcnico alucinación fallo sistema entender fallo portamiento anmalo previsto consecuencia directo miento normal sistema entrenar modelo generar texto plausible alucinación especialmente prevalente modelo capaz proporcionar respuesta correcto ejemplo preguntar conjunto entrenamiento hicks et argumentar alucinación entender considerndola bullshit sentido descrito frankfurt libro on bullshit frankfurt frankfurt bullshit forma comunicacin emisor preocupar falsedad nicamente efecto palabra segn hicks et modelo lenguaje serar generador soft bullshit aqullo emitir intencin manipular smil humano tipo discurso persona querer admitir tema cosa salir paso preguntar investigador pln creador llms estn trabajar reducir alucinación mnimo ajuste instrucción ouyang et visto seccin jugar papel importante proporcionar forma incentivar modelo priorizar generacin texto falso lneo activo investigacin generacin aumentado peracin ingls generation lewis et tcnica modelo consultar base dato fuente informacin externo recuperar informacin relevante generar respuesta permitir modelo combinir conocimiento interno informacin actualizado especfica contexto mejorar precisin respuesta reducir alucinación minimizar situación modelo acceso respuesta rreir soft bullshit mencionbar llm deber consciente tcnica reducir alucinación suficiente proporcionarno modelo til aplicable multitud situación garantizar eliminar él completo alucinación explicar caracterstico intrnseca estn diseado entrenado llm descritos limitacin innato inevitable xu et existir banerjee et llm importante otorgar presuncin veracidad informacin proporcionar verificar él fuente importante volver aspecto positivo llms nes existir caso modelo s capaz proporcionarno respuesta veraz punto mejor llms alcanzar tasa fiabilidad alto petición requeír razonamiento complejo saber especializado conocimiento noticia actualidad enfoquir algn punto dbil modelo aritmtico llms modelo lenguaje enfocado palabra acceso directo qu cifra conformar nmero ah sorprendente fracaso aspecto gambardella et pedirl trabajar informacin acceso ms indirectamente persona ciego responder pregunta color cantidad cosa s fiable enorme demostrar capacidad responder pregunta capaz traducir texto resumirlo corregirlo programar distinto lenguaje programacin nivel escritura creativo inciso ltimo convenir desterrar mito dido segn llm podran generar texto creativo predecir pre palabra ms probable ted chiang the new yorker chiang promedio elección escritor representado texto encontrado internet promedio equivaler elección interesante posible texto generado ia soler realmente insulso descripción simplificación engaosa reflejar funcionamiento real llms ver sto funcionar predecir palabra necesariamente palabra ms probable promedio eliminir diversidad atpico cadena markov utilizar distribucin estadstico palabra ms probable segn dato entrenamiento predecirn ms ms atpica impedir llm desmarcar cin realmente inusual fomentar ajustar parmetro llamado temperatura peeperkorn et aplanar distribucin estadstico minuir probabilidad palabra ms probable aumentar margen consideración resultado mostrar llms generar historia juez humano considerar mejor escrita persona williams resultado depender factor longitud texto gnero literario idioma empleado nivel escritor humano compar condición tarea resultado tipo experimento diverso marco et chakrabarty et argumento terico emprico defender texto generado llms tener qu insulso captulo iv modelo lenguaje prediccin palabra comprensin regresar habilidad llms pregunta qu cmo modelo desarrollar absoluto coment apartado hacer él ms modelo lenguaje dejasar simplemente generar texto azar proporcionar respuesta sentido buscado giro inesperado investigador habilidad emergente capacidad surgir rentemente espontnear sistema alcanzar determinado escala explcitamente programado desear estudio intentar explicar cmo qu surgir capacidad schaeffer et du et lejos lograr responder pregunta ignorancia cmo llm lograr implicar asimismo falta transparencia explicar qu llm est proporcionar respuesta desaconsejable utilizar él toma decisión relevante asistente informar persona tomar decisin llm resultar polmico aspecto deberar bsico llm entender algn grado lenguaje humano experto bender et defender comprender absoluto basndo él funcionamiento ver des modelo lenguaje generar palabra basndo él modelo estocstico similar esencia cadena markov bender et llamar loro estocstico limitndo él repetir patrón lingstico previamente observado dato miento entender realmente significado contexto escogen palabra dado basndo él significado mero estadstico dolar azar distribucin consciencia intención respuesta lenguaje falso vaco pensamiento intencionalidad experimento mental bender recordar clsico argumento habitacin chino sear él bender compar aprendizaje llm persona ds siquiera alfabeto estuviese encerrado biblioteca lleno libro tailands argumento disponer tiempo ilimitado imposible persona lograr comprender realmente lengua tailandés podra n observado libro capaz respuesta convincente oración escrito idioma mero ejercicio reconocimiento patrón comprensin real lenguaje podra deducir significar palabra tendrar referente mundo real vincular él concepto experiencia autor estn s ver comprensin real guaje humana limitación llm sepamos cmo ver llm ms ces responder pregunta necesitar razonar objeto propiedad mundo fsico observar li et patel pavlick tabla respuesta chatgpt peticin humano implicar hacer él modelo mental mundo pensar cmo cambiar distinto objeto medida suceder hecho narrado llm proporcionar respuesta correcto as mejor modelo soler grado fiabilidad alto pregunta sta ms compleja difcil capacidad entender mundo ende significado palabra podrar argumentar llms capaz responder tipo pregunta razonamiento superficial reconocimiento patrón dadero modelo interno mundo experimento proporcionar prueba slida capacidad modelado entorno juguete li et entrenar modelo lista jugada juego mesa othello entrenamiento cabo cero modelo acceso ningn tipo informacin absoluto siquiera expuesto lenguaje humano nico ver lista jugada procedente partida principio persona encerrado biblioteca tailandés informacin permitir vincular palabra aparecer libro objeto concepto modelo informacin permitir lista recibir corresponder juego tablero cmo tablero nmero jugador experimento li et mostrar cmo sistema lista jugada capaz jugar correctamente sugerir jugada legal ms generar tacin interno tablero neurona representar casilla tablero juego cosa autor poder comprobar manipular bit cin interno comprobar efecto partida modelo lenguaje capaz inferir significado cadena texto recibir prisionero biblioteca tailandés disponer informacin correspondencia cadena entidad real hacer él idea funcionamiento mundo rior trabajo li et juego othello dicho cadena ningn momento dejar hablar modelo simple exclusivamente predecir continuacin plausible texto caso othello lista jugada modelo li et construir modelo tablero peticin habitacin coger vaso lleno agua poner cubito hielo meto vaso caja negro llevar caja negro saln saco vaso caja vierto do suelo volver meter vaso caja cocina poner caja encimera dndir est cubito hielo dnde veinte minuto respuesta verter contenido vaso suelo cubito hielo asumir encontrar vaso estar suelo saln veinte minuto depender temperatura habitacin probable cubito hielo derretir estar forma agua suelo saln tabla chatgpt responder pregunta requerir capacidad modelado mundo captulo iv modelo lenguaje prediccin palabra comprensin othello forma predecir jugada partida comprender juego as nico objetivo capaz adquirir comprensin modelo cmo funcionar othello podrar hipotetizar forma predecir palabra texto comprender mundo referencia texto ah podra provenir capacidad modelado mundo llms lograr seguir misterio caso obvia laguna an entendimiento cmo llms adquirir habilidad debate contine abierto cuestin comprender lenguaje refutacin concluyente posibilidad loro estocstico suficientemente complejo adquirir comprensin lenguaje prueba concluyent as othello dejar ejemplo juguete comparacin complejidad prender lenguaje humano debate suscitar cuestin avivar debate qu significar exactamente comprender lenguaje humano lejos senso sgaard havlk aparicin sistema capaz manejar lenguaje manera exclusivo humano plantear profundo cuestión relacionado antropomorfismo frecuente caer error describir llms utilizar trmino antropomrfico abercrombie et buirl cualidad intención humano alabar funcionamiento ejemplo atribuyndol empata emoción criticar él ejemplo acusndolos mentir engaar usuario obstante tambin cuestionable descartar completo entidad humano consciente sujeto verbo tradicionalmente vo humano razonar prctico modelo lograr resultado resultar indistinguibl obtendrar humano acción huang chang descartar barrera actualmente inteligencia humano capacidad llms ir difuminar ms medida avance tcnico continar mejorar ltimos ejemplo chalmer advertir ble modelo lenguaje actual consciente deber tomar serio lidad sucesor poder llegar ser él futuro conclusin explicar qu llms radicalmente anterior tecnologa lenguaje seccin descripcin cmo funcionar seccin permitir comprender principal capacidad limitación seccin mente sistema predecir continuacin plausible texto nes estadstica obtenido dato entrenamiento estn sujeto problema presencia sesgo procedente dicho dato as generacin respuesta falso sentido alucinacin limitación comportamiento anmalo consecuencia inherente funcionamiento sistema mitigar punto diverso tcnica citar ajuste instrucción momento garanta eliminar él completo consciente limitación llm resultar til manejo idioma enorme versatilidad capaz lizar cantidad tarea involucrar texto incluir generacin ta humano requeriran habilidad creatividad razonamiento lgico conocimiento realidad fsico debatible qu punto llms poseer realmente capacidad actar loro imitador mostrar aparente creatividad razonamiento respuesta ilusin punto vista prctico resultado indistinguible tener obstante deber perder vista cmo surgen capacidad emergente llms funcionamiento resulte opaco tener tambin observación previo sesgo alucinación confiar llm tomar decisin relevante ltimo caber puntualizar captulo centrar tratar arrojar luz aspecto debate pblico referencia capacidad ción derivar directamente funcionamiento quedar alcance captulo cuestión escapar aspecto tcnico llms centrar ms consideración social tico econmica legal utilizacin plo implicación legal material protegido copyright entrenar llm permiso autor riesgo creado generar informacin falso manipular persona cometer fraude acadmico posibilidad llm acabar sustituir masivamente puesto trabajo huella carbono generar considerable requisito computacional oligopolio empresa nolgica ostentar mejor modelo discusin detallado tes quedar captulo confo anlisis aqu presentado contribuir tambin abordar cuestión contexto referencia abercrombie et mirag on anthropomorphism in dialoguir systems bouamor pino bali proceedings of the conference on empirical methods in natural language processing association for computational linguistics doi banerjee agarwal singla llms will always hallucinatar and we need to live with this arxiv bender thought experiment in the national library of thailand accessed captulo iv modelo lenguaje prediccin palabra comprensin bender et on the dangers of stochastic parrots can language models be too big proceedings of the acm conference on fairness accountability and transparency facct virtual event canado association for computing machinery isbn doi https bommasani et on the opportunitie and risks of foundation models arxiv brown et statistical approach to machine translation computational linguistics brown et language models are learners larochelle et eds advances in neural information processing systems currar associat chakrabarty et art or artifice large language models and the false promise of creativity proceedings of the chi conference on human factors in computing systems chi honolulu hi association for computing machinery isbn doi chalmer could large language model be consciou arxiv org chen goodmar an empirical study of smoothing techniques for language modeling annual meeting of the association for computational linguistics santa cruz california association for computational linguistics doi url chiang why isnt going to make art the new yorker accessed cho et learning phrar representations using rnn encoderdecoder for statistical machine translation moschitti pang daelemans eds proceedings of the conference on empirical methods in natural language processing emnlp doha qatar association for computational linguistics doi url chomsky syntactic structur the haguir mouton co devlin et bert of deep bidirectional transformer for language understanding burstein doran solorio eds proceedings of the conference of the north americar chapter of the association for computational linguistics human language technologies volumir long and short papers minneapoli minnesota association for computational linguistics doi url du et understanding emergent abiliti of language models from the loss perspective arxiv dubey et the llamar herd of models arxiv fang et is chatgpt highly fluent grammatical error correction system comprehensivir evaluation arxiv frankfurt on bullshit princeton university press isbn url gage new algorithm for data compression c user feb issn gambardella iwasawo matsuo language models do hard arithmetic tasks easily and hardly do easy arithmetic tasks ku martins srikumar eds proceedings of the annual meeting of the association for computational linguistics volumir short papers bangkok thailand association for computational linguistics doi goldberg hirst neural network methods in natural language processing morgar claypool publisher isbn williams confederacy of models comprehensivir evaluation of llms on creativir writing bouamor pino bali eds findings of the association for computational linguistics emnlp singapore association for computational linguistics doi url williams the unlikely duel evaluating creativir writing in llms through unique scenario xx conferenciar asociacin espaola inteligencia artificial caepia corua spain asociacin espaola inteligencia artificial isbn goodfellow bengio courvillir deep learning cambridge  ma mit press gu dao mamba sequence modeling with selective statir spaz first conference on language modeling havlk meaning and understanding in large language models arxiv org hicks humphri slater chatgpt is bullshit ethics and inf technol issn doi url hirschberg every time i fire linguist my performance goes up and other myths of the statistical natural language processing revolution invited talk fifteenth national conference on artificial intelligence huang chang towards reasoning in large language models survey roger okazaki findings of the association for computational linguistics acl toronto canado association for computational linguistics doi hutchins the experiment demonstrated in january frederking taylor eds proceedings of the conference of the association for machine translation in the america technical papers washington springer jelinek interpolated estimation of markov source parameters from spar él datar ji et survey of hallucination in natural language generation acm comput surv issn doi url katz estimation of probabilities from spar él datar for the language model component of speech recognizer ieee transactions on acoustics speech and signal processing doi lecun bengio hinton deep learning naturir lewis et generation for nlp tasks advanz in neural information processing systems li nye andrea implicit representations of meaning in neural language models zong et eds proceedings of the annual meeting of the association for computational linguistics and the international joint conference on natural language processing volumir long papers online association for computational linguistics doi captulo iv modelo lenguaje prediccin palabra comprensin li et emergent world representations exploring sequence model trained on synthetic task the eleventh international conference on learning representations liu et dataset for large language models comprehensive survey arxiv manning schtze foundations of statistical natural language massachusetts the mit press manning et the stanford corenlp natural language processing toolkit bontcheva zhu eds proceedings of annual meeting of the association for computational linguistics system demonstrations baltimore maryland association for computational linguistics doi marco et pron vs prompt can large language models already challenge fiction author at creativar text writing bansal chir eds proceedings of the conference on empirical methods in natural language processing miami florida association for computational linguistics doi mikolov et efficient estimation of word representations in vector space corr ouyang et training language models to follow instructions with human feedback koyejo et eds advanz in neural information processing systems ed currar associat patel pavlick mapping language models to grounded conceptual spaz international conference on learning representations peeperkorn et is temperature the creativity parameter of large language models arxiv peng et towards making the most of chatgpt for machine translation bouamor pino bali eds findings of the association for computational linguistics emnlp singapore association for computational linguistics doi finding emnlp pennington socher manning glove global vectors for word representation moschitti pang daelemans eds proceedings of the conference on empirical methods in natural language processing emnlp doha qatar association for computational linguistics doi url pu gao wan summarization is almost dead arxiv schaeffer miranda koyejo are emergent abiliti of large language models mirage proceedings of the international conference on neural information processing systems nips new orleans currar associat inc sear él minds brains and programs mind design cambridge  ma mit press isbn sgaard understanding models understanding language english synthese issn doi spanish concordancer bruno spanish corpus accessed turing computing machinery and intelligence english mind new serie isnn vaswani et attention is all you need guyon et advanz in neural information processing systems currar associat inc wei et emergent abiliti of large language models tran mach learn r weizenbaum elizar computer program for the study of natural language communication betweir man and machine commun acm enir issn doi url workshop et bloom multilingual language model arxiv xu jain kankanhalli hallucination is inevitable an innatar limitation of large language models arxiv