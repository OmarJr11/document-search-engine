73 modelo lenguaje conocido chatgpt inesperado   revolucin mbito inteligencia artificial contar multitud   aplicación prctica enorme potencial todava explorar tambin   objeto debate punto vista cientfico filosfico social duda   mecanismo exacto funcionamiento capacidad real comprensin   lenguaje aplicación plantear dilema tico captulo describimos cmo   llegar tecnologa fundamento funcionamiento permitindono as   comprender capacidad limitación introducir principal   debate rodear desarrollo palabra clave 	  inteligencia artificial procesamiento lenguaje natural   modelo lenguaje   captulo iv modelo lenguaje   prediccin palabra comprensin carlos gmez-rodrguez 	  parcialmente financiado proyecto scanner-udc pid2020-113230rbc21 financiado   miciu aei/10.13039/501100011033 gap pid2022- 139308oa-i00 financiado miciu/ aei/10.13039/501100011033/ feder ue latching pid2023-147129ob-c21 financiado miciu/ aei/10.13039/501100011033 feder ue tsi-100925-2023-1 financiado ministerio transfor- macin digital funcin pblico nextgenerationeu prtr as xunta galicia ed431c   2024/02 centro investigacin galicia citic financiado xunta galicia travs   colaboracin consellera cultura educacin formacin profesional universidad univer- sidad gallego refuerzo centro investigacin sistema universitario galicia cigus   74 1 	  introduccin modelo lenguaje conocido sigla ingls llms   large language models precipitar cambio paradigma procesamiento   lenguaje natural pln rama inteligencia artificial buscar desarrollar programa   poder comprender generar lenguaje humano clave xito modelo   aprender directamente texto simple capaz responder adecuada- mente tipo consulta sta expresabl respondibl   texto incluir realizacin tarea traduccin peng et 2023 correccin gra- matical fang et 2023 resumen texto pu et 2023 respuesta pregunta factual   brown et 2020 escritura creativo gmez-rodrguez williams 2024   paradigma anterior manning schtze 1999 manning   et 2014 tarea requero disear entrenar ajustar sistema especfico   incluir costoso proceso recogida anotacin dato especializado   llms permitir llevarla cabo paragua sistema ajuste espec- fico interfaz natural llevar millón usuario probar tecnologa   integrar él vida tiempo rcord obstante modelo importante   limitación principal siguiente   	  mejor llms sistema computacionalmente costoso est   propiciar oligopolio facto empresa tecnolgica   bommasani et 2021 enfrentado modelo ms pequeo incluir algu- cdigo abierto momento lograr cerrar brecha   modelo  	  respuesta sesgada directamente incorrecto llamar   alucinacin fenmeno llm producir respuesta   sintcticamente redactado coherencia interno   sentido responder consulta alejar realidad  	  sistema opaco contar mecanismo fiable deter- minar qu estn proporcionar respuesta agravar punto   modelo proporcionar informacin verificar res- puesta desaconsejable fiar él salida llm verificacin externo   adems deber consciente limitación   fruto inmadurez tecnologa incipiente resolver él prximo   ao podran intrnseca inevitable xu et 2024 banerjee et 2024 asimismo funcionar correctamente llms plantear diver- sos reto social cuestión tico derivado malintencionado ejemplo   generar informacin falso manipular persona cometer fraude trabajo exmén   sobreuso ejemplo sustituir decisión sensible deberar tomar persona   75 captulo iv modelo lenguaje prediccin palabra comprensin comprender oportunidad creado tecnologa   desafo plantear captulo explicaremos base funcionamiento llms   clave tcnica xito anterior sistema pln ms   importante pregunta abierto desafo enfrentar investigacin campo resto captulo estructurar seccin 2 explicar   qu llms revolucionario contextualizndolo resumen   paradigma suceder historia procesamiento len- guaje natural posicionar avance suponer seccin 3 explicar cmo funcionar   llms seccin 4 basar funcionamiento explicar   capacidad limitación funcional llms ltimo seccin 5 resumir   conclusión captulo 2 	  qu revolucionario mencionar modelo lenguaje llms provocar   cambio paradigma pln situar adems campo investigacin cande- lero qu revolucionario qu diferencia anterior siste- trabajar lenguaje humano saber él convenir perspectiva   histrico distinto paradigma suceder pln pln orgén aos 50 siglo xx hito   propuesta test turing 1950 sugera evaluar inteligencia mquina tra- vs capacidad mantener conversacin forma indistinguible humano   tiempo guerra fra despertar creciente inters posibilidad tradu- cir texto automticamente experimento georgetown-ibm 1954 hutchins 1954   mostr mundo sistema capaz traducir oración ingls ruso   rudimentario titular grandilocuente electronic brain translat russian1   difundir pblico general idea traduccin automtico ense- guido suceder predicción optimista segn problema estar resuelto   definitivamente ao lingstica computacional definir   disciplina cientfico cuyo aplicacin prctico pln empez recibir financiacin   consiguiente logro avance tecnologa orgén actualidad   podra dividir evolucin tecnolgico pln etapa segn   venir ensear mquina trabajar lenguaje humano etapa abarcarar final aos 80 siste- pln basar exclusivamente regla escrito mano experto noam   chomsky chomsky 1957 describir cmo sistematizar gramtica idioma   ingls regla sintctica permitir generar interpretar oración basndo él   principio desarrollar analizador sintctico capaz descomponer oracio- 1 	  https://aclanthology.org/www.mt-archive.info/chemengnews-1954.pdf   76 n componente gramatical facilitar extraer informacin estructurado   texto construyen tambin sistema basado regla tarea   traducir texto responder pregunta mantener conversacin   conocido sistema elizar weizenbaum 1966 problema enfoque   serio problema escalabilidad resultar costoso escribir regla manualmente   problema idioma dominio aplicacin difcil capturar variación   excepción lenguaje humano cambiante ambiguo final 80 comenzar aparecer volver dominant sis- tema basado aprendizaje estadstico necesitar regla enseir   idioma mquina conjunto ejemplo algoritmo aprendizaje   automtico aprender ejemplo entrenar traductor automtico   utilizar corpus oración idioma origen traduccin idioma destino   brown et 1990 tipo enfoque ms escalabl barato basado   regla falta involucrar experto diseo sistema   contar dato calidad ejemplificar famoso frase fred jelinek every   time i fire linguist the performance of the system goes up hirschberg 1998 etapa calidad algoritmo aprendizaje automtico mejo- rar segn avanzar investigación especialmente irrupcin llamado apren- dizaje profundo deep learning evolucin devolvi lneo olvidada   red neuronal mitad dcada 2010 sistema aprendizaje   automtico basado deep learning representar palabra espacio continuo vec- tor denso mikolov et 2013 lugar entidad discreto   entrada red neuronal aprender tarea cho et 2014   mejora cuantitativo precisin sistema logrado tecnologa   cambiar limitacin fundamental etapa sistema propsito especfico   llevar cabo tarea concreto entrenar red neuronal oración   castellano traducción ingls lograr sistema traduccin automtico   efectivo traducir texto castellano ingls intil idioma   estn conjunto entrenamiento ms an tarea responder pregun- ta resumir texto necesitaramos entrenar sistema distinto conjunto   dato ajustado tarea deseado as desear resumir texto castellano tener   crear sistema cero entrenndolo corpus texto caste- llano resmén as tarea querer acometer precisamente limitacin venir resolver tecnologa motivar   captulo modelo lenguaje inaugurar an incipiente   etapa desarrollo pln punto vista tcnico modelo len- guaje provenir escalado modelo neuronal etapa red neurona- ms gracias avance hardware dato entrenamiento ms   arquitectura neuronal mejor transformers vaswani et 2017   punto vista ms amplio modelo provenir evolucin   ruptura anterior suponer cambio paradigma perfectamente resumido    77 captulo iv modelo lenguaje prediccin palabra comprensin ttulo artculo introducir modelo ms transformativo gpt-2 radford   et 2018 podrar perfectamente considerar hito inaugurar etapa    language models are unsupervised multitask learners palabra   modelo lenguaje necesitar dato especializado tarea oración   traduccin idioma texto versin resumido aprender   manejar idioma cantidad gigantesco texto descargado internet   fuente necesidad algn humano adapte tarea especfico   unsupervised estn restringido tarea concreto   sistema cabo variedad tarea pedir multitask suponer   autntico revolucin prctica obviar necesidad dato   especializado tarea idioma dominio aplicacin querer aplicar   pln sistema propsito general chatgpt interac- usuario idioma tipo solicitud necesi- dad aprender utilizar software especializado modelo   lenguaje ms verstil universalmente accesible pln acercar   superacin test turing cambio paradigma dar reciente aparicin vertiginoso velocidad   suceder modelo avance suscitar pregunta   especialista tema pblico general ejemplo cuestin   debate modelo lenguaje realmente capaz entender lenguaje   menor grado nicamente simular respuesta coherente entender abso- luto cuestión actualidad modelo llegar creativo   podran llegar adquirir consciencia serie debate tico social   plantear torno fiabilidad posible malo uso generar desinformacin   qu punto podran deberan sustituir humano rol ejemplo cuestión momento respuesta definitivo   suscitar consenso entender debate evitar postura simplista necesario com- prender fundamento cmo funcionar llm describir   seccin evitar deliberadamente tpico enfoque tcnico diseccionar   componente realmente necesario har explicacin acce- sible contingente   garantizar ao arquitec- tura neuronal funcionar llm radicalmente distinto actual   gu dao 2024 centrarno esencial qu llm qu   tener caracterstica describir ms 3 	  cmo funcionar   esencia llm concepto sencillo modelo canti- dad masivo texto dato entrenamiento predecir continuacin   plausible texto ejemplo modelo lenguaje pasar texto    78 romper capaz continuar palabra coherente contexto   mvil jarrn corazn llms basar tex- to ver entrenamiento 3.1 	  generar texto cadena markov comprender él ms detalle conveniente remontarno   orgén pln 1906 matemtico ruso andrey markov descubrir   modelo describir proceso estocstico conocido cadena markov cadena   markov orden k modelo atravesar serie   probabilidad depender k anterior ejemplo   cadena markov podra asignar probabilidad tiempo har maana funcin   tiempo ayer funcin tabla tabla 1 tabla proporcionar probabilidad maana hacer soleado nublado   lluvioso funcin tiempo ayer ejemplo ayer   nublado est lluvioso tabla sexto fila probabilidad 0.2   maana soleado 0.4 nublado 0.4 lluvioso podra- ayer probabilidad    maana soleado   probabilidad   maana nublado   probabilidad   maana lluvioso soleado soleado 0.7 0.2 0.1 soleado nublado 0.4 0.4 0.2 soleado lluvioso 0.4 0.3 0.3 nublado soleado 0.4 0.4 0.2 nublado nublado 0.2 0.5 0.3 nublado lluvioso 0.2 0.4 0.4 lluvioso soleado 0.3 0.5 0.2 lluvioso nublado 0.2 0.5 0.3 lluvioso lluvioso 0.1 0.3 0.6 tabla 1   cadena markov orden 2 modelar tiempo atmosfrico   79 captulo iv modelo lenguaje prediccin palabra comprensin mos consultar probabilidad combinacion da ejemplo   necesitar da ayer estimar probabilidad tiempo   maán cadena markov orden 2 orden podra   tambin superior inferior caso ejemplo probabilidad tabla   inventado caso real basarir dato histrico ejemplo   despu nublado lluvioso vino soleado 20   pondrar 0.2 correspondiente celda tabla probabilidad   correspondern caber esperar dato histrico modelo generar secuencia estocstico abarcar   tiempo maana tambin das siguiente ejemplo   suponer ayer nublado est lluvioso generar tiempo   plausible maán sorteo aleatorio segn probabilidad tabla   probabilidad 0.2 salir soleado 0.4 nublado 0.4 lluvioso   suponer resultado sorteo lluvioso repetir   proceso maán da anterior serar lluvioso lluvioso as   probabilidad serar ltimo fila tabla 0.1 soleado 0.3 nublado 0.6   lluvioso proceso repetir él indefinidamente generar das   querar medida alejar futuro ms difcil prediccin   modelo corresponder suceder realidad continuacin   plausible tiempo registrado ltimos da concepto aplicar generar texto lenguaje humano con- siderir palabras2 lugar condición meteorolgica   corpus texto tabla probabilidad estimar fcilmente corpus   contar aparición k-grama secuencia k palabras palabra   seguir ejemplo buscar bigrama 2-grama unido corpus bruno   pequeo corpus milln palabra espaol interfaz web libre acceso   fcil lector querer experimentar spanish concordancer bruno spanish   corpus fijar qu palabra venir despu obtener tabla 2 ver   bigrama aparecer seguido norteamrica amrico asociado   deducir referencia ee uu tambin aparecer   palabra trabajar oracin compromiso unido trabajar   pase vas desarrollo variedad palabra pequeo tamao   corpus tuviser ms dato duda podrar observar ms palabra aparecer   despu unidos estimación probabilidad serar tambin ms precisa   2 	  realidad modelo lenguaje trabajar palabra dividir texto subpalabra   tpicamente obtener algoritmo codificacin par bit gage 1994   diverso motivo tcnico incluir eficiencia flexibilidad trabajar   palabra modelo ver simplicidad claridad exposicin explicacin igno- rarar aspecto supondrer trabajar palabra tambin implementacin   ms eficaz eficiente cambiar comprensin concepto fundamental   80 generar texto cadena markov orden 2 recopilaramos dato   tabla 2 bigrama corpus tendramos tabla   exhaustivo fila bigrama continuacin tabla 1   preferir tabla individual tabla 2 combinacin bigrama   proceso generacin sencillo 	  empezar k-grama cualquiera corpus k-grama constituir princi- pio texto generado  	  repetir desear   	  k ltima palabra generado mirar tabla posible palabra   siguiente probabilidad aparezcar continuacin k-grama  	  b sorteo elegir palabra siguiente utilizar dicho   probabilidad posible palabra siguiente tendr pro- babilidad salir indicar tabla  	  c palabra seleccionado aadir texto generado   as ejemplo bigrama inicial ser mujer algoritmo genera- cin consultar palabra aparecer continuacin bigrama conjunto   entrenamiento obtener resultado mostrar bloque superior tabla 3   continuacin escogerar aleatoriamente palabra posible continuación   probabilidad proporcional estimación tabla caso palabra as   escogido ser texto generado mujer iteracin   algoritmo consultar palabra aparecer continuacin mujer   corpus entrenamiento obtener dato bloque inferior tabla 3 ah esco- wi2 wi1 wi   frecuencia corpus   probabilidad estimado unido norteamrico 7 0.5385 unido amrica 5 0.3846 unido trabajar 1 0.0769 tabla 2   frecuencia probabilidad estimado obtener modelar palabra   venir despu bigrama unido corpus milln   palabra espaol nota notacin wi2 wi1 wi referencia palabra posición i-2 i-1 i texto   81 captulo iv modelo lenguaje prediccin palabra comprensin gera palabra azar ejemplo quedar texto mujer   proceso seguir iterar generar texto desear proceso estocstico generar texto lenguaje   tabla 4 mostrar resultado aplicar proceso biblia conjunto entrena- miento3 efecto valor k. texto as generado apre- ciar lxico temtico bblico fuente modelo obtener palabra   caracterstica sintctica semntica texto segn valor   k. k=1 texto carecer coherencia bastante error sintctico llegu   pecado adems sentido lgico aumentar valor k   texto cobrar coherencia error sintctico desaparecer vase k=3   sintaxis est cerca correcto sentido fragmento   texto conjunto carecer sentido lgico consistencia suficiente   aumento progresivo coherencia disminucin error sintctico medida   aumentar k cadena markov garantizar k+1)-grama   texto resulte plausible construccin k+1)-grama   texto aparecer conjunto entrenamiento wi2 wi1 wi frecuencia corpus   probabilidad estimado mujer 7 0.0414 mujer 6 0.0355 mujer 5 0.0296 mujer 5 0.0296 mujer 5 0.0296 mujer mujer 1 0.1667 mujer mayor 1 0.1667 mujer tal 1 0.1667 mujer 1 0.1667 mujer 1 0.1667 mujer fundado 1 0.1667 tabla 3   frecuencia probabilidad estimado obtener modelar palabra   venir despu bigrama mujer mujer   corpus milln palabra espaol 3 	  santa biblia antiguo versin casiodoro reina 1569 revisado cipriano valera 1602 revi- sión 1862 1909 1960 revisin 1960 disponible https://raw.githubusercontent.com/kblok/rnn-bible- generator refs heads master biblia.txt   82 podra pensar él alcanzar texto totalmente coherente bastara   aumentar k necesario encontrarer   problema agotamiento espacio muestral observar texto generado   k=10 tabla 4 poder efectivamente coherente problema   copia literal fragmento conjunto entrenamiento stir   suficientemente encontrer l muestra 10-grama   10-grama agua mar faltarn ro   tabla posible continuación tendr opcin agotar pro- babilidad 1 modelo estocstico reproducir texto bblico   lugar generar texto generacin texto cadena markov est lejos apariencia   inteligencia atribuir texto generado llms dudoso aplicabilidad   prctico limitación k pequeo texto coherencia   k ms encontrar barrera escasez dato podrar   pensar mitigar ltimo conjunto entrenamiento ms s   alcance limitado probabilidad encontrar determinado   k-grama texto disminuir exponencial k juntser   texto escrito historia humanidad podramos estima- ción probabilidad aceptable k=10 bastar pensar elegir secuencia   k palabra consecutivo texto haber escribir ms probable   valor k texto generado 1 jehov har prevaricado animal templo isacar   borde dejar dormir llegu pecado recogie- ron doble tiempo jehov vida cerrojo anunciar dio   har agua salado sirvient jordn ngel parvaim 2 venir m palabra jehov llar tierra sa- crificio alabanza accin gracia luz hijo hija   hermano hija sanar hora edific all altar jehov   grosura sacrificio paz santificar completo 3 acordar pacto jehov espritu mo est   lomos descalzar sandalia pie l poner diestra m   dicindome temar hiri levant campo   joab est mar galilea ensear da adversidad 10 agua mar faltarn ro agotar secar alejarn ro   agotarn secarn corriente foso caa carrizo sern cortado   pradera ro ribera ro sementero ro   secarn perdern sern ms tabla 4   generacin texto bblico cadena markov distinto   valor k   83 captulo iv modelo lenguaje prediccin palabra comprensin escribir caso concreto texto formulaico   frase hecho texto legal conjunto   entrenamiento seguro modelo k=10 limitar copiar literalmente limitación cadena markov atajar sali- mos regla estricto imponer modelo hacindolo ms flexible des- cubrimiento cadena markov aparecer mejora incremental   sentido tcnica suavizado chen goodmar 1996 interpolacin jelinek   1980 back-off katz 1987 avance realmente permitirar desbloquear   modelo generativo lenguaje valor k modelo lenguaje   neuronal 3.2 	  cadena markov modelo neuronal modelo lenguaje neuronal cadena markov modelo   generar continuacin plausible texto observar con- entrenamiento lugar basar él puramente clculo proba- bilidad condicional generacin producir red neuronal   modelo poder generar texto ms flexible soportar as longitud   contexto k ms larga explicar detalle funcionamiento modelo escapar   objetivo captulo s caber destacar principalmente avance   posibilitar lograr flexibilidad cadena markov carecar repre- sentación denso lenguaje avance tcnico red neuronal disponibilidad   enorme conjunto dato obtenido internet continuacin describir brevemente   avance relevancia representación denso lenguaje representación denso lenguaje conocido ingls word embeddings   vector numrico capturan significado palabra ubicarla espacio   vectorial palabra significado similar estn cerca tpicamente   vector representar palabra est formado ciento componente   coma flotante aproximacin informtico nmero real popularizacin   representación producir raz modelo word2vec mikolov et 2013   obtener vector palabra sencillo red neuronal   ir aparecer alternativa pennington et 2014 incluir represen- tación contextualizada vector palabra varar segn contexto   aparecer devlin et 2019 representación denso lenguaje ventaja considerar   palabra entidad discreto red neuronal adaptar    84 trabajar vector denso nmero real entidad discreto principal   ventaja flexibilidad aadir modelar forma natural similitud   palabra cadena markov vista apartado 3.1 as mode- estadstico tradicional usado pln palabra representar entidad   atmico relacin aparente dems modelo tipo palabra aba- nico avin aeroplano cadena texto traducir interiormente   algn tipo identificador numrico modelo tendr constancia   ltima parecer ms s as cadena markov   continuar texto perdi control aeroplano k-grama exacto aparecer   conjunto entrenamiento s aparecer perdi control avin poder   aprovechar similitud generar continuacin texto cambio modelo   neuronal s podr lugar buscar coincidencia exacto palabra tabla   modelo neuronal trabajar espacio continuo representación vectorial as   aprovechar él fragmento texto similar igual ver   entrenamiento proporcionar flexibilidad adicional frente rigidez cadena   markov capacidad sacar ms partido dato avance tcnico red neuronal red neuronal modelo computacional inicialmente inspirado   estructura funcionamiento cerebro humano diseado aprender relacin   entrada salida deseado problema complejo estn formado unidad pro- cesamiento llamado neurona organizado capa conectar s. red   neuronal neurona recibir entrada realizar clculo transmitir resultado   neurona capa travs conexión unir proceso   entrenamiento conexión ajustar usar algoritmo optimizacin permitir   red aprender patrón dato mejore tarea especfico   procesamiento texto red neuronal conocido mediados siglo xx final   siglo tecnologa olvidado sar solar   opcin prctica mayora problema claramente superado   alternativa mquina vector soporte sealar lecun   et 2015 red neuronal medida abandonado comunidad   aprendizaje automtico pensar ampliamente forma aprendar   dato inviable resurgir ejemplo cmo inversin inves- tigacin bsico aplicación claro vista clave avance cientfico   dcada 2010 lograr importante mejora red neuronal   serie descubrimiento posibilitir llamado aprendizaje profundo goodfellow et   2016 enfoque transformar campo inteligencia artificial gracias avance   algoritmos capa red neuronal ms profundo implementacin   tcnica regularizacin optimizacin as aumento capacidad cmputo    85 captulo iv modelo lenguaje prediccin palabra comprensin facilitado gpu red comenzar alcanzar rendimiento notable   tarea complejo incluir procesamiento lenguaje natural goldberg hirst 2017 inicialmente red neuronal usado pln arquitectura conoci- da problema adaptar procesar lenguaje goldberg hirst 2017   avance supondrar pistoletazo salida modelo lenguaje neuronal   ms evolucionar modelo lenguaje desarrollo arquitectura   neuronal especficamente pensado procesado texto transformers vaswani   et 2017 arquitectura mecanismo autoatencin self-attention   permitir relacionar directamente elemento secuencia dato   palabra oracin dems capturar as dependencia larga distan- cia caracterizar lenguaje humano modelo basado transformer   empezar mostrar especial eficacia tipo tarea pln superar   arquitectura neuronal anterior devlin et 2019 estn ncleo   modelo lenguaje ms conocido tambin estn empezar explorar él   arquitectura alternativo podran sustituirlo gu et 2024 volmén dato tercer avance clave lograr modelo ms eficaz continuar plau- sible texto entrenamiento cantidad mayor dato comen- tamos apartado 3.1 s suficiente romper limitación   cadena markov tuviser texto jams escribir factor   s resultar clave combinacin representación vectorial lenguaje   mejora arquitectura neuronal permitir precisamente explotar   dato concretamente modelo lenguaje actual utilizar conjunto   entrenamiento terabyt liu et 2024 contener billón espaol trillón   americano palabra ejemplo llamar 3 modelo lenguaje meta entrar   15 billón espaol tokens fragmento palabra dubey et 2024   suponer 7 10 billón espaol palabra hacer él idea enorme   cifra estimar libro existente mundo actualidad   podran sumar 16 billón espaol palabras4 3.3 	  prediccin palabra realizacin tarea ingrediente explicado seccin mode- neuronal lenguaje cadena markov generar con- tinuacin plausible texto ver conjunto entrenamiento   probabilidad deduzar dato continuación ms tpi- 4 	  https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/#all-books   86 ca común sern ms probable ms atpica sern ms improbable   limitación intrnseca cadena markov   modelo lenguaje neuronal permitir tamao contexto ciento   palabra aparecer problema escasez dato mejorar medida   coherencia texto generado as tabla 5 mostrar texto generado versin espaol   gpt-25 llm pequeo estndar actual entrenado 11.5 gb dato   texto generar proporcionar entrada torre hrcul permitir   modelo predijese palabra ah apreciar objetivo   generar texto correcto espaol cumplir creces texto generado   sintcticamente correcto semnticamente coherente contenido   texto ajustar realidad caso generar continuacin aleatorio   sitar torre hrcu él lugar ubicacin real corua   adems proporcionar dato veraz deberar   sorprenderno reiterar captulo modelo   estn entrenado simple exclusivamente generar continuacin plausible texto   ningn momento proceso entrenamiento procurar generar   texto factualmente realmente sorprendente suponer giro inesperado comunidad   investigacin pln wei et ,2022 simplemente escalar modelo entrenn- dolo ms dato red neuronal ms ms neurona   conexión mejora cualitativo especial empiecir   tabla 6 mostrar generación texto principio   tabla 5 provenir bloom workshop et 2023 modelo ms   entrenado 200000 millón palabra dems   esencialmente distinto gpt-2 tabla resultado texto   torre hrcul construcción ms antiguo ciudad alicante en- cuentra plaza constitucin centro ciudad construir siglo xvi   ms antiguo ciudad torre hrcul torre piedra situado ciudad palma mallorca altura   86 metro torre hrcules datar siglo xiv construy seguir modelo inspirado   edificio crcega torre hrcul construccin siglo xi ms antiguo conservar catalua   estructura original reconstruir mltipl ocasión nicamente   ltimo permanecer pie tablar 5   generacin texto gpt-2 cadena torre hrcul 5 	  https://huggingface.co/deepesp/gpt2-spanish   87 captulo iv modelo lenguaje prediccin palabra comprensin sintcticamente correcto semnticamente coherente adems   mayor dato fiabilidad ejemplo   torre hrcu él est islote utilizar modelo continuar pregunta qu   torre hrcul obtener texto tabla 7 ver resultado   modelo responder pregunta comportamiento lgico   texto contener pregunta continuacin plausible texto responder dicho   preguntar pregunta aparecer respuesta sistema habr visto   patrn conjunto entrenamiento tambin continuación plausibl   implicar responder ejemplo texto examen aparecer pregunta   seguido ms pregunta respuesta                                 escalado modelo lenguaje neuronal obte- nir sistema capaz proporcionar informacin veraz fiabili- dad responder pregunta ms all curiosidad   generar texto azar modelo podra realmente til logrrar dotar él consis- torre hrcul faro ms antiguo mundo funcionamiento nico mundo   romano dar servicio torre situado pennsula ciudad   corua est construido base rocoso 40 metro altura   torre hrcul lugar duda smbolo ciudad corua   faro funcionamiento ms antiguo mundo historia faro empezar siglo i d.c   romano construir ruina faro etrusco   torre hrcul faro ms antiguo mundo todava funcionamiento   situado ciudad corua galicia islote pennsula torre faro datar   siglo i. tabla 6   generacin texto bloom cadena torre hrcul qu torre hrcul torre hrcul corua faro funcionamiento ms   antiguo mundo smbolo ciudad   qu torre hrcul torre hrcul monumento ms emblemtico   espaa faro construido romano siglo i   qu torre hrcul cunto medir qu altura torre hrcul dndir est   qu lugar ubicar cunto tiempo necesario subir   tabla 7   generacin texto bloom cadena qu torre   hrcul   88 tencia ejemplo gustarar pregunta intentar él responder   ejemplo tabla 7 lugar generar pregunta ltimo   modelo genere continuacin plausible texto   veraz posible continuación plausibl potenciar   responder pregunta desincentivar ms pregunta ltimo tcnica denominado ajuste instrucción   instruction tuning utilizar ajustar modelo preferir generar   tipo continuación texto humano variedad tcnica   instruction tuning dividir tipo soler aplicar con- ouyang et 2022   	  ajuste supervisado proporcionar modelo ejemplo tipo respuesta   seguir formato instruccin-respuesta modelo imitar  	  aprendizaje refuerzo realimentacin humano ingls reinforcement   learning with human feedback rlhf proporcionar modelo realimentacin   humano calidad respuesta prefierar respuesta ms   deseable alejar deseable ejemplo dar respuesta   tabla 7 generado modelo evaluador humano marcara   primero deseabl indeseable dato usarar   ajustar modelo preferir responder pregunta   tcnica cmo obtener llms conocer usuario   final sistema chatgpt claude ms modelo neuronal len- guaje entrenado proporcionar continuacin plausible texto funcionalmente   cadena markov desempeo gracias tecnologa   neuronal capacidad cmputo dato someter proceso ajuste   posible continuación plausibl prefierar aqulla   indicar prioritaria 4 	  qu capacidad limitación ver seccin llm estn entrenado explcitamente   responder petición usuario siquiera texto reflejar realidad   generar continuación plausibl texto observado   dato entrenamiento sentido modelo pequeo limitar generar texto   coherente inventado reflejar realidad responder pedir usuario   modelo ms ms neurona conexión   red entrenar mayor volmén dato espontneamente contenido   texto volver ms fiable surgir habilidad responder   pregunta importante resaltar ajuste instrucción servir   ms consistencia modelo hacerlo ms previsible ejemplo    89 captulo iv modelo lenguaje prediccin palabra comprensin visto responder pregunta lugar   tambin mitigar posible sesgo causado dato entrenamiento modelo   producir respuesta sesgo indeseab él machismo racismo sto estn   presente dato entrenamiento soler caso conjunto dato   filtrado descargado internet ningn caso servir aadir capacidad   ejemplo visto seccin tabla 7 cmo modelo bloom   bruto ningn ajuste capaz responder pregunta ajuste instrucción   capacidad aprovechar forma ms consistente usuario noción funcionamiento llms permitir explicar pro- bablemente limitacin tecnolgico conocido fenmeno alucinación   ji et 2023 llamar situación llm generar   texto sonar plausible contener informacin factualmente incorrecto   sentido conocer funcionamiento llm deducir punto   vista tcnico alucinación fallo sistema entender fallo com- portamiento anmalo previsto consecuencia directo funciona- miento normal sistema entrenar modelo generar texto plausible   alucinación especialmente prevalente modelo capaz   proporcionar respuesta correcto ejemplo preguntar   conjunto entrenamiento hicks et 2024 argumentar   alucinación entender considerndola bullshit sentido descrito   frankfurt libro on bullshit frankfurt 2009 frankfurt bullshit forma   comunicacin emisor preocupar falsedad   nicamente efecto palabra segn hicks et 2024   modelo lenguaje serar generador soft bullshit aqullo   emitir intencin manipular smil humano tipo discurso   persona querer admitir tema cosa   salir paso preguntar l. investigador pln creador llms estn trabajar   reducir alucinación mnimo ajuste instrucción ouyang et 2022   visto seccin jugar papel importante proporcionar   forma incentivar modelo priorizar generacin texto   falso lneo activo investigacin generacin aumentado recu- peracin ingls retrieval-augmented generation lewis et 2020 tcnica   modelo consultar base dato fuente informacin externo   recuperar informacin relevante generar respuesta permitir   modelo combinir conocimiento interno informacin actualizado especfica   contexto mejorar precisin respuesta reducir alucinación   minimizar situación modelo acceso respuesta incu- rreir soft bullshit mencionbar llm deber   consciente tcnica reducir alucinación suficiente   proporcionarno modelo til aplicable multitud situación   garantizar eliminar él completo alucinación explicar    90 caracterstico intrnseca estn diseado entrenado llm   descritos limitacin innato inevitable xu et 2024 existir   banerjee et 2024 llm importante otorgar   presuncin veracidad informacin proporcionar verificar él fuente   importante volver aspecto positivo llms alucinacio- nes existir caso modelo s capaz proporcionarno respuesta   veraz punto mejor llms alcanzar tasa fiabilidad alto   petición requeír razonamiento complejo saber   especializado conocimiento noticia actualidad enfoquir algn punto   dbil modelo aritmtico llms modelo lenguaje   enfocado palabra acceso directo qu cifra conformar nmero   ah sorprendente fracaso aspecto gambardella et 2024   pedirl trabajar informacin acceso ms indirectamente   persona ciego responder pregunta color cantidad   cosa s fiable enorme demostrar   capacidad responder pregunta capaz traducir texto resumirlo   corregirlo programar distinto lenguaje programacin   nivel escritura creativo inciso ltimo convenir desterrar mito difun- dido segn llm podran generar texto creativo predecir siem- pre palabra ms probable ted chiang the new yorker chiang 2023   promedio elección escritor representado   texto encontrado internet promedio equivaler elección interesante   posible texto generado ia soler realmente insulso descripción   simplificación engaosa reflejar funcionamiento real llms   ver sto funcionar predecir palabra necesariamente   palabra ms probable promedio eliminir diversidad atpico   cadena markov utilizar distribucin estadstico   palabra ms probable segn dato entrenamiento predecirn ms   ms atpica impedir llm desmarcar elec- cin realmente inusual fomentar ajustar parmetro   llamado temperatura peeperkorn et 2024 aplanar distribucin estadstico dis- minuir probabilidad palabra ms probable aumentar   margen consideración resultado mostrar   llms generar historia juez humano considerar mejor   escrita persona gmez-rodrguez williams 2023 resultado depender   factor longitud texto gnero literario idioma empleado nivel   escritor humano compar condición tarea   resultado tipo experimento diverso marco et 2024 chakrabarty   et 2024 argumento terico emprico defender   texto generado llms tener qu insulso   91 captulo iv modelo lenguaje prediccin palabra comprensin regresar habilidad llms pregunta qu cmo   modelo desarrollar absoluto   coment apartado 3.3 hacer él ms modelo   lenguaje dejasar simplemente generar texto azar proporcionar respuesta   sentido buscado giro inesperado investigador   habilidad emergente capacidad surgir apa- rentemente espontnear sistema alcanzar determinado escala   explcitamente programado desear   estudio intentar explicar cmo qu surgir capacidad schaeffer et 2024   du et 2024 lejos lograr responder pregunta ignorancia   cmo llm lograr implicar asimismo falta transparencia   explicar qu llm est proporcionar respuesta   desaconsejable utilizar él toma decisión relevante   asistente informar persona tomar decisin llm resultar polmico   aspecto deberar bsico llm entender algn grado lenguaje   humano experto bender et 2021 defender comprender   absoluto basndo él funcionamiento ver gran- des modelo lenguaje generar palabra basndo él modelo estocstico similar   esencia cadena markov bender et llamar loro estocstico   limitndo él repetir patrón lingstico previamente observado dato entrena- miento entender realmente significado contexto escogen   palabra dado basndo él significado mero estadstico eligin- dolar azar distribucin consciencia intención   respuesta lenguaje falso vaco pensamiento intencionalidad experimento   mental bender 2020 recordar clsico argumento habitacin chino sear él   1985 bender compar aprendizaje llm persona tailan- ds siquiera alfabeto estuviese encerrado biblioteca lleno libro   tailands argumento disponer tiempo ilimitado imposible   persona lograr comprender realmente lengua tailandés podra patro- n observado libro capaz respuesta convincente oración escrito   idioma mero ejercicio reconocimiento patrón comprensin   real lenguaje podra deducir significar palabra tendrar   referente mundo real vincular él concepto experiencia autor estn s ver comprensin real len- guaje humana limitación llm   sepamos cmo ver llm ms capa- ces responder pregunta necesitar razonar objeto propiedad mundo   fsico observar li et 2021 patel pavlick 2022 tabla 8   respuesta chatgpt peticin humano implicar hacer él   modelo mental mundo pensar cmo cambiar distinto   objeto medida suceder hecho narrado llm proporcionar    92 respuesta correcto as 100 mejor modelo soler   grado fiabilidad alto pregunta sta ms compleja   difcil capacidad entender mundo ende   significado palabra podrar argumentar llms capaz responder tipo   pregunta razonamiento superficial reconocimiento patrón ver- dadero modelo interno mundo experimento proporcionar prueba slida   capacidad modelado entorno juguete li et 2023 entrenar   modelo gpt-2 lista jugada juego mesa othello entrenamiento   cabo cero modelo acceso ningn tipo informacin   absoluto siquiera expuesto lenguaje humano nico ver   lista jugada c3 d4 procedente partida principio   persona encerrado biblioteca tailandés informacin permitir   vincular palabra aparecer libro objeto concepto modelo   informacin permitir lista recibir corresponder juego   tablero cmo tablero nmero jugador experimento li   et mostrar cmo sistema lista jugada capaz jugar   correctamente sugerir jugada legal ms 99 generar represen- tacin interno tablero 64 neurona representar casilla tablero 8x8   juego cosa autor poder comprobar manipular bit representa- cin interno comprobar efecto partida modelo lenguaje capaz   inferir significado cadena texto recibir prisionero   biblioteca tailandés disponer informacin correspondencia   cadena entidad real hacer él idea funcionamiento mundo exte- rior trabajo li et 2023 juego othello dicho cadena   ningn momento dejar hablar modelo entrena- simple exclusivamente predecir continuacin plausible texto caso   othello lista jugada modelo li et construir modelo tablero   peticin habitacin coger vaso lleno agua poner cubito hielo meto   vaso caja negro llevar caja negro saln saco vaso caja vierto conteni- do suelo volver meter vaso caja cocina poner caja encimera dndir   est cubito hielo dnde veinte minuto respuesta verter contenido vaso suelo cubito hielo asumir   encontrar vaso estar suelo saln veinte minuto depender   temperatura habitacin probable cubito hielo derretir estar   forma agua suelo saln tabla 8   chatgpt gpt-4 responder pregunta requerir   capacidad modelado mundo   93 captulo iv modelo lenguaje prediccin palabra comprensin othello forma predecir jugada partida comprender   juego as nico objetivo capaz adquirir comprensin modelo cmo   funcionar othello podrar hipotetizar forma predecir   palabra texto comprender mundo referencia texto ah   podra provenir capacidad modelado mundo llms   lograr seguir misterio caso obvia laguna an entendimiento cmo   llms adquirir habilidad debate contine abierto cuestin   comprender lenguaje refutacin concluyente posibilidad   loro estocstico suficientemente complejo adquirir   comprensin lenguaje prueba concluyent as   othello dejar ejemplo juguete comparacin complejidad com- prender lenguaje humano debate suscitar cuestin avivar debate qu   significar exactamente comprender lenguaje humano lejos con- senso sgaard 2022 havlk 2023 aparicin sistema capaz   manejar lenguaje manera exclusivo humano plantear   profundo cuestión relacionado antropomorfismo frecuente caer error   describir llms utilizar trmino antropomrfico abercrombie et 2023 atri- buirl cualidad intención humano alabar funcionamiento ejemplo   atribuyndol empata emoción criticar él ejemplo acusndolos mentir   engaar usuario obstante tambin cuestionable descartar completo   entidad humano consciente sujeto verbo tradicionalmente exclusi- vo humano razonar prctico modelo lograr resultado   resultar indistinguibl obtendrar humano   acción huang chang 2023 descartar   barrera actualmente inteligencia humano capacidad   llms ir difuminar ms medida avance tcnico continar   mejorar ltimos ejemplo chalmer 2024 advertir improba- ble modelo lenguaje actual consciente deber tomar serio posibi- lidad sucesor poder llegar ser él futuro 5 	  conclusin explicar qu llms radicalmente anterior tecnologa   lenguaje seccin 2 descripcin cmo funcionar seccin 3 permitir   comprender principal capacidad limitación seccin 4 esencial- mente sistema predecir continuacin plausible texto distribucio- nes estadstica obtenido dato entrenamiento estn sujeto problema   presencia sesgo procedente dicho dato as generacin respuesta falso    94 sentido alucinacin limitación comportamiento anmalo   consecuencia inherente funcionamiento sistema mitigar   punto diverso tcnica citar ajuste instrucción   momento garanta eliminar él completo consciente limitación llm   resultar til manejo idioma enorme versatilidad capaz rea- lizar cantidad tarea involucrar texto incluir generacin respues- ta humano requeriran habilidad creatividad razonamiento lgico   conocimiento realidad fsico debatible qu punto   llms poseer realmente capacidad actar loro imitador   mostrar aparente creatividad razonamiento respuesta ilusin   punto vista prctico resultado indistinguible tener obstante deber perder vista cmo surgen   capacidad emergente llms funcionamiento resulte   opaco tener tambin observación previo sesgo   alucinación confiar llm tomar decisin relevante ltimo caber puntualizar captulo centrar tratar arrojar   luz aspecto debate pblico referencia capacidad limita- ción derivar directamente funcionamiento quedar alcance   captulo cuestión escapar aspecto tcnico llms centrar ms   consideración social tico econmica legal utilizacin ejem- plo implicación legal material protegido copyright entrenar llm   permiso autor riesgo creado generar informacin falso   manipular persona cometer fraude acadmico posibilidad llm acabar   sustituir masivamente puesto trabajo huella carbono generar   considerable requisito computacional oligopolio empresa tec- nolgica ostentar mejor modelo discusin detallado deba- tes quedar captulo confo anlisis aqu presentado contribuir   tambin abordar cuestión contexto referencia abercrombie g. et 2023 mirag on anthropomorphism in dialoguir systems h. bouamor j.   pino k. bali proceedings of the 2023 conference on empirical methods in natural language processing   4776-4790 association for computational linguistics doi 10.18653 v1/2023.emnlp-main.290 https:// aclanthology.org/2023.emnlp-main.290 banerjee s. agarwal a. singla s. 2024 llms will always hallucinatar and we need to live with this   arxiv 2409.05746 stat.ml https://arxiv.org/abs/2409.05746 bender e. m. 2020 thought experiment in the national library of thailand accessed 2024-11-02 2020   https://medium.com/@emilymenonbender/thought-experiment-inthe-national-library-of-thailand- f2bf761a8a83   95 captulo iv modelo lenguaje prediccin palabra comprensin bender e. m. et 2021 on the dangers of stochastic parrots can language models be too big   proceedings of the 2021 acm conference on fairness accountability and transparency facct 21 610- 623 virtual event canado association for computing machinery 2021 isbn 9781450383097 doi   10.1145/3442188.3445922 https //doi.org/10.1145/3442188.3445922 bommasani r. et 2021 on the opportunitie and risks of foundation models arxiv https://crfm stanford.edu/assets/report.pdf brown p. f. et 1990 statistical approach to machine translation computational linguistics 16.2   79-85 https://aclanthology.org/j90-2002 brown t. et 2020 language models are few-shot learners h. larochelle et eds advances in   neural information processing systems 1877-1901 vol. 33 currar associat https://proceedings  neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-paper.pdf chakrabarty t. et 2024 art or artifice large language models and the false promise of creativity   proceedings of the 2024 chi conference on human factors in computing systems chi 24 honolulu   hi association for computing machinery isbn 9798400703300 doi 10.1145/3613904.3642731   https://doi.org/10.1145/ 3613904.3642731 chalmer d. j. 2024 could large language model be consciou arxiv 2303.07103 cs.ai https://arxiv org abs/2303.07103 chen s. f. goodmar j. 1996 an empirical study of smoothing techniques for language modeling 34th   annual meeting of the association for computational linguistics 310-318 santa cruz california   association for computational linguistics doi 10.3115/981863.981904 url https://aclanthology.org/p96-1041 chiang t. 2023 why a.i isnt going to make art the new yorker accessed 2024-11-02 https://www newyorker.com/culture/the-weekend-essay/why-aiisnt-going-to-make-art cho k. et 2014 learning phrar representations using rnn encoderdecoder for statistical machine   translation a. moschitti b. pang w. daelemans eds proceedings of the 2014 conference on   empirical methods in natural language processing emnlp 1724-1734 doha qatar association for   computational linguistics doi 10.3115 v1 d14-1179 url https://aclanthology.org/d14-1179 chomsky n. 1957 syntactic structur the haguir mouton co devlin j. et 2019 bert pre-training of deep bidirectional transformer for language understanding   j. burstein c. doran t. solorio eds proceedings of the 2019 conference of the north americar   chapter of the association for computational linguistics human language technologies volumir 1 long   and short papers 4171-4186 minneapoli minnesota association for computational linguistics doi   10.18653 v1 n19-1423 url https://aclanthology.org/n19-1423 du z. et 2024 understanding emergent abiliti of language models from the loss perspective arxiv   2403.15796 cs.cl https://arxiv.org/abs/2403.15796 dubey a. et 2024 the llamar 3 herd of models arxiv 2407.21783 cs.ai https://arxiv.org/abs/2407.21783 fang t. et 2023 is chatgpt highly fluent grammatical error correction system comprehensivir   evaluation arxiv 2304.01746 cs.cl https://arxiv.org/abs/2304.01746 frankfurt h. on bullshit princeton university press 2009 isbn 9781400826537 url https://books.google books?id = bfpznitio7oc gage p. 1994 new algorithm for data compression c user j. 12.2 feb 1994 23-38 issn 0898-9788 gambardella a. iwasawo y. matsuo y. 2024 language models do hard arithmetic tasks easily and   hardly do easy arithmetic tasks l.-w ku a. martins v. srikumar eds proceedings of the   62nd annual meeting of the association for computational linguistics volumir 2 short papers 85-91   bangkok thailand association for computational linguistics doi 10.18653 v1/2024.acl-short.8 https:// aclanthology.org/2024.acl-short.8   96 goldberg y. hirst g. 2017 neural network methods in natural language processing morgar claypool   publisher isbn 1627052984 gmez-rodrguez c. williams p. 2023 confederacy of models comprehensivir evaluation of llms on   creativir writing h. bouamor j. pino k. bali eds findings of the association for computational   linguistics emnlp 2023 14504-14528 singapore association for computational linguistics doi   10.18653 v1/2023 findings-emnlp.966 url https://aclanthology.org/2023.findings-emnlp.966 gmez-rodrguez c. williams p. 2024 the unlikely duel evaluating creativir writing in llms through   unique scenario xx conferenciar asociacin espaola inteligencia artificial caepia   2024 225-226 corua spain asociacin espaola inteligencia artificial isbn 978-84-09- 62724-0 goodfellow i. j. bengio y. courvillir a. 2016 deep learning cambridge  ma mit press http:// www.deeplearningbook.org/ gu a. dao t. mamba 2024 linear-time sequence modeling with selective statir spaz first conference   on language modeling https://openreview.net/forum?id=teyskw1vy2 havlk v. 2023 meaning and understanding in large language models arxiv 2310.17407 cs.cl https://arxiv org abs/2310.17407 hicks m. t. humphri j. slater j. 2024 chatgpt is bullshit ethics and inf technol 26.2 issn 1388- 1957 doi 10.1007 s10676-024-09775-5 url https://doi.org/10.1007/s10676-024-09775-5 hirschberg j. 1998 every time i fire linguist my performance goes up and other myths of the statistical   natural language processing revolution invited talk fifteenth national conference on artificial intelligence   aaai-98 huang j. chang k. c.-c 2023 towards reasoning in large language models survey a. roger j.   boyd-graber n. okazaki findings of the association for computational linguistics acl 2023 1049- 1065 toronto canado association for computational linguistics doi 10.18653 v1/2023.findings-acl.67   https://aclanthology.org/2023.findings-acl.67 hutchins w. j. 2024 the georgetown-ibm experiment demonstrated in january 1954 r. e. frederking   k. b. taylor eds proceedings of the 6th conference of the association for machine translation   in the america technical papers 102-114 washington springer https://link.springer.com/ chapter/10.1007/978-3-540-30194-3_12 jelinek f. 1980 interpolated estimation of markov source parameters from spar él datar https://api semanticscholar.org/corpusid:61012010 ji z. et 2023 survey of hallucination in natural language generation acm comput surv 55.12 issn   0360-0300 doi 10.1145/3571730 url https://doi.org/10.1145/3571730 katz s. 1987 estimation of probabilities from spar él datar for the language model component of speech   recognizer ieee transactions on acoustics speech and signal processing 35.3 400-401 doi 10.1109/ tassp.1987.1165125 lecun y. bengio y. hinton g. 2015 deep learning naturir 521.7553 436 lewis p. et 2020 retrieval-augmented generation for knowledge-intensive nlp tasks advanz in neural   information processing systems 33 9459-9474 li b. z. nye m. andrea j. 2021 implicit representations of meaning in neural language models c.   zong et eds proceedings of the 59th annual meeting of the association for computational linguistics   and the 11th international joint conference on natural language processing volumir 1 long papers 1813- 1827 online association for computational linguistics doi 10.18653 v1/2021.acl-long.143 https:// aclanthology.org/2021.acl-long.143    97 captulo iv modelo lenguaje prediccin palabra comprensin li k. et 2023 emergent world representations exploring sequence model trained on synthetic   task the eleventh international conference on learning representations https://openreview.net/ forum?id = deg07_tczvt liu y. et 2024 dataset for large language models comprehensive survey arxiv 2402.18041 cs.cl   https://arxiv.org/abs/2402.18041 manning c. d. schtze h. 1999 foundations of statistical natural language processing.cambridge   massachusetts the mit press http://nlp.stanford.edu/fsnlp/ manning c. et 2014 the stanford corenlp natural language processing toolkit k. bontcheva   j. zhu eds proceedings of 52nd annual meeting of the association for computational linguistics system   demonstrations 55-60 baltimore maryland association for computational linguistics doi 10.3115 v1/ p14- 5010 https://aclanthology.org/p14-5010 marco g. et 2024 pron vs prompt can large language models already challenge world-class fiction   author at creativar text writing y. al-onaizan m. bansal y.-n chir eds proceedings of the   2024 conference on empirical methods in natural language processing 1954-1967 miami florida   association for computational linguistics doi 10.18653 v1/2024.emnlp-main.1096 https://aclanthology org/2024.emnlp-main.1096 mikolov t. et 2013 efficient estimation of word representations in vector space corr abs/1301.3781   http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781 ouyang l. et 2022 training language models to follow instructions with human feedback s. koyejo   et eds advanz in neural information processing systems ed vol. 35 currar associat 27730- 27744 https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731- paper-conference.pdf patel r. pavlick e. 2022 mapping language models to grounded conceptual spaz international   conference on learning representations https://openreview.net/forum?id=gjcem8sxhk peeperkorn m. et 2024 is temperature the creativity parameter of large language models arxiv   2405.00492 cs.cl https://arxiv.org/abs/2405.00492 peng k. et 2023 towards making the most of chatgpt for machine translation h. bouamor pino   j. k. bali eds findings of the association for computational linguistics emnlp 2023 5622-5633   singapore association for computational linguistics doi 10 18653 v1 2023 finding emnlp 373   https://aclanthology.org/2023.findings-emnlp.373 pennington j. socher r. manning c. 2014 glove global vectors for word representation   a. moschitti b. pang w. daelemans eds proceedings of the 2014 conference on empirical methods in   natural language processing emnlp 1532-1543 doha qatar association for computational linguistics   doi 10.3115 v1 d14-1162 url https://aclanthology.org/d14-1162 pu x. gao m. wan x. 2023 summarization is almost dead arxiv 2309.09558 cs.cl https://arxiv.org/ abs/2309.09558 schaeffer r. miranda b. koyejo s. 2024 are emergent abiliti of large language models mirage   proceedings of the 37th international conference on neural information processing systems nips 23 new   orleans currar associat inc sear él j. r. 1985 minds brains and programs mind design 282-307 cambridge  ma mit press   isbn 0262580527 sgaard a. 2022 understanding models understanding language english synthese 200.6 1-16 issn 0039- 7857 doi 10.1007 s11229-022-03931-4 spanish concordancer bruno spanish corpus accessed 2024-10-22 2010 https://www.lextutor.ca/conc/ span/   98 turing a. m. 1950 computing machinery and intelligence english mind new serie 59236 433-460   isnn 00264423 http://www.jstor.org/stable/2251299 vaswani a. et 2017 attention is all you need i. guyon et advanz in neural information   processing systems vol. 30 currar associat inc https://arxiv.org/abs/1706.03762 wei j. et 2022 emergent abiliti of large language models tran mach learn r 2022 http://dblp uni-trier.de/db/journals/tmlr/tmlr2022.html#weitbrzbybzmchvldf22 weizenbaum j. 1966 elizar computer program for the study of natural language communication   betweir man and machine commun acm 9.1 enir 1966 36-45 issn 0001-0782 doi   10.1145/365153.365168 url http://doi.acm.org/10.1145/365153.365168 workshop b. et 2023 bloom 176b-parameter open-access multilingual language model arxiv   2211.05100 cs.cl https://arxiv.org/abs/2211.05100 xu z. jain s. kankanhalli m. 2024 hallucination is inevitable an innatar limitation of large language   models arxiv 2401.11817 cs.cl https://arxiv.org/abs/2401.11817