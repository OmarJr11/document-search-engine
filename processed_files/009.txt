red neuronal artificial fernando izaurieta carlos saavedra departamento fsica universidad concepcin concepcin chile resumir charla entregar descripcin caracterstica principal funcionamiento red neuronal artificial lugar presentar modelo sencillo red neuronal familia problema modelar adems describir esquema simple entrenamiento red orientado reconocimiento patrón informacin presentar ejemplo aplicacin red reconocimiento texto 1 introduccin actividad investigacin desarrollado torno estudio red neuronal artificial simplemente red neuronal neurored estn motivada modelar forma procesamiento informacin sistema nervioso biolgico especialmente forma funcionamiento cerebro humano completamente distinto funcionamiento computador digital convencional cerebro humano corresponder sistema altamente complejo no-lineal paralelo trmino sencillo equivaler operación simultneamente diferencia computador común tipo secuencial realizar slo operacin sentido neurored procesador informacin distribucin altamente paralelo constituido unidad sencillo procesamiento llamado neurona neurored caracterizar principalmente   inclinacin natural adquirir conocimiento travs experiencia almacenar cerebro peso relativo conexión interneuronal   altsimo plasticidad adaptabilidad capaz cambiar dinmicamente   poseer alto nivel tolerancia falla sufrir dao considerable continuar tener comportamiento ocurrir sistema biolgico   comportamiento altamente no-lineal permitir procesar informacin procedente fenmeno no-lineal motivación principal estudio funcionamiento red neuronal encontrar fenmeno neurolgico cerebro procesador informacin muchsimo ms eficiente computador clave inmenso plasticidad cerebro tarea cotidiano cerebro impensable computacin tradicional ejemplo capacidad reconocer persona tiempo 100 200 ms lapso cerebro capaz procesar patrn informacin tridimensional ejemplo persona quizs cambiar aspecto lucir distinto simplemente envejeci paisaje cambiante contener rostro actualidad tarea ms simple consumir da trabajo computador ms veloces plasticidad percibir tambin capacidad responder forma correcto frente estmulo recibido capacidad presentar alguien sepamos automticamente persona objeto biolgico caracterstica neurored convertir ayuda procesamiento dato experimental comportamiento complejo adems comportamiento iterativo lineal une natural caos teora complejidad posibilidad amplio empezar hablar campo aparte biologa matemtica fsica neurociencia decir desear inicialmente imitar parcialmente funcionamiento cerebro revisarar superficialmente concepto bsico neurobiologa 2 neurobiologar neurona tpico poseer aspecto parte mostrar figura 1 deber observar dibujo est escala axn alcanzar tpico centmetro metro dendrita tambin terminal sinptica ms larga numeroso tupida figurar 1 neurona parte tpicamente neurona 6   5 rdén magnitud ms lenta compuerta lgico silicio evento chip silicio tomar nanosegundo 10 9 s neurona tiempo   2 f. izautireta c. saavedra orden milisegundo 10 3 cerebro compensar forma excepcional lentitud relativo funcionamiento neuronal nmero inmenso neurona interconexión masivo estimar nmero neurona cerebro orden 1010 nmero conexión sinptica 6   1013 red resultante cerebro estructura enormemente eficiente especficamente eficiencia energtico cerebro 10 16 j=(operación   s orden 1010 mejor computador actualidad mayora neurona codificar salida serie breve pulso peridico llamado potencial accin originar cercano soma clula propagar travs axn pulso llegar sinapsis ah dendrita neurona sinapsis interconexin neurona dibujo esquemtico incluir figura 2 botn sinptico corresponder trmino axn neurona pre-sinptico dendrita correspondiente neurona post-sinptico 50 nm botn sinptico dendrita figurar 2 salto sinptico tipo ms comn sinapsis sinapsis qumico funcionar seal neural elctrico pre-sinptica llegar botn sinptico figura 2 all sta vescula sinptica azul figura rompan liberndo él as sustancia llamado neurotransmisor sustancia qumico difundir travs espacio neurona captar dendrita estimular emisin impulso elctrico post- sinptico propagar derecha as ver dendrita zona receptivo neurona axn lneo transmisin botón terminal comunicar impulso neurona neurona comportamiento importantsir impulso llegar sinapsis salir igual general tipo pulso saldr depender sensiblemente cantidad neurotransmisor cantidad neurotransmisor cambiar proceso aprendizaje aqu almacenar informacin sinapsis modifico pulso reforzndolo debilitndolo soma sumar entrada dendrita entrada sobrepasar umbral transmitir pulso axn caso contrario transmitir despus transmitir impulso neurona transmitir tiempo 0.5 ms 2 ms tiempo llamar perodo refractario base caracterstica construirar modelo red neural 3 modelo neuronal aqu desear introducir modelo sencillo neurona construir red ltimo modelar correctamente comportamiento global red pretender modelar exactamente comportamiento fisiolgico neurona ms slo caracterstica ms relevant entrar juego interaccin red esquema neurona figura 3 l neurona inters yj n neurona xi estn enviar seal entrada valor numrico valor wj i representar peso sinptico dendrita yj obsrvese notacin ndice denotar neurona dirigir informacin ndice denoto qu neurona proceder informacin xi xn x1 wj1 wji wj0 yj v1j v2j j in figurar 3 esquema neurona peso sinptico simplemente multiplicar entrada correspondiente definir importancia relativo entrada recorder soma neurona biolgico sumar entrada proveniente dendrita entrada neurona yj y(i n j = n x i = 1 wj i xi 1 ndice in denotar input entrada mencionir neurona activar entrada superar umbral aplicar funcin activacin y(i n j ejemplo funcin tipo escaln sigmoidea tangente hiperblico seal output salida neurona yj yj =   y(i n j   2   red neuronal artificial 3 3.1 función activacin función activacin tpica lineal presentar figura 4 5 y(in figurar 4 escaln y(in figurar 5 sigmoidear función evidentemente transmitir idea disparar umbral neurona función activacin dividir tipo bipolar antisimtrica binaria primeras -a   yj   generalmente = 1 segunda 0   yj   1 adems soler funcin activacin relacin lineal generalmente funcin identidad general neurona entrada red sensor evidentemente esperar sensor indicar precisamente est percibir funcin activacin neurona lineal decimos neurona lineal caso contrario decimos neurona lineal aqu  neurona lineal representar cuadrado lineal crculo 3.2 umbral inclinacin anteriormente explic neurona activo disparar entrada superar umbral deseable modificar umbral ms difcil neurona dispare subir umbral ms fcil bajar umbral directamente soler engorroso programar 1= x0 wj0 xi xn x1 wj1 wji wj0 yj v1j v2j j in figurar 6 esquema inclinacin resultar ms compacto prctico aadir llamar neurona inclinacin x0 asignar valor fijo 1 peso sinptico wj 0 neurona yj asignar umbral fijo cero ver claramente equivalente neurona yj umbral   wj 0 y(i n j = n x i = 0 wj i xi x0 = 1 3 3.3 comienzo mcculloch-pitts despus definición previo conveniente revisar ejemplo sencillo instructivo tema consiste modelo crar red neural ao 1943 construyerar computador mcculloch siquiatro neuroanatomista pitts matemtico pas 20 ao estudiar cul representacin evento sistema nervioso modelo siguiente caracterstica neurona tipo binario 0 1 umbral sinapsis mantener fija funcin activacin tipo escaln demostrar función lgica describir combinación apropiado neurona tipo podar crear principio red capaz resolver funcin computable adems modelo servir explicar fenmeno biolgico sencillos forma describir función lgica   4 f. izautireta c. saavedra 1 1 x1 x2 x1 x2 1 1 1 0 1 0 1 0 0 0 0 0 figura 7 funcin and 2 2 x1 x2 x1 x2 1 1 1 0 1 1 1 0 1 0 0 0 figura 8 funcin or ejemplo suponer umbral neurona lineal 2 =   0 yi n < 2 1 yi n   2 4 fcil comprobar tabla efectivamente cumplen1 ejemplo lnea tabla and 1   1 + 1   1 = 2 = yi n = 1 ver funcin lgico ms xor or excluyente 2 2 z x1 x2 z 1 1 0 0 1 1 1 0 1 0 0 0 x1 x2 y1 y2 2 2 -1 -1 fig 9 funcin xor fcil comprobar red mostrado efectivamente cumplir tabla llamar atencin red ms complejo función and or pese slo diferenciar funcin or lnea pudiramos dar yo trabajo buscar red representar xor buscar ms sencillo red tambin representar sencillo and or fijmono consistir complejidad red and or neurona entrada salida estn conectado directamente cambio demostrar funcin xor habr conexin indirecto entender diferencia incorporar concepto problema linealmente separabl capa neural 3.4 problema linealmente separabl capa neural 3.4.1 problema linealmente separabl volver red simple and or ms general figura 10 aadir neurona inclinacin umbral w2 w1 x1 x2 x0 w0 1 figura 10 funcin lgico simple entrada y(i n dar y(i n = w0 + w1x1 + w2x2 5 respuesta 1se asumir 1 = 0 = falso   red neuronal artificial 5 =   0 y(i n < 0 1 y(i n   0 6 dividir plano formado x1 x2 región tendr = 0 y(i n < 0 tendr = 1 y(i n   0 frontera ambos est dado ecuacin lineal recta w0 + w1x1 + w2x2 = 0 ver ejemplo ocurrir funcin and y(i n = x1 + x2   2 frontera x1 + x2 = 2 superponer respuesta arrojar red grfico región obtenemos figura 11 x 1 x 2 0 1 0 1 1 0 0 0 c s   1 c s   0 x 1 + x 2 = 2 figurar 11 and plano entrada est regin clase 1 producir salida 1 est clase 0 salida 0 ver separar entrada producir salida 1 producir salida 0 lneo recto problema linealmente separable resolver problema linealmente separable bastar red sencillo reviser cambio funcin xor plano x1 x2 0 1 0 1 0 1 0 1 figura 12 xor plano simple inspeccin comprobar efectivamente imposible encontrar lneo recta dejar entrada producir 0 producir 1 caso decimos problema linealmente separable bastar red sencillo resolver xor realidad caso sencillo problema general clasificacin patrón clasificar entrada clase 1 clase clase 0 clase falso concepto separabilidad lineal extender natural entrada ms dimensión entrada pertenecer clase pertenecer simplemente separar él hiperplano np i = 0 wj i xi = 0 espacio x entrada clase 2 clase 1 linealmente separable clase 2 clase 1 linealmente separable      wji xi = 0 i = 0 n espacio x figura 13 separabilidad lineal aclarar concepto red sencillo revisarer concepto capa neural 3.4.2 capa neural trabajar cantidad neurona natural ordenar comportamiento similar capa figura 14 ah usen subndiz neurona capa vector neurona   6 f. izautireta c. saavedra w10 wj0 wn0 x0 w11 wj1 wn1 x1 w1 m wjm wnm w1i wji wni xi xm yn yj y1 1= entrada salido capa 0 capa 1 figura 14 red unicapo acostumbrar contabilizar capa entrada red figura 14 unicapar sinapsis obviamente estn ordenado matriz wj i n   m + 1 evidentemente anlisis red unicapa slo resolver problema linealmente separabl red unicapo neurona salida lineal lineal evidente seguir aadeir capa mostrar figura 15 u10 uj0 un0 x0 u11 uj1 un1 x1 u1 m ujm unm u1i uji uni xi xm 1= entrada capa 0 capa 1 v10 vk0 vp0 v11 vk1 vp1 w1n wkn wpn v1i vkj vpj y0 zp zk z1 capar 2 salida 1 yj y1 yn figura 15 red multicapo   red neuronal artificial 7 red multicapo capa oculto figura corresponder capa 2 lineal demostrar fcilmente construir red multicapo capa oculto lineal sta equivalente red unicapo fcilmente idea paralelismo observar capa red neurona capa necesitar dems capa trabajar capaz trabajar simultneamente cualidad aprovechar disear chip paralelo tecnologa vlsi very large scalir integrated implementar tipo neurored red multicapo capaz resolver problema ms complejo proceso aprendizaje tambin ms complicado 4 aprendizaje entrenamiento aprendizaje clave plasticidad neurored esencialmente proceso adaptar sinapsis red respondar distinto estmulo recorder neurored informacin adquirido guardar valor peso sinptico neurona ser vivo sistema nervioso caracol hombre esencialmente igual ms inteligente caracol nmero organizacin cambio conexión sinptica aprendizaje dividir principalmente tipo aprendizaje profesor supervisado profesor supervisado slo estudiarer aprendizaje profesor variante ste 4.1 aprendizaje profesor supervisado proceso completamente anlogo ensear él nio decir ejemplo reconocer vocal paso proceso siguiente profesor disponer conjunto n par entrenamiento f xi n dj n)gn n= 1 xi n n-simo entrada dj n respuesta correcto entrada ejemplo significar vocal dibujada papel xi n respuesta correcto dj n figura sonido i u. introducir entrada xi n esperar red respondar mostrar él nio letra preguntar él dime qu letra   neurored responder salida oj n decir nio respondi e.   comparar ambos seal respuesta deseado dj n respuesta red oj n crear seal error ej n = dj n   oj n mmm nio est despierto esperar   seal error ej n corrijo sinapsis red algn  algoritmo continuacin hijo dj n + xi n   oj n       ej n profesor xi(n);dj(n neurored figurar 16 aprendizaje profesor supervisado secuencia completo n par entrenamiento conocer general aprendizaje detener red respondar correctamente par entrenamiento general adapter sinapsis forma ecuacin wj i n + 1 = wj i n +   wj i n 7 wj i n peso sinptico red responder n-simo ejemplo equivaler cambiar peso sinptico forma radical simplemente variar cantidad pequeo   wj i n diferenciar algoritmo regla aprendizaje bsicamente encontrar   wj i n haber distinto algoritmo base biolgico neurona distinto parte cerebro aprender forma distinto tambin 4.2 regla hebb ms antiguo ms famoso regla aprendizaje base completamente biolgico encontrar neurofisiologo hebb 1949 descubri neurona lado sinapsis activo inactivo simultneamente sinapsis reforzar activar desactivar asincrnicamente debilitar forma expresar idea forma sencillo   wj i n =   yj n)xi n   > 0 8) capa neurona xi yj estn distribuido figura 14 constante proporcionalidad   llamar razn aprendizaje funciona suponer xi yj bipolar antisimtrica = 1 xi   8 f. izautireta c. saavedra yj tomar ambos simultneamente valor 1 -1   wj i n =   sinapsis reforzar cambio tomase valor -1 1   wj i n =    sinapsis debilitar aprendizaje explicar famoso experimento pavlov l dar alimento perro simultneamente tocar campanilla despu repetir pavlov toc slo campanilla alimento perro slo oir campanilla salivar explicacin simple activar él simultneamente neurona controlar salivacin percibir campanilla sinapsis reforzar 4.3 aprendizaje red unicapa 4.3.1 regla aprendizaje perceptrnico objetivo funcionamiento general regla aprendizaje est diseado especialmente reconocimiento patrón red unicapo slo patrón linealmente separabl perceptrn naci intento modelar retina 1958 rosenblatt usual pensar retina simplemente receptor detector ccd cmara vdeo realidad red altamente complejo slo poder reproducir ojo robot retina artificial ciego ltima dcado llamado circuito neuromrfico retina adems receptor capaz reconocer movimiento borde adaptar él cambio local brillo perceptrn red capa figura 14 neurona salida lineal funcin activacin tipo escaln experimento numrico utilizamos función activacin bipolar antisimtrica yj = 8 > < >   1 y(i n j < 0 0 y(i n j = 0 1 y(i n j > 0 9 ntesir incluy punto neutro soler llamar punto indeterminacin regin torno origen producir salida cero llamar banda indeterminacin simplemente neurona responder neurona salida representar clase determinado activar entrada significar pertenecer clase est desactiva pertenecer aqu  incluimos experimento clasificar imgén letra entrada xi corresponder i-simo pxel imagen decir ejemplo red clasificar entrada x o. funcionir mostrar figura 17 neurona marcado x reconocer clase x clase x 1 s n x   -1 n   n     x -1 n   n   x   1 s n   x figura 17 funcionamiento perceptrn algoritmo perceptrnico ver entrenar red mo m1 nmero neurona entrada salido respectivamente adems n par entrenamiento f xi n dj n)gn n= 1 forma algoritmo pasar 0 inicializar sinapsi red elegir wj i 0 = 0   valor aleatorio elegir razn aprendizaje   0 <    1 paso 1 condicin parada paso 5 falso paso 2 5 pasar 2 par entrenamiento xi n dj n n = 1 n paso 3 4 pasar 3 j = 1 m1 y(i n j n = m 0 x i = 0 wj i n)xi n yj n = 8 > < >   1 y(i n j n < 0 0 y(i n j n = 0 1 y(i n j n > 0   red neuronal artificial 9 paso 4 yj n 6= dj n algn j 1 m1 wj i n + 1 = wj i n +   dj n)xi n j = 1 m1 i = 0 m0 caso contrario wj i n + 1 = wj i n pasar 5 peso sinptico cambiar patrn entrenamiento ltimo realiz paso 2 parar as continuar ver claramente caso   wj i n =   dj n)xi n 0 depender error entender intuitivamente algoritmo forma suponer j -sima neurona respondi forma incorrecto -1 1 significar y(i n j n pequeo deber crecer ms trmi yo sumatorio m 0 p i = 0 wj i n)xi n positivo mximo i-sima entrada xi n +1 i-sima sinapsis wj i n deber positivo ms tambin deber hacer él crecer contrario xi n -1 deber bajar wj i n rir   ejar forma construir   wj i n dj n +1   wj i n signo xi n caso contrario revs evidente problema linealmente separable infinito peso sinptico servirn solucionar problema bastar multiplicar constante ecuacin np i = 0 wj i xi = 0 seguir tener hiperplano separacin distinto peso sinptico adems generalmente hiperplano podrar delimitar frontera ms infinito mostrar figura 18 clase 2 clase 1 espacio x figura 18 infinito solución solucin infinito demostrar solucin algoritmo perceptrnico converger infinito solución nmero finito paso experimento computacional ejemplo incluir experimento computacional clasificacin letra siguiente entrada figurar 19 patrón entrenamiento experimento 1 imagen 7   9 = 63 pxels pxel negro corresponder +1 blanco -1 us   = 1 sinapsis inicializar 0 construir vector xi entrada simplemente poner fila imagen despus despus entrenamiento patrón clasificar correctamente siguiente   10 f. izautireta c. saavedra c d k aqu observar funcionamiento red construir simple plasticidad capaz generalizar ver patrón error entrenamiento capaz reconocer qu letra corresponder ampliar experimento preguntar podr patrón ms qu imagen hacer yo cmo estn distribuido sinapsis responder pregunta construimos perceptrn slo clasificar x i entrada resolucin 56   72 = 4032 pixel trabajar exactamente ejemplo patrón entrenamiento siguiente figurar 21 patrón entrenamiento 2 necesitar slo patrón clasificar correctamente x i nuevamente observar plasticidad cmo distribuir sinapsis ver él grfico simplemente reordenar forma imagen original sinapsis obtenindo él 3 grfica sinapsis conectar neurona x i. x             i 4 0 -4 figurar 23 sinapsis x i. simplemente observar entender cmo  funcionar sinapsis qu región ms crucial reconocer patrn decir sinapsis nico empezar valor inicial aleatorio llegar tipo conexión sinptica x             i 4 0 -4 figurar 24 sinapsis x i. pasarer tarea realizar neurored predecir 4.3.2 regla delto correccin error regla popular red capa perceptrnico neurona salida funcin activacin derivable generalmente funcin identidad tangente hiperblico regla algoritmo ms sencillo simplemente calcular error ej n = dj n   yj n correspondiente entrada corregir sinapsis red regla   wj i n =   ej n 0 j y(i n j n))xi n 10 neurona salida identidad funcin activacin 0 j y(i n j n = 1   red neuronal artificial 11   wj i n =   ej n)xi n forma ms comn algoritmo regla realidad caso particular sencillo algoritmo retropropagacin error convergencia algoritmo depender fuertemente valor   elegir pequeo convergencia har lento elegir proceso volver inestable converger criterio determinar cota superior   soler complicado experimento simplemente recurrimos ensayo error soler ms rpido predictor lineal filtro lineal adaptativo suponer sistema dinmico describir nico parmetro x. nico conocer l historia muestrear x perodo t. predecir cul respuesta sistema prximo instante interaccin red sistema dinmico mostrado figura 25 sistema dinmico + x n+1]t wi wn w1 w0 x n+1]t    wi(n e(n 1 x nt x it x t x figurar 25 predictor lineal aqu ver papel profesor llevar automtico sistema dinmico red conocer entrada x t x nt predecir valor x n + 1 t papel respuesta deseado jugar x n + 1 t entrada historial proceso completamente anlogo proceso aprendizaje profesor nmero neurona entrada aumentar constantemente experimento computacional   = 0:01 neurona salida funcin identidad sistema dinmico seal senoidal variación aleatorio 50 100 150 200 250 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 tiempo t x  sist dinmico neurored figurar 26 experimento prediccin observar fcilmente medida red aprender capaz predecir 4.4 aprendizaje red multicapo romper limitación anterior estudiar caso nolineal deber recordar tipo red función activacin capa oculto lineal adems ver ecuación necesitar funcin activacin diferenciable dominio adems algoritmo aprendizaje ms difcil visualizar slo estudiar tipo aprendizaje aprendizaje retropropagador error 4.4.1 estructura notacin general estructura red mostrar figura 27 capa salida capa l-sima ml neurona entrada capa 0 m0 neurona decir red l capas l llamar profundidad red   12 f. izautireta c. saavedra   m0     m1          m2   ml-2       ml-1      ml   salida entrado figurar 27 red multicapo supondrer capa neurona inclinacin general dibujarer diagrama general neurona capa estn completamente conectado funcionamiento red encontrarer tipo seal seal funcin seal error seal funcin estmulo entrar capa 0 pasar capa capa tradicional ltimo capa l generar seal salida seal error etapa venir retropropagacin error atrs corregir sinapsis corregir capa l observar sinapsis capa l corregir capa l   1 as sucesivamente capa llamar seal error ltima capa primero corregir sinapsis ilustrar figura 28 seal funcin seal error 2 etapa figurar 28 etapa atrs 4.4.2 definición error suponer capa salida est constituido neurona zk error cometido presentar él n-simo par entrenamiento ek(n = dk(n   zk(n 11 energo error energa error presentar él n-simo par entrenamiento n = 1 2 m l x k= 1 e2 k(n 12 energa fsico jerga neurored slo llamar as forma anlogo energa cintico fsico trabajar campo emplear trmino fsica energo promedio error promedio energa error completa presentacin patrón pro = 1 n n x n= 1 n 13 n pro función sinapsis red objetivo proceso aprendizaje minimizar pro wj i sinapsis cualquiera red fcil av(wj i n)(wj i constituir superficie error idea algoritmo descenso pasar paso aproximacin aclarar concepto   red neuronal artificial 13 gradiente av sealo direccin crecimiento evidentemente venir @j i pro(wj i = pro @wj i 14 minimizar pro deberamos dirigirno gradiente ver relacin wj i p + 1 = wj i p    pro @wj i p 15 p simplemente sealo p-simo paso esquiar resbalarno superficie error tratar llegar mnimo global superficie corrar peligro quedar atrapado minmo local superficie alcanzar mnimo global ilustrar figura 29 av wji figurar 29 peligro caer mnimo local intentar evitar tratar minimizar n pro especial explicar seccin 4.4.3 idea algoritmo intentar minimizar av minimizar n tener wj i n + 1 = wj i n    @"(n @wj i n 16 patrn presentar superficie error n presentar n- simo par entrenamiento corregir sinapsi red n-simo superficie resbalar paso presentar n + 1)-simo par entrenamiento corregir nuevamente sinapsi red cambiamo superficie resbalar paso constante cambio superficie difcil quedar atrapado mnimo local imagen mental esquiar montaa est temblar alocadamente evidentemente llegarer valle ms profundo existir poceso ilustrar figura 30 n wji n+1 wji figurar 30 esquivar minmo local suponer implcitamente promedio cambio individual sinapsis estimador cambio deber ocurrir minimizarar directamente pro adems orden presentacin par entrenamiento randomizar trayectoria seguido superficie completamente estocstico suponer randomizrar conjunto entrenamiento tendrar estaramo repetir procedimiento llammoslo f estaramos iterar wj i n + 1)-poca = f wj i n-poca 17 teora cao proceso converger metaestabl ciclo lmit eliminar posibilidad intentar randomizar conjunto entrenamiento   14 f. izautireta c. saavedra wj i 1-poca = f wj i 0-poca wj i 2-poca = g(wj i 1-poca wj i 3-poca = h wj i 2-poca etc. 4.4.4 algoritmo retropropagacin error considerer red figura 31 dibujar sinapsis slo sealamos   echa red seguir tener ms capa oculto atrs demostrar   wkj n =   @"(n @wkj =   k(n)yj n k = 1 ml j = 0 ml   1 18   vj  i n =   @"(n @vj i =   j n)xi n j = 1 ml   1 i = 0 ml   2   wkj   yj   zk   vji   xi ml-2                        ml-1                         ml salido figura 31 red multicapa cambio sinapsis directamente proporcional seal enviado capa   wkj yj   vj i xi as capa red capa salida oculto gradiente local l =   n @y i n l calcular forma capa salida l k n = e(l k n 0 k(z(i n k k = 1 ml 19 aadir superndice l notacin usar estrictamente necesario recalcar referir capa salida 0 k 0 significar derivado argumento subndice k referir neurona general poder funcin activacin distinto resultado tenar filtro lineal adaptativo capa as venar ecuacin simplemente estbamos tratar minimizar av forma descrito capa oculto l   1 j n = 0 j y(i n j n m l x k= 1 l k n)w(l kj n 20 j = 1 ml   1 aqu ver l   1 j depender l k capa ms capa oculto exactamente correccin depender forma suceder capa ms referar seal error atrs mtodo evitar mnir local llamar mtodo secuencial pese riesgo caer mnir local minimizar directamente av llamndo él mtodo grupal usado caracterstica estocstica mtodo secuencial evitar caer mnimo local difcil establecer tericamente convergencia cambio mtodo grupal convergencia mnimo local est garantizado mnimo condición pese mtodo secuencial altamente popular simple implementar computacionalmente adems efectivamente funcionar inmenso mayora caso difcil   red neuronal artificial 15 5 conclusión trabajo pretender pequea introduccin caracterstica neurored conocido an estudio ms avanzado tema estn alejado entender funcionamiento cerebro motivacin inicial tema realidad vasto pese ver nfimo poder apreciar cualidad mecanismo procesamiento informacin deber lugar destacar modelar funcionamiento neurona forma extremadamente simple poseer capacidad ver sencillez complejidad unidas maravilloso ejemplo describi posibilidad procesar cantidad increbl informacin forma paralelo sencillo natural establecer él función lgica combinacin neurona ver tambin posibilidad construir computadora principio caracterstica fundamental olvidar robustez capacidad aprendizaje neurona capaz imitar predecir comportamiento sistema dinmico ningn modelo explcito capaz reconocer patrón sto tener error adems interesante fsica procesar informacin s ltimo descubrir interesante rea relacionar neurored teora informacin caos mecnica estadstico 6 bibliografa 1 laurene fausett fundamentals of neural networks prentice-hall new yersey 1994 2 simon haykin neural networks prentice-hall new yersey 1999