deep learning isaac prez borrero manuel gegndez aria deep learning fundamento teora aplicacin deep learning fundamento teora aplicacin isaac prez borrero manuel emilio gegndez aria edicin formato ebook abril edicin formato papel abril servicio publicación universidad huelva isaac prez borrero manuel emilio gegndez aria papel epub depsito legal h l n s b r b papel cartulina grfica g estucado mate g encuadernacin encuadernacin pur printed in spain impreso espaa maquetacin ebook jos antonio sala hernndez dato s d i c i n publicación universidad huelva miembro une pa p l ebook permitir volver ndice pulsar pie pgina compartir librosuhu publicación movimiento citar libro navegar marcador hipervnculo nota bsqueda interno nete comentar novedad golpe clik suscrbete novedad prez borrero isaac deep learning fundamento teora aplicacin isaac prez borrero manuel emilio gegndez aria huelva universidad huelva pgina cm serie alonso barba isbn papel isbn epub inteligencia artificial aprendizaje automtico gegndez aria manuel lio ii gonzlez galn mara dolor iii universidad huelva ver ttulo serie c p indicacin contrario contenido edicin electrnico distribuir licencia distribucin creativir commons reconocimiento comercial compartir internacional cc ndizar prefacio agradecimiento introduccin red neuronal introduccin perceptrn función activacin principal aplicación clasificacin regresin limitación red neuronal arquitectura aplicación limitación desarrollo red neuronal etapa entrenamiento vs inferencia paradigma aprendizaje obtencin preparacin dato mtrica evaluacin estimacin error aprendizaje basado gradiente inicializacin peso principal algoritmo optimizacin backpropagation problema aprendizaje basado gradiente parmetro entrenamiento control seguimiento entrenamiento red neuronal recurrente introduccin red neuronal recurrente entrenamiento red neuronal recurrente aplicación limitación deep learning introduccin red neuronal convolucional arquitectura aplicación arquitectura popular bibliografa ndice figura evolucin ratio nmero publicación conexionismo simbolismo parte neurona biolgico neurona interpretacin geomtrico perceptrn modelar funcin or interpretacin geomtrico perceptrn modelar funcin or variable perceptrn vector normal hiperplano definido peso perceptrn perceptrn bia decidir comprar casa ejemplo hiperplano funcin valor divisinir espacio hiperplano ejemplo perceptrn decidir comprar casa perceptrn entrada sesgo funcin activacin identidad perceptrn entrada sesgo funcin activacin escalonado perceptrn entrada sesgo funcin activacin sigmoidir perceptrn entrada sesgo funcin activacin tanh perceptrn entrada sesgo funcin activacin relu clasificacin multiclar enfoque hiperplano lneo punteado problema clasificacin izquierda regresin derecha ejemplo separacin lineal hiperplano funcin or and ejemplo outlier dato ejemplo dato separabl linealmente funcin xor salida perceptrn aproximar funcin or salida perceptrn aproximar funcin not and salida perceptrn aproximar funcin and perceptrón aproximar funcin xor salida perceptrn aproximar funcin or salida perceptrn aproximar funcin not and salida construido perceptrón aproximar funcin xor capa red neuronal elemento red neuronal red neuronal aproximar funcin xor transformacin softmax codificacin ejemplo operacin flattening efecto cambio valor peso bia neurona funcin activacin sigmoidir construccin funcin pulso red neuronal construccin funcin pulso ampliado red neuronal aproximacin funcin red neuronal ejemplo funcin aproximado correctamente zona etapa entrenamiento vs inferencia aprendizaje supervisado supervisado refuerzo efecto clasificacin valor umbral matriz confusin n clase matriz confusin considerar clase positivo negativa mtrica problema clasificacin persona segn estn embarazado curir roc mtrico iou grfica funcin ecm grfica funcin ce superficie funcin prdida perceptrn entrada superficie tpico funcin prdida red neuronal evolucin vector peso algoritmo entrenamiento perceptrn representacin grfica funcin derivada posible punto crtico funcin efecto definicin superficie error estrategia optimizacin región superficie error ideal comenzar entrenamiento modificacin variable acercar él mnimo funcin modificacin peso algoritmo descenso gradiente comparativo algoritmo descenso gradiente momento funcin variable plano curva nivel reduccin oscilacin momento ejemplo iteración algoritmo descenso gradiente ejemplo grafo operación ejemplo completo red neuronal problema xor grafo operación red neuronal figura clculo iteracin algoritmo descenso gradiente grafo operación red grfica derivada principal ción activacin ejemplo sobreajuste izquierda ajuste centro infraajuste derecha base conjunto dato ejemplo color morado posible escenario modelo sesgo varianza bsqueda compromiso sesgo rianza definir complejidad modelo ciclo vida desarrollo red neuronal relacin dato conjunto dato tamao lote iteracin entrenamiento diferencia mnimo pronunciado aplanado funcin prdida definido lote ejemplo definido dato grfica habitual entrenamiento red neuronal deteccin sobreajuste infraajuste grfica prdida efecto valor learning ratar grfica prdida diferencia descenso gradiente valor learning ratar fijo variable superficie funcin prdida dato normalizado normalizar efecto regularizacin complejidad modelo red dropout efecto limitacin gradiente entrenamiento informacin instante anterior predecir posicin vagn base informacin predecir vagn mover derecha esquema red neuronal recurrente esquema red neuronal recurrente multicapo divisin ventana secuencia dato iteracin entrenamiento entrenar validar v prueba p proceso desenrrollado grafo operación iteración red neuronal recurrente relacin inteligencia artificial chine learning deep learning comparativo desarrollo sistema basado tcnica machine learning deep learning descomposicin pxel imagen representacin jerrquico caracterstica menor complejidad esquema completo modelo deep learning etapa evolucin porcentaje error cometido modelo ganador imagenet ltimo ao comparativo rendimiento modelo machine learning deep learning base tamao conjunto dato disponible coste almacenamiento dato frente disponibilidad izquierda derecha yann lecun geoffrey hinton yoshua bengio panorama industrial deep learning experimento llevado cabo hubel wiesel localizacin corteza visual cerebro jerarqua caracterstica obtener cerebro informacin visual retina clula s compartir peso extraer caracterstico región entrada estructura neocognitron caracterstica extrada capa red neuronal convolucional mostrar imagen producir activacin caracterstica ejemplo imgén real conjunto dato producir alto activacin caracterstica extrada capa red neuronal convolucional izquierda mostrar imagen producir activacin caracterstica derecha ejemplo imgén real conjunto dato producir alto activacin caracterstica extrada capa red neuronal convolucional izquierda mostrar imagen producir activacin caracterstica derecha ejemplo imgén real conjunto dato producir alto activacin ejemplo dato representacin denominacin base dimensin ejemplo grfico clculo convolucin función continuo ejemplo grfico clculo convolucin función discreto representacin convolucin pieza informacin dimensin tamao limitado representacin convolucin pieza informacin relleno representacin convolucin pieza informacin paso representacin convolucin pieza informacin dimensión convolución imagen dimensión canal rojo verde azul informacin relativo capa convolucin comparativo capa red neuronal capa convolucin mejora resultado aumento profundidad red error error cometido red clase esperado primero clase modelo ejemplo clculo average pooling ejemplo bloque inception ejemplo skip connection ejemplo dense block principal caso deep learning izquierda derecha clasificacin teccin segmentacin red conduccin autnoma tesla hydranet red neuronal convolucional lstm usado describir imagen reescalado imgén reconstruccin coloreado imgén representacin invariante objeto transferencia estilo imagen estimacin profundidad estimacin po él modelo basado deep learning entrenado aprendizaje refuerzo jugar juego atari modelo basado deep learning entrenado aprendizaje refuerzo jugar juego go conduccin autnoma modelo deep learning entrenado aprendizaje refuerzo manipulacin objeto modelo deep learning entrenado aprendizaje refuerzo procesamiento lenguaje red neuronal convolucional reconocimiento audio red neuronal convolucional combinado red neuronal recurrente ejemplo segmentación instancia realizado red mask arquitectura red arquitectura red alexnet arquitectura red googlenet arquitectura red arquitectura red ndice tabla salida neurona ejemplo entrada principal función activacin función booleana variable función booleana separabl linealmente ratio funcin nmero variable entrada salida perceptrón red neuronal aproximar funcin xor ndice algoritmo algoritmo entrenamiento perceptrn algoritmos basado gradiente prefacio estudio materia fcil caso deep learning adems aadir temprano edad carcter multidiciplinar englobar concepto campo matemtica neurociencia informtica motivo libro pretender base estudio deep learning espaol permitir adquirir concepto necesario dominio nico obra intentar explicar forma amena pormenor matemtico perder rigor perspectiva divulgadoro objetivo ms ameno estudio libro pretender alumno capaz solventar duda conceptual poder surgir él estudio asignatura relacionado deep learning hincapi multitud concepto bsico soler ignorar mayora libro deep learning rar él materia adems introducir multitud ejemplo figura ayudar comprensin concepto ms complejo agradecimiento gustarar agradecer comunidad deep learning acceso libre to conocimiento tecnologa diferencia disciplina encontrar comienzo recurso disponible relacionado deep learning abundar red forma gratuito desinteresado apuesta circulacin libre conocimiento haber revolucin deep learning sociedad ltima instancia creacin libro lnea invitamos profesional seguir espritu compartir conocimiento resto comunidad seguir avanzar campo potencial deep learning errata agradecerar escribierar correo direccin introduccin organizacin libro buscar proveer estudiante conocimiento bsico necesario adentrar estudio deep learning captulo centrar estudio red neuronal modelo desarrollar campo deep learning estudio red abarca historia concepto ms elemental guardar cin captulo aborda desarrollo red neuronal versal captulo libro provee conocimiento rio desarrollar modelo deep learning tercer captulo abordar procesamiento serie temporal red neuronal tipo red necesario abordar problema dato independiente presentar algn  tipo relacin ltimo cuarto captulo profundizar deep learning comenzar pequeo introduccin divulgativo familiarizar estudiante disciplina despu pasar estudiar principal modelo deep ning red neuronal convolucional captulo red neuronal introduccin red neuronal tcnica ms utilizado aprendizaje tico ingls machine learning campo inteligencia artificial englobar algoritmo basado modelo representación matemtica dad predefinido ajustar base dato disponible resolver problema especfico comienzo inteligencia artificial esfuerzo centrar algoritmo permitiesar ordenador tarea humano razonar hablar reconocer ver tarea encontrar algoritmo resuelir caso bsqueda camino ms corto algoritmo ejemplo mayora problema humano resolver forma evidente algoritmo permitir ordenador resolver él ejemplo determinado objeto imagen tarea fecha demostrar extremadamente difcil ordenador limitacin comenz surgir inters tcnica permitierar obtener algoritmo necesidad detallar paso seguir forma similar cmo humano aprender tarea proceso aprendizaje base dato ejemplo aqu aparecer machine learning diferencia tcnica tradicional inteligencia artificial deep learning fundamento teora aplicacin capaz ajustar modelo prediseado base dato disponible problema resolver él tcnica algoritmo machine learning diferenciar cmo definir modelo ejemplo caso red neuronal utilizar modelo matemtico inspirado neurona principalmente distinguir tipo paradigma hora definir modelo simbolismo mo simbolismo basar anlisis formal lgica manipulacin representación abstracta problema gracias turaleza tcnica momento porqu decisin tomado algoritmo permitir ms confianza tcnica hora problema real simbolismo modelo ms usado sistema basado regla rbol decisin conexionismo contrario carecer cualidad autoexplicativo tcnica anterior basar bsqueda patrón dato procesar red nodo conectado distribucin conexión representar modelo problema cuestin principal modelo conexionismo red neuronal historia paradigma ir turnndo él ms metedor principalmente expectativa crear descubrimiento quedar reflejado nmero publicación nada paradigma tiempo figura apreciar figura conexionismo actualmente paradigma gozar popularidad medida resultado obtenido modelo deep learning basado red neuronal seccin estudiar principal componente red nal perceptrn isaac prez borrero manuel emilio gegndez aria figura evolucin ratio nmero publicación conexionismo naranja simbolismo azul normalizado nmero publicación indexado web of science fuente perceptrn orgén perceptrn datar ao inventar trmino inteligencia artificial conferencia dartmouth ao ao warrar mcculloch walter pitts rocientfico especialista lgica publicar modelo mtico neurona cerebro conocido neurona modelo matemtico inspirar neurona biolgico figurar componer parte principal soma dendrita axn soma cuerpo celular encargado agregar impulso nervioso proveniente neurona llegar travs canal entrada dendrita agregar impulso superar umbral neurona activo enva impulso canal salida axn axn conectar dendrita neurona conocer conexin sinptico conexin completo pequeo distancia axn dendrita deep learning fundamento teora aplicacin mite neurona regular impulso enva transmisin excitadora inhibidoro influir activacin neurona receptor figurar parte neurona biolgico fuente neurona unidad bsica procesamiento cerebro isaac prez borrero manuel emilio gegndez aria ver inspiracin warren mcculloch walter pitts desarrollo neurona artificial neurona biolgico neurona artificial mcculloch pitts figura recibir tipo entrada variable ria excitadora xi inhibidora x j neurona realizar agregacin entrada excitadora suma superar to umbral permitir activacin neurona entrada inhibidoro activo caso neurona activar salida valdr cero activar entrada inhibidoro est activo salida valdr cero independientemente agregacin superar valor umbral figurar neurona figura observar analoga neurona biolgico delo matemtico ver entrada dendrita color azul rojo sentar lista variable xn excitadora x x k inhibidora cuerpo neurona soma representado deep learning fundamento teora aplicacin cunferencia recibir entrada realizar agregacin color morado funcin tambin cuerpo neurona definir funcin tivacin color naranja f decidir agregacin activar neurona base superar umbral entrada inhibidora activo ltimo valor obtenido funcin activacin pasar axn salida neurona color verde neurona entrada salida valor binario principal modelo implementacin función nas definicin parmetro describir to deseado neurona continuacin mostrar problema ejemplo podrar emplear modelo neurona diseado mcculloch pitts ejemplo sistema decisin deber cine pelcula considerar sistema nimo gnero pelcula aventura actar johnny depp necesario variable entrada estoyalegre variable representar nimo valer caso estar alegre caso contrario generoaventura variable indicar pelcular gnero aventura valer valer actuajohnnydepp variable indicar actor johnny depp acto pelcula valer caso actir caso contrario ir pelcula cumplir condición construir modelo resolver ejemplo neurona lloch pitts definir parmetro dar condicin activacin modelo ir pelcula cumplir condición valor umbral valor modelo activar entrada estn activo tabla mostrar salida modelo valor posible entrada ejemplo entrada excitadora perceptrn ejemplo corresponder funcin and salida isaac prez borrero manuel emilio gegndez aria activo entrada estn neurona representar función booleana ms común funcin or funcin and funcin not entrada entrada inhibidoro forma cmo funcionar modelo consistir dar él tacin geomtrico entrada modelo punto espacio variable entrada espacio tendr tanto sión entrada ejemplo espacio dimensión deep learning fundamento teora aplicacin cubo discreto contnuo modelo dividir cio parte hiperplano separacin definido ecuacin n xi parte dividir espacio formar punto situado hiperplano provocar activacin formar punto sitar hiperplano activar modelo hiperplano lneo caso particular variable entrada dimensión plano variable dimensión ma general hiperplano referencia caso particular tambin llamar hiperplano frontera decisin cambiar salida modelo cero viceversa tomar modelo funcin or entrada hiperplano frontera decisin caso pondir lnea variable entrada figura mostrar representacin geomtrico modelo figurar interpretacin geomtrico perceptrn modelar funcin or verde indicar entrada activar salida rojo frontera decisin marcar lneo punteado color azul modelo variable binaria espacio variable entrada est definido siguiente punto isaac prez borrero manuel emilio gegndez aria caso salida activo xi caso salida activar xi caso salida activar xi caso salida activar xi figura mostrar hiperplano separacin funcin or variable entrada plano caso modelo til limitado función booleana mecanismo ajuste forma automtico valor umbral definir él previamente adems interesar entrada igual querer asignar ms peso importancia entrada solventar carencia presentar neurona frank rosenblatt presentar perceptrn perceptrn procesar entrada real limitar valor binario permitir deep learning fundamento teora aplicacin figura interpretacin geomtrico perceptrn modelar funcin or variable verde indicar entrada activar salida rojo frontera decisin definir plano formar lnea punteada color azul dir entrada inhibidora valor negativo entrada simular comportamiento adems entrada ponderado peso controlar importancia modelo ajuste umbral aadeir perceptrn entrada valor umbral contar peso permitir ajustar él geomtricamente permitir perceptrn trazar hiperplano separacin necesidad pasar origen gracias aadir trmino independiente definicin peso umbral actar sesgo introducir sistema novedad eliminar funcin activacin cuerpo neurona aadar elemento posterior perceptrn eliminar él funcin activacin perceptrn salida valor rio real hiperplano separacin dividir punto activar salida punto generar valor real positivo estn hiperplano generar valor real negativo caer hiperplano trabajo original autor aplicar funcin escalonado salida perceptrn isaac prez borrero manuel emilio gegndez aria ceptrn podrar prescindir depender problema utilizar funcin adems perceptrn seccin rosenblatt proponer mecanismo ajustar valor peso momento necesario definir mano valor umbral problema establecer valor fijo ajustar él segn problema mejora presentar perceptrn neurona introducir adaline adaptative linear element modelo posterior perceptrn creado profesor bernard widrow alumno ted hoff concreto adaline cir umbral entrada perceptrn separar funcin activacin salida consista nicamente combinacin lineal entrada mejora aadir perceptrn original versin conocer figura mostrar esquema perceptrn figurar perceptrn deep learning fundamento teora aplicacin observar figura cmo agregacin producir resultado multiplicar entrada peso lugar utilizar nicamente entrada adems entrada pasar valor real binario modelo crear sistema decisin trabajar variable real ms importancia peso habitual describir perceptrn ecuacin notacin vectorial cin activacin independiente perceptrn mostrar cin entrada considerar nico vector peso x w wt x literatura encontrar descripcin perceptrn trado correspondiente umbral denotar forma saber entrada representar sesgo sistema fijar valor normalmente autor optar denotar peso multiplicar entrada letra b bia sesgo espaol caso ecuacin perceptrn quedar mostrar ecuacin entrada considerar nico vector peso x w b wt x ecuación relacionado perceptrn ecuacin hiperplano venir venir definido peso indicar ecuacin wt x hiperplano venir definido vector normal w perpendicular hiperplano figurar vector normal apuntar regin espacio quedar punto salida perceptrn positivo satisfacer wt x isaac prez borrero manuel emilio gegndez aria figura vector normal hiperplano definido peso perceptrn ejemplo aplicación ceptrn ejemplo sistema decidir comprar casa base precio mil euros superficie ejemplo podrar modelar él neurona entrada variable real adems estudiar siguiente sección construir conjunto dato ejemplo indicar qu salida modelo desear valor precio superficie posteriormente modelo ajustar peso imitar comportamiento inherente dato algoritmo entrenamiento figura mostrar perceptrn bia ejemplo observar punto espacio caracterstica caracterstica precio tamao entrada vlido modelo salida valor real utilizar funcin activacin escalonado funcin escaln unitario salida valor binario representar activacin neurona punto activar salida deep learning fundamento teora aplicacin generar valor positivo salida perceptrn sern generar valor salida funcin activacin figurar perceptrn bia decidir comprar casa indicar punto ejemplo modelo capaz clasificar punto plano color verde indicar punto activar salida modelo rojo isaac prez borrero manuel emilio gegndez aria observar hiperplano perceptrn ver efecto utilizar peso permitir rotacin funcin valor hiperplano presentar ms pendiente posibilidad positivo negativa figura mostrar hiperplano ejemplo funcin valor figura mostrar divisin espacio hiperplano punto espacio activar salida color verde color rojo figurar ejemplo hiperplano funcin valor perceptrn trmino independiente definir plano gracias utilizar umbral entrada perceptrn podran crear modelo cuyo hiperplano tener pasar origen deep learning fundamento teora aplicacin figura divisin espacio hiperplano ejemplo color verde indicar espacio generar valor positivo salida perceptrn rojo generar valor negativo necesario tuviramos restriccin tipo comprar casa mnimo figura mostrar hiperplano perceptrn ejemplo aadisar restriccin isaac prez borrero manuel emilio gegndez aria figura perceptrn decidir comprar casa mnimo indicar punto ejemplo modelo capaz clasificar punto plano color verde indicar valor activar salida modelo rojo deep learning fundamento teora aplicacin tabla salida neurona ejemplo entrada estoyalegre generoaventura actuajohnnydepp voyalcine isaac prez borrero manuel emilio gegndez aria función activacin estudiar seccin introduccin perteceptrn funcin activacin dejar formar pasar elemento posterior perceptrn actar salida permitir modificar salida perceptrn sentido base problema est abordar comienzo aplicacin limitar función nas neurona emplear funcin escalonado valor representar activacin cero activacin llegada perceptrn par trabajar valor real ampliar posible aplicación ejemplo querer utilizar perceptrn indicar probabilidad comprar casa deberar zar funcin escaln funcin activacin salida red combinacin lineal entrada est restringido rango importante notar entrada podrar negativas ga sentir precio tamao negativo frontera decisin extender espacio incluir valor negativo control entrada escapo perceptrn comprobar estn rango valor permitido obstante s controlar rango tipo valor salida limitar rango valor salida percetrn utilizar función funcin escalonado conforme aparecer aplicación perceptrn necesario función distinto rango valor podrar emplear funcin tabla recoger función activacin ms popular propiedad olvidar efecto funcin activacin modificacin valor salida hiperplano perceptrn figura mostrar efecto función activacin tabla valor salida perceptrn ejemplo entrada sesgo deep learning fundamento teora aplicacin tabla principal función activacin nombre grfica ecuacin ranguir identidad x escalonado umbral x x logstico sigmoidir x ex tangente hiperblico tanh ex ex ex ex unidad lineal rectificado relu sigla ingls x x x isaac prez borrero manuel emilio gegndez aria figura perceptrn entrada sesgo funcin activacin identidad deep learning fundamento teora aplicacin figura perceptrn entrada sesgo funcin activacin escalonado isaac prez borrero manuel emilio gegndez aria figura perceptrn entrada sesgo funcin activacin sigmoidir deep learning fundamento teora aplicacin figura perceptrn entrada sesgo funcin activacin tanh isaac prez borrero manuel emilio gegndez aria figura perceptrn entrada sesgo funcin activacin relu deep learning fundamento teora aplicacin principal aplicación clasificacin regresin multitud problema utilizar perceptrn solver él tcnica machine learning caracterizar utilizar conjunto dato resolver problema problema ms habitual machine leanring modelado predictivo delado predictivo pretender crear modelo utilizar dato disponible buscar patrón tratar predecir resultado dato visto formulacin hiptesis caso perceptrn hiptesis corresponder ecuacin plano definir salida valor entrada permitir predecir salida dato visto ejemplo trn salida dato entrada desear incluir estudiar casa milln euros lln metro cuadrado perceptrn limitado entrada trabajar dato hora definir peso predecir salida valor modelado predictivo ver problema matemtico ximacin función desear obtener transformacin aplicar dato entrada salida esperado dar entrada x rn valor salida esperado r buscar obtener cin f rn r aplicar él x obtener problema abordar perceptrn red neuronal describir trmino aproximacin función objetivo definir peso forma ecuacin hiperplano f ms parecida funcin objetivo ejemplo buscar aproximar isaac prez borrero manuel emilio gegndez aria funcin precio superficie casa devolver valor binario indicar comprar casa f precio superficiecasar comprar modelado predictivo distinguir categora principal cin regresin clasificacin categora entrar ejemplo problema cacin consistir aproximar funcin transfor yo dato entrada salida valor discreto cero ejemplo do nmero valor salida distinguir clasificacin binario valor clasificacin multiclar ms valor salida continuacin mostrar ejemplo clasificacin binario clasificacin multiclase detectar tumor maligno dar edad paciente tamao tumor construir perceptrn realizar clasificacin binario decidir tumor maligno benigno detectar nmero dibujado imagen ma clasificacin multiclase imagen representar vector nmero pxel imagen entrada perceptrn clasificacin multiclar abordar perspectiva rent utilizar perceptrón realizar clasificacin binario dgito cero cacin podrar tomar clase perceptrn generar valor ms alto estudiar seccin podrar utilizar red neuronal neurona forma salida neurona represente probabilidad pertenencia entrada clase opcin conocer clasificacin clasificacin multiclase soler abordar red ronal figura mostrar ejemplo clasificacin multiclar clase enfoque perceptrón deep learning fundamento teora aplicacin clase discernir dato clase observar hiperplano separar dato clase resto figurar clasificacin multiclar enfoque regresin problema regresin consistar aproximar funcin transformar dato entrada valor continuo salida tipo problema utilizar función activacin salida discreto funcin umbral continuacin ejemplo problema regresin estimar precio vivienda base superficie problema recido ejemplo problema regresin consistir estimacin mil euros precio vivienda utilizar funcin activacin identidad salida continuo altura persona presente imagen problema abordar utilizar funcin activacin relu salida perceptrn valor salida est rango permitir descartar valor altura negativo clasificacin regresin recibir valor discreto continuo entrada nico diferenciar tipo dato salida isaac prez borrero manuel emilio gegndez aria caso clasificacin tipo dato salida discreto trabajar etiqueta clase problema binario positivo vs negativo nrio rubio vs moreno vs pelirrojo caso regresin salida continuo predice cantidad altura precio figura mostrar ejemplo tipo problema observar cmo caso clasificacin hiperplano perceptrn situar él zona intermediar clase provocar activacin perceptrn caso regresin r hiperplano punto sitar tambin zona intermedio dato entrada forma distancia punto hiperplano valor utilizar obtener salida eje figurar hiperplano lneo punteado problema clasificacin izquierda regresin derecha problema clasificacin abordar problema regresin viceversa mostrar continuacin clasificacin regresin clasificar imagen base contener persona lugar utilizar funcin activacin bral emplear funcin sigmoidir interpretar salida probabilidad persona est imagen aplicar umbral probabilidad podr clasificacin entrada perceptrn deep learning fundamento teora aplicacin problema regresin valor probabilidad esperado entrada regresin clasificacin problema regresin tambin predecir valor discreto asignar clase realmente etiqueta clase discretizar valor salida perceptrn discretizacin consistir redondear valor salida obtener tidad entero dividir rango valor salida clase inters ejemplo medir altura metro persona imagen definir clase bajo normal alto obtener valor continuo salida compruebir qu ranguir est etiqueta clase correspondiente salida limitación perceptrn crear expectativa podrar modelo prueba comunicado prensa new yorktimes public perceptrn ao the navy revealed the embryo of an electronic computer today that it expects will be able to walk talk see write reproducir itself and be ciou of its existence later perceptrons will be able to recognize people and call out their nam and instantly translate speech in onir language to speech and writing in another language new navy devizar learns by doing julio new york times ao marvin minsky seymour papert publicar libro perceptrons obra minsky papert riguroso estudio perceptrn limitación principal conclusin perceptrn resolver blema linealmente separabl problema linealmente separable trazar perplano dividir espacio parte forma punto isaac prez borrero manuel emilio gegndez aria clase caer parte clase definicin definicin conjunto punto espacio eucldeo dimensin n linealmente separabl wn r x n xi wi x n xi wi xi componente notacin vectorial w rn r x wt x x wt x ejemplo problema separabl linealmente funcin or funcin and definir hiperplano separar valor activar salida figura figurar ejemplo separacin lineal hiperplano funcin or and minsky papert demostrar disponer conjunto dato finito linealmente separable hallar peso hiperplano separar tamente dato algoritmo entrenamiento estudiar seccin deep learning fundamento teora aplicacin obstante función separabl linealmente caso función booleana disponer n variable construir función vertablar mostrar latabla aumento nmero variables disminuir drsticamente ratio función separabl linealmente reducir nmero caso utilizar perceptrn tabla función booleana variable tabla función booleana separabl linealmente ratio funcin nmero variable nmero variable función booleana función separabl ratio adems mayora problema real separabl linealmente guna situación existencia outlier dato figurar inherente dato figurar ltimo caso funcin xor figura estudiar minsky papert libro demostrar perceptrn incapaz aproximar cin isaac prez borrero manuel emilio gegndez aria figura ejemplo outlier dato figurar ejemplo dato separabl linealmente as podrar resolver problema xor emplear ms perceptrn red perceptrón conocer red neuronal solucin utilizar perceptrón trazar perplano punto situado origen activar punto perceptrn ms funcin or figura perceptrn har revs salida activar entrar punto perceptrn deep learning fundamento teora aplicacin figura funcin xor cin not and conseguirar comportamiento figurar ltimo tercer perceptrn utilizar funcin and figura drar salida activo entrada estn activo comportamiento funcin and aprovechar lugar recibir entrada recibir salida perceptrón anterior pequeo truco lograr aproximar funcin xor figurar poder representar funcin xor gracias crear composicin ción perceptrón realizar funcin or entrada or funcin not and entrada not and funcin and salida perceptrón anterior or and not and qu transformacin sufrir dato representar salida perceptrón figura presente tercer preceptrn recibir entrada salida valor entrada original observar cmo ltimo perceptrón dejar activo anterior estn activo ltimo tabla calcular salida trón permitir comprobar red perceptrón aproximar correctamente funcin xor isaac prez borrero manuel emilio gegndez aria figura salida perceptrn aproximar funcin or figurar salida perceptrn aproximar funcin not and polmico torno minsky papert conocar problema xor podar solucionar red perceptrón as mayora investigador extrapolar limitación perceptrn red conocer algoritmo permitir ajustar peso red unido aparicin deep learning fundamento teora aplicacin figura salida perceptrn aproximar funcin and figurar perceptrón aproximar funcin xor algoritmos ms prometedor basado simbolismo propiciar abandono red neuronal afect medida financiacin proyecto lacionado conexionismo aos conocer invierno inteligencia artificial isaac prez borrero manuel emilio gegndez aria figura salida perceptrn aproximar funcin or figurar salida perceptrn aproximar funcin not and deep learning fundamento teora aplicacin figura salida construido perceptrón aproximar funcin xor peso indicado ltimo perceptrn implementar funcin and salida isaac prez borrero manuel emilio gegndez aria tabla entrada salida perceptrón red neuronal aproximar funcin xor funcin or funcin not and funcin xor or and not and or not and deep learning fundamento teora aplicacin red neuronal ver seccin perceptrn conocer mo neuronar artificial combinacin perceptrón formar trn multicapa tambin conocido red neuronal artificial simplemente red neuronal seccin estudiar profundidad red nes limitación arquitectura describir red neuronal soler referencia arquitectura organizacin conexión neurona red componente red neuronal capa neurona cibir entrada agrupada capa figurar red neuronal comn denotar operación formato vectorial facilitar lectura conocer ecuacin perceptrn neurona formato vectorial wt general capa ms neurona mantener notacin vectorial mostrar figura n entrada agrupan nico vector x componente bia normalmente salida m neurona capa agrupada tambin nico vector conocido vector salida capa tanto componente neurona capa ltimo peso neurona wi wi wi agrupar matriz pesos w vector columna matriz n entrada n entrada sesgo m neurona capa matriz peso tendr n fila m columna w notacin vector salida capa calcular mostrar ecuacin w t x ecuacin incluir funcin activacin normalmente neurona capa contar estructura conexión función activacin funcin activacin aplicar isaac prez borrero manuel emilio gegndez aria valor salida neurona separado utilizar notacin ecuacin indicar funcin activacin f aplicar componente vector w t t x figurar capa red neuronal observar figura trmino bia entrada comn neurona peso entrada deep learning fundamento teora aplicacin capa neurona conectado entrada conocer capa completamente conectado fully connected ingls nico capa red neuronal red nmero capa ms capa soler nombrar forma rente segn posicin ocupar red figura capa entrada habitual dato entrada vector x representar capa ms red capa caso elemento capa componente vector entrada x lugar neurona libro soler denotar neurona capa letra x lugar salida capa directamente vector entrada capa simplificar notacin operación capa oculto capa salida capa estn despus capa trado neurona perceptrón estudiar seccin contar peso encarguir transformar dato entrada funcin activacin nico diferencia capa oculto capa salida lugar ocupar red capa salida ltimo capa red capa oculto est capa entrada capa salida importante notar capa capa salida trmino bia representado neurona capa soler dibujar dems neurona especial normalmente utilizar valor capa red soler forma utilizar bia implicar capa distinto capa entrada consistir salida capa valor bia capa r entrada xr consistir vector resultado concatenar salida capa bia capa obtenindo él entrada capa r xr capa entrada soler hora indicar nmero capa red capa red ms pequea crear consistira nico capa capa realmente entrada salida capa salida capa intermedia hablar capa oculto observar figura capa nmero isaac prez borrero manuel emilio gegndez aria neurona salida trabajar red neuronal soler interesado salida red salida ltimo capa salida capa intermedio salida red calcular capa capa composicin función capa perceptrón oculto salida depender nmero capas tomar valor capa entrada aplicar transformacin ecuacin salida capa entrada volver aplicar ecuacin capa utilizar peso llegar ltimo capa obtener salida red expresar proceso composicin función segn mostrar ecuacin yl f lt xl f l w lt f w t f l w lt f f w t yl salida red obtenido calcular salida ltimo capa f i funcin activacin capa w i matriz pesos capa xi entrada capa vector xi resultado concatenar salida bia capa i ejemplo detallar clculo salida red neuronal utilizar aproximar funcin xor ejemplo funcin xor red neuronal capa neurona presentado figura peso neurona arquitectura detallo red mostrar figura segn peso neurona salida red calcular mostrar continuacin deep learning fundamento teora aplicacin f t f t f funcin umbral dato entrada salida esperado siguiente aplicar ecuacin entrada observar red lograr aproximar funcin xor correctamente f t f t f t f isaac prez borrero manuel emilio gegndez aria f t f t f t f t f f t deep learning fundamento teora aplicacin f t f t f t f f t f t f t isaac prez borrero manuel emilio gegndez aria f t f f t ventaja calcular salida capa red notacin vectorial permitir generalizar clculo salida red nmero entrada vector entrada x pasar matriz columna matriz vector entrada disponer n vector entrada xn xi xi xi matriz entrada red x x xn m xn m columna matriz vector entrada concatenar bia x salida red matriz columna yi vector salida red vector entrada xi red k neurona capa salida n vector entrada salida red deep learning fundamento teora aplicacin yn k yn k continuacin mostrar clculo salida red ejemplo forma simultneo entrada dar matriz entrada x clculo matriz salida salida red f t f t f t f isaac prez borrero manuel emilio gegndez aria f t f t t neurona capa salida nico entrada salida escalar introducir ms entrada caso salida pasar vector componente entrada capa salida tener ms neurona salida nico entrada vector lugar escalar introducir ms entrada salida matriz tanto columna entrada fila neurona capa salida deep learning fundamento teora aplicacin figura elemento red neuronal isaac prez borrero manuel emilio gegndez aria figura red neuronal aproximar funcin xor deep learning fundamento teora aplicacin aplicación red neuronal contar multitud aplicación incluir perceptrn puesto nico neurona capa salida red neuronal comportar nico perceptrn ventaja ofrecer red neuronal ms rona salida permitir tratar problema nico perceptrn incapaz abordar clasificacin multiclase ejemplo problema consistir reconocer dgito imagen podrar utilizar neurona capa salida neurona encargar detectar dgito ne asociado imagen entrada garantizar neurona ir activar activar tipo problema utilizar funcin activacin identidad capa salida aplicar funcin softmax vector salida funcin consistir funcin sigmoidir convertir vector salida distribucin probabilidad aplicacin s rn rn v v s evi n evj gracias aplicacin salida neurona interpretar probabilidad obstante an clasificacin red entrada necesario obtener vector clasificacin transformar vector probabilidad codificacin indicar clase asignado red entrada normalmente clase asociado neurona probabilidad generalmente utilizar codificacin consistir asignar valor componente vector probabilidad ms tomar cualquiera resto elemento valor figura mostrar ejemplo transformacin softmax codificacin observar vector salida aplicar él transformacin softmax isaac prez borrero manuel emilio gegndez aria figura transformacin softmax codificacin sentar distribucin probabilidad vector probabilidad codificar obtener clasificacin entrada clase diente posicin vector valor caso figura entrada quedar clasificado clase representar neurona salida abordar problema ms complejo clasificacin multiclar red neuronal representar imagen vector pxel procesar red neuronal ms all generar imagen salida utilizar tanto neurona capa salida pxel querer imagen ejemplo entrada imagen escala grís n pxel utilizar neurona capa salida representar imagen canal canal rojo verde azul tamao imagen entrada utilizar red transformar imagen entrada imagen color colorear imagen entrada observar ejemplo limitación red ronal entrada salida vector procesar dato tener dimensión espacial imgén sario convertir él representacin vectorial proceso conocer aplastamiento flattening ingls figura operacin consistir descomponer dimensión espacial dato entrada formar nico vector ejemplo imagen escala grís tamao pxel vector resultado operacin flattening podra vector componente elemento pxel fila ltir pxel ltimo fila deep learning fundamento teora aplicacin figura ejemplo operacin flattening isaac prez borrero manuel emilio gegndez aria limitación saber red capa multitud garanta red ms capa conseguir mejor resultado red expresividad red trmino soler referencia capacidad red aproximar función ms compleja función aproximar red expresividad función ms elemental función lineal función aproximar nico perceptrn red neuronal capa neurona capa función ms compleja funcin xor rir capa aproximar él puesto separabl linealmente obstante aadir ms capa red permitir aproximar correctamente función función tivacin lineal salida combinacin lineal entrada funcin activacin lineal implicar transformacin dato red equivalente podrir red contar nico capa tanto neurona capa salida red original ro qu funcin activacin lineal aproximar función lineal presente funcin activacin lineal ma mx b equivalente introducir transformacin matriz peso capa utilizar funcin activacin funcin identidad salida capa yr ms combinacin lineal entrada peso w rt xr propiedad asociativo cin salida red l capas x yl lt xl f w lt f w t f w lt f f w t w lt w t w lt w t w deep learning fundamento teora aplicacin red equivalente matriz pesos w dimensión matriz peso capa salida multiplicar matriz trado capa obtener salida aplicar operación red función activacin lineal sividad gracias funcin activacin umbral lineal ejemplo aproximar funcin xor capacidad red neuronal aproximar función quedar teorema aproximacin universal demostracin formal teorema est alcance libro teorema afirmar dar funcin continuo aproximar funcin cisin deseado utilizar red neuronal importar complicado funcin desear aproximar utilizar función cin continuo acotada constante segn teorema podrar aproximar funcin utilizar nicamente red capa forma aproximar funcin red nico capa oculto dividindola intervalo caso neurona funcin activacin sigmoide efecto modificacin peso bia nico entrada mostrar figura observar medida aumentar peso pendiente ms pronunciado ms funcin umbral bia provocar pendiente mover izquierda derecha segn positivo negativo respectivamente utilizar neurona capa oculto conectar neurona nico cambiar signo sumar valor construir especie pulso figura desear ampliar él aumentar valor peso ltimo neurona figurar forma trabajar necesitar neurona construir pulso depender precisin desear alcanzar aadir par neurona aproximar funcin intervalo ms modificar pulso aproximir forma funcin figurar caso aproximar función ms dimensin dar emplear tanto neurona salida dimensión utilizar isaac prez borrero manuel emilio gegndez aria nmero neurona capa oculto construir forma ms complejo pulso dimensin cilindro caso dimensión conclusión sacar teorema interpretar cerebro humano especie funcin recibir entrada sensor salida sistema motriz principio red neuronal capaz tipo tarea realizar humano ejemplo podrar construir red dar imagen radiografa capaz detectar enfermedad red hablar humano capaz responder idioma tarea función recibir entrada salida motivación empujar investigador seguir trabajar campo posibilidad construir cerebro artificial evidente diferencia humano red neuronal capaz lugar teorema arquitectura peso red teorema existencia garantizar cmo adems red necesitar nmero neurona infinito imposible tecnologa actual garanta encontrar funcin exactamente funcin objetivo teorema garantizar precisin aproximacin asegurar ambos ir idntica red neuronal tratado caja negro modelo interpretabl comprender qu  entrada red generar salida implantacin entorno crtico toma decisión vida persona est juego conduccin autnoma implicar riesgo seccin vern problema enfrentar modelo forma definir peso red destacar incapacidad retener tarea aprendido utilizar tarea robustez modelo ltimo aproximar funcin objetivo región s forma funcin figura entrar dato pertenecer zona aproximacin salida modelo coincidir esperada fcilmente caso conduccin autnoma red deep learning fundamento teora aplicacin entrenado imgén pase norte utilizar red pase ms tropica él geografa delimitacin carretera seal red podro comportamiento inesperado leve modificacin valor dato entrada provocar salida errneo red neuronal idea precaución hora utilizar él ltimo neurona artificial imitar forma trabajar nas biolgica cerebro humano torno neurona conexión nmero alejado capacidad actual ordenador comercial tipo neurona existir ponente temporal presentar red neuronal artificial regir cionamiento adems utilizar mecanismo aprendizaje an descifrar ms rpido eficiente red neuronal artificial cuyo mecanismo aprendizaje asemejar él neurona biolgica aadir él neurona biolgica procesar informacin velocidad inferior ordenador segundo activacin desactivacin capaz resolver tarea extremadamente ja imposible equivalente artificial gracias elevado nivel paralelismo valer analoga jeff hawkins libro on intelligence humano tarea considerable tiempo gundo ejemplo podra mostrarl fotografa pedirl indicarar gato imagen labor pulsar botn gato ver oso jabal rbano tarea difcil imposible ordenador actual humano forma ble neurona biolgica lenta as informacin entrar cerebro capaz atravesar cadena cien neurona cerebro computar solución problema cien paso prescindir cunta neurona poder participar instante luz entrar ojo momento pulsamos botn podra participar cadena ms larga cien neurona cien instrucción ordenador bastar mover carcter pantalla decir interesante isaac prez borrero manuel emilio gegndez aria figura efecto cambio valor peso bia neurona funcin activacin sigmoidir deep learning fundamento teora aplicacin figura construccin funcin pulso red neuronal isaac prez borrero manuel emilio gegndez aria figura construccin funcin pulso ampliado red neuronal deep learning fundamento teora aplicacin figura aproximacin funcin red neuronal isaac prez borrero manuel emilio gegndez aria figura ejemplo funcin aproximado correctamente zona captulo desarrollo red neuronal estudiar componente aplicación limitación red neuronal tocar centrar él desarrollo crear red neuronal necesario definir arquitectura etapa diseo ms arte ciencia depender problema est tratar habr arquitectura funcionar soler proceso prueba error estrategia ms habitual consistir utilizar arquitectura popular resultado problema genrico presuponer arquitectura funcionar problema harn tambin problema est abordar priori ms sencillo original disar arquitectura elemento clave creacin red neuronal problema abordar red neuronal reducir problema aproximacin función funcin desear aproximar describir base serie punto ejemplo problema funcin xor disponer entrada salida esperar obtener entrada mayora conocer funcin aproximar valor funcin punto dominio valor funcin ejemplo piense lector sistema deteccin persona imgén imposible recopilar entrada salida sistema normalmente trabajar muestra dato arquitectura conjunto dato formar punto partida deep learning fundamento teora aplicacin desarrollar red neuronal resolver problema seccin estudiar detalle relacionado creacin red etapa entrenamiento vs rencia trabajar red neuronal modelo machine learning soler distincin obtencin modelo modelo etapa conocido etapa entrenamiento etapa inferencia figurar etapa entrenamiento etapa red an peso definido utilizar dato disponible forma red ajustar funcin desear aproximar ajuste peso forma manual aleatorio automticar algn  algoritmo entrenamiento caso red neuronal soler denominar etapa etapa aprendizaje resultado etapa red peso definido etapa inferencia actualizar peso red red etapa inferencia etapa til validar entrenamiento red decidir seguir base resultado obtener ventaja red neuronal saber permitir obtener salida valor entrada limitar dato utilizar etapa entrenamiento dar él red dato ver predicción resultado etapa nicamente salida red dato entrada suministrar etapa entrenamiento peso definido utilizar red modelo escoger problema cuestin entrenar dato disponible considerar finalizado entrenamiento pasar etapa inferencia modelo isaac prez borrero manuel emilio gegndez aria figurar etapa entrenamiento vs inferencia cado simplemente obtener salida dato conjunto dato disponer volver etapa entrenamiento mejorar modelo proceso cclico mejora conforme obtener ms dato namiento paradigma aprendizaje etapa entrenamiento ajustar peso red chas forma machine learning soler utilizar algn algoritmo deep learning fundamento teora aplicacin aprendizaje encargar ajustar peso modelo utilizar conjunto dato base tipo dato utilizar ajustar modelo distinguir paradigma aprendizaje aprendizaje supervisado supervisado refuerzo figurar aprendizaje supervisado dato asociado salida ejemplo funcin xor est problema dizaje supervisado disponer informacin permitir supervisar namiento modelo segn diferencia salida modelo valor esperado ajustar peso red tipo problema incluir clasificacin regresin estudi seccin principal problema aprendizaje supervisado falta dato entrenar correctamente modelo mayora problema aprendizaje supervisado salida esperado dato entrada definir forma manual ralentizar encarecer elaboracin conjunto dato aprendizaje supervisado dato salida asociado problema aprendizaje supervisado informacin cmo red caso soler utilizar red neuronal analizar dato buscar patrón permitir agrupar él conocer clustering requerir dato tener salida asociado problema aprendizaje supervisado soler ms dato disponible aplicación amplio aprendizaje supervisado aprendizaje refuerzo paradigma aprendizaje red agente interactar entorno lugar disponer conjunto dato caso anterior agente enva acción entorno recibir agente despus ejecutar accin entorno recompensa simbolizar aplicar accin enviado agente contrar videojuego tratar agente ajustar red base feedback recibir entorno mejorar tipo zaje ms identificar humano humano agente mundo entorno ejecutar acción recibir retroalimentacin base acción realizado parecer ms idneo etiquetar dato forma manual isaac prez borrero manuel emilio gegndez aria aprendizaje supervisado principal desventaja falta retroalimentacin encontrar agente evidente valor retroalimentacin ejemplo mitad laberinto red girar izquierda instante recompensar castigar accin llegar laberinto relevante resultado posibilidad modelar problema paradigma plo problema clasificacin abordar él paradigma aprendizaje supervisado ms evidente aprendizaje refuerzo podrar interpretar clasificacin correcto red accin pensar clasificacin incorrecto accin penalizar entorno dato problema tambin abordar problema perspectiva aprendizaje supervisado clasificacin dato est lacionado dispersin presentar encontrar agrupación utilizar él clasificacin clase problema paradigma ms genrico nico quedar alcance libro estudio deep learning fundamento teora aplicacin figura aprendizaje supervisado supervisado refuerzo isaac prez borrero manuel emilio gegndez aria obtencin preparacin dato ver sección anterior utilizar red neuronal pre manejar dato entrada considerar capa red verificar modelo funcionar correctamente ajustar peso necesitar conjunto dato trabajar red dato ir recopilar representativo problema coger correctamente casustica conocer polmica reciente google aplicacin google foto clasificar imgén persona negro mo gorila sistema desear imgén persona blanco mayoritariamente sesgo adquirir red ronal fallar clasificar persona color piel curiosidad decisin adoptar eliminar clase gorilo aplicacin ejemplo desafo enfrentar él trabajar red neuronal interpretabilidad obligar prestar cha atencin hora construir conjunto dato introducir sesgo provocar comportamiento tico discriminatorio sos ejemplo coche autnomo entrenado conjunto dato construido imgén carretera pase clido utilizar pase norte sabr comportamiento tendr sistema conducir paisaje nevado fase obtencin dato clave eliminar sesgo esencial resultado captulo llegada denominado big data propiciar factor xito red neuronal obtencin conjunto dato soler tarea boriós clasificar forma manual mayora dato despu entrenar red empresa dedicar exclusiva clasificar tiquetar dato empresa querer utilizar entrenar red compaas inventar idea ms creativa teslar ejemplo conexin internet vehculo sistema conduccin autnoma est procesar dato entrada conductor desactive do decisin sistema coincidir tomar conductor enir deep learning fundamento teora aplicacin dato entrada sistema servidor tesla etiquetar entrenar red neuronal importante notar dato ms escaso puesto caso contrario red haber actuar entrada similar utilizado entrenamiento diferencia sistema radicar conjunto dato utilizar ms nes anmala recoger ms probabilidad sistema acte correctamente dar situación atpica caso poder obtener ms dato difcil acceso carecer recurso utilizar tcnica aumento dato crear dato sinttico entrenar red ejemplo caso imgén aplicar multitud alteración cambio brillo contraste crear imagen mantener salida esperado imagen original opcin consistir acudir conjunto dato pblico estn disponible internet utilizar él aumentar conjunto to original tambin descargar dato estn accesib él sitio web est seguro est respetar normativa relacin derecho autor dueo dato autorizar conjunto dato entrenar red neuronal resolver problema normalmente conjunto dato original datar set dividido conjunto entrenamiento train ingls validacin validation ingls prueba test ingls conjunto entrenamiento dato conjunto sern utilizado red ajustar peso neurona etapa entrenamiento conjunto validacin red comportamiento dato ver utilizar conjunto entrenar red dato evaluar red entrenamiento tomar decisión base resultado conjunto prueba conjunto dato objetivo conjunto validacin evaluar rendimiento modelo diferencia conjunto validacin conjunto prueba utilizar namiento red finalizar estimar comportamiento tendr red etapa inferencia conjunto dato entrenamiento red neuronal isaac prez borrero manuel emilio gegndez aria especialmente soler habitual dejar torno dato validar prueba dejar resto dato entrenar red obstante tamao conjunto validacin prueba indicar granularidad detectar mejora red ejemplo disponer dato conjunto detectar mejora imposible red neuronal porcentaje contrario modelo machine learning soler utilizar tcnica divisin conjunto dato similar conjunto dato soler entrenamiento red tcnica suficientemente costoso suponer algn tipo cio manipular conjunto dato enorme perder control problema dato repetido errneo comprobar integridad dato trabajar as aparezcar dato repetido mitad conjunto validacin idntico modelo hacer dato sensacin funcionar correctamente mitad tiempo evitar tipo problema importante control dato revisar él extraer estadstica conjunto globalmente seguro presentar problema forma correr riesgo consistir divisin dato manipular dato ruta sistema fichero garantizar duplicar pierdan corrompar ahorrar movimiento cantidad dato almacenamiento dato ofrecer garanta seguridad posible dao fsico sistema almacenamiento comn contar copia nube prevenir posible prdida conjunto dato soler bastante pretender durir tiempo conveniente almacenarlo algn formato conocido ofrecer algn tipo compresin ahorrar espacio formato csv soler ms utilizado dato tabulado jpg imgén tcnica compresin olvidar red trabajar valor real entrada tante dato qu valor real deep learning fundamento teora aplicacin transformar dato representacin usar valor real dato imgén necesitar manipulacin valor xel representar valor real s requerir tcnica ms elaborada representacin ejemplo caso contar sistema capaz distinguir perro gato imgén guardar clase imagen utilizar codificacin componente vector clase perro clase gato segn clase pertenecer clase entrada asignar valor posicin pondiente problema codificacin utilizar multitud clase necesitar vector ejemplo detectar tweet determinado producto codificar tweet vector caracter posible adems tweet nmero caracter variable impracticable utilizar codificacin caso suerte elaborar representación ms compleja mejor sentar texto conocer word embedding modelo buscar representar palabra vector espacio multidimensional bra similar ocupar posición cercano forma representacin facilitar resolucin problema red caso dato audio soler trabajar imagen espectrograma convertir problema procesamiento audio problema procesamiento imgén mtrica evaluacin seguro red funcionar correctamente forma evaluar rendimiento objetivo mtrica evaluacin adems ayudar monitorizar entrenamiento estudiar seccin mtrica evaluacin proporcionar medida objetivo permitir comparar red peso arquitectura mtrica evaluacin estimacin base criterio cmo funcionar red evidentemente estimacin necesario clase dato entrada necesitar salida esperado entrada red as poder él comparar isaac prez borrero manuel emilio gegndez aria salida obtenido utilizar mtrica evaluacin fase inferencia disponer salida esperado dato entrada contrario conjunto dato entrenamiento validacin prueba s disponer salida esperado principio impedir medir rendimiento conjunto namiento validacin prueba obstante normal evaluar red conjunto prueba validacin ltimo evalar vara gn estipular entrenamiento midindo él pulso red detectar mtrica mejorar empezar empeorar parar entrenamiento intentar estrategia buscar algn error cdigo evaluacin conjunto entrenamiento soler utilizar ro comparar él validacin esperar ambos similar ocurrir contrario soler sntoma algn problema problema derivado entrenamiento estudiar detalle seccin caso entrenamiento necesite elevado tiempo procesamiento obviar evaluacin conjunto entrenamiento guiar él mtrica conjunto validacin utilizar mtrica evaluacin varar funcin tipo problema est trabajar mtrica ms popular relacionado problema clasificacin saber problema clasificacin consistir asignar entrada clase problema caso red neuronal aplicar operacin softmax seguido codificacin salida red depender posicin encontrar valor vector salida clase asignar red entrada problema enfoque radicar acabar clasificar entrada perteneciente clase valor probabilidad alternativa utilizado hora evaluar red corregir problema consistir emplear valor umbral ejemplo valor probabilidad salida superar umbral asignar clase asociado valor inferior superior corresponder clase esperado entrada comparar correctamente salida red salida esperado gue clase positivo negativo caso problema ms deep learning fundamento teora aplicacin clase soler extraer mtrica considerar positiva resto negativa posteriormente promediar mtrica estimar comportamiento general red clase relativo salida esperado consistir vector codificacin valor indicar clase corresponder entrada clase posicin clase positivo entrada resto clase considerado clase negativo entrada actual salida red valor probabilidad superar umbral clase positivo clase asociado posicin valor resto clase considerado clase negativo entrada contrario superar umbral probabilidad clase considerar negativa entrada figura mostrar ejemplo clasificacin red valor umbral salida esperado figurar efecto clasificacin valor umbral base comparacin clase positivo negativa obtenido red esperada obtener mtrica bsica problema cin verdadero positivo true positiv ingls tp sigla clase isaac prez borrero manuel emilio gegndez aria red positivo coincidir clase positivo esperado positivo verdadero negativo true negativ ingls tn sigla s s asignado s negativa s red coincidir s clase s negativo s esperado s negativo falso positivo fals positiv ingls fp sigla clase positivo predicho red coincidir esperada falso positivo falso negativo fals negativ ingls fn sigla s clase s negativo s red coincidir s clase s negativa esperado s est falso negativo mtrica soler agrupar conocer matriz fusin matriz utilizar contador fila corresponder clase esperado entrada columna dar clase predicha modelo dato entrada base clase esperado clase predicho red incrementar celda correspondiente matriz confusin construir clase modelo mostrar figura metodologa seguir utilizar codificacin salida red contrario aplicar umbral salida matriz confusin clase resto clase considerado negativo ltima opcin ms habitual mostrado figura matriz confusin obtenido clase derivar mayora mtrica ms utilizado exactitud accuracy ingls mtrica ratio dato clasificado correctamente clase positivo negativa accuracy tp tn tp tn fp fn exhaustividad recall sensitivity ingls mtrica poner foco modelo dato clase positivo ratio dato clase positivo clasificado deep learning fundamento teora aplicacin figura matriz confusin n clase recall tp tp fn precisin precision ingls mtrica utilizar ratio dato realmente clase positivo ejemplo red clasificar positivo precision tp tp fp isaac prez borrero manuel emilio gegndez aria figurar matriz confusin considerar clase positivo negativo especificidad specificity ingls similar recall mtrica centrar dato clase negativo specificity tn tn fn figura mostrar problema ejemplo calcular mtrica problema principal mtrica definir valor umbral asignar él dato entrada clase positivo negativo depender valor umbral habr mtrica salir ms beneficiado eleccin umbral tarea fcil umbral permite detectar positivo red contar sensibilidad alto aparecer falso positivo contrario utilizar umbral alto falso positivo acabar desaparecer costa poder perder positivo verdadero caso sistema tendr especificidad alto mtrica incompatible valor umbral ayuda elegir comportamiento modelo ms adecuado problema comparar modelo utilizar mtrica pendar valor umbral elegido rea curva roc mtrico deep learning fundamento teora aplicacin figurar mtrica problema clasificacin persona segn estn embarazado consistir enfrentar valor sensibilidad especificidad cin medir rea curva auc sigla ingls mtrica isaac prez borrero manuel emilio gegndez aria definir rendimiento general modelo figurar clasificador perfecto rea unidad dor cuyo curva sitar cerca diagonal til puesto confundir ms acertar diagonal soler interpretar rendimiento modelo realizar predicción aleatorio figurar curir roc est problema clasificacin soler utilizar él ca ms especficas problema cuestin ejemplo est trabajar problema deteccin objeto salida red soler to recuadro delimitar objeto red detectar imagen problema soler emplear mtrica conocido intersection over union iou sigla consistir calcular ratio interseccin recuadro predicho red recuadro esperado unin figurar encontrar multitud ejemplo mtrica orientado problema depender problema est abordar necesario buscar literatura mtrica ms utilizado comparar solucin propuesto igualdad condición deep learning fundamento teora aplicacin figura mtrico iou estimacin error objetivo fase entrenamiento red neuronal establecer peso utilizar conjunto dato entrenamiento modificar peso red utilizar medida indicar desviacin valor red valor esperado dato conjunto entrenamiento valor conocer error red utilizar estimacin cmo malo determinado valor error conocer prdida loss ingls funcin dar salida red salida esperado devolver valor error prdida llamar funcin error funcin prdida funcin coste hora elegir funcin error problema est abordar función ms indicado segn problema ms utilizado siguiente isaac prez borrero manuel emilio gegndez aria error cuadrtico ecm mean square error ingls normalmente utilizar problema regresin consistir media diferencia cuadrado salida esperado salida obtenido conjunto n dato dimensin m ecm calcular ecm n n m salidaredi j salidaesperadai ce funcin utilizar problema clasificacin salida esperado modelo distribución probabilidad calcular logaritmo natural probabilidad dado red clase positivo clase valor vector codificacin salida esperado disponer conjunto n dato m neurona salida ce calcular ce n n m j salidaesperadai j figura mostrar grfica funcin ecm ce vamente observar figura funcin ecm comenzar valor alto medida diferencia valor esperado obtenido alejar unidad tambin observar valor salida independientemente diferencia valor esperado obtenido positivo negativo caso valor salida coincido valor esperado apreciar figura funcin ce podra salida valor negativo entrada funcin est rango tratar él probabilidad obtener valor positivo salida concretamente rango salida red coincidir salida esperado valor funcin valor salida red posicin esperar valer valor funcin ce indicar red desviar valor esperado deep learning fundamento teora aplicacin figura grfica funcin ecm justificacin funcin ce problema clasificacin basado distribución probabilidad encontrar teora macin quedar alcance libro diferencia mtrica función error prdida medida recta error prdida red respectivamente permitir ajustar peso modelo resolver problema isaac prez borrero manuel emilio gegndez aria figura grfica funcin ce aprendizaje basado problema ajustar peso red neuronal conocer aprendizaje entrenamiento red problema soler enfocar perspectiva problema optimizacin matemtico caso aprendizaje supervisado disponer conjunto dato deep learning fundamento teora aplicacin entrada x valor salida esperado par valor x provenir distribucin d x tupla referencia ejemplo conjunto dato prctica n muestra ejemplo conformar conjunto dato d x d d prctica inviable obtener posible dato problema ejemplo problema reconocimiento vehcu él imgén imposible conjunto dato imgén podrar considerar vehculo s muestra conjunto adems conjunto dato proceder entrenamiento necesitar funcin prdida l parametrizado peso red w rn base problema optimizacin intentar resolver sistar encontrar valor peso red tal minimizar funcin prdida w arg min w lw comprender qu consistir problema optimizacin niente visualizar superficie funcin prdida tomar dato conjunto entrenamiento calcular valor prdida combinacin valor dibujar superficie funcin prdida figura mostrar superficie perceptrn ejemplo entrada ejemplo figura superficie dimensión puesto perceptrn peso tener ms peso podrar dibujar perficie funcin prdida observar ejemplo valor peso hacer perceptrn equivocar cometer algn error clasificar ejemplo estudiar existan outlier dato directamente linealmente separabl obstante deducir mejor valor peso perceptrn segn observar grfica valor ir reducir ms valor isaac prez borrero manuel emilio gegndez aria figura superficie funcin prdida perceptrn entrada prdido punto mnimo global funcin prdida intentar encontrar resolver problema optimizacin buscar valor peso red neuronal superficie figura soler habitual normal superficie funcin prdida parecer ms mostrado figura superficie convexa multitud mnir local enfoque abordar problema optimizacin podra buscar solucin forma analtico saber buscar mnimo funcin prdida punto peso wi w derivado nulo lw wi funcin prdida convexa solucin analtico resolver lw wi limitacin ms evidente problema regresin simplicidad imagine utilizar nico perceptrn funcin activacin funcin ecm funcin deep learning fundamento teora aplicacin figura superficie tpico funcin prdida red neuronal fuente prdida caso ecuacin permitir calcular mejor valor parmetro modelo regresin lineal perceptrn caso w xt necesitar hallar inversa x existir resolver ecuacin adems coste asociado clculo tcnica alternativa encontrar mnimo inviable calcular dimensión llegar matriz involucrar dato problema posibilidad consistir utilizar tcnica metaheurstica algoritmo lutivo simple bsqueda aleatorio opción quedar descartado isaac prez borrero manuel emilio gegndez aria elevado dimensionalidad espacio bsqueda figura mostrar superficie dimensión bsqueda peso red valor real bsqueda dimensión complejo difcil an ms trabajar problema real llegar red millón parmetro elevar dimensin espacio bsqueda exponencial alternativa mtodo analtico tcnica metaheurstica mtodo iterativo resolver problema optimizacin mtodo iterativo calcular solucin problema pequeo paso reducir demanda computacional comparado mtodo analtico encontrar solucin ptimo calcular solucin garantizado mtodo analtico adems razón puramente tcnica mtodo analtico inters buscar mtodo similar utilizar cerebro intentar emular red neuronal algoritmo entrenamiento red neuronal inspirado teora ir desarrollar forma cerebro aprender donald hebb present teora cmo cerebro capaz der resumido frase neurona disparar junto permanecern conectado teora venir neurona activo tiempo conexin fortalecer frank rosenblatt utilizar teora hebb proponer algoritmo actualizacin peso perceptrn algoritmo observar forma actualizar peso algoritmo consistir sumar vector pesos w vector entrada x caso entrada positivo perceptrn clasificar negativa wt x entrada negativo perceptrn clasificar positiva wt x restar vector pesos vector entrada forma actualizar peso perceptrn conocer regla actualizacin rosenblatt demostr algoritmo convergar tiempo finito dato linealmente separabl peso inicial perpectrn algoritmo namiento modificar peso algn ejemplo perceptrn deep learning fundamento teora aplicacin algoritmo algoritmo entrenamiento perceptrn entrada vector pesos perceptrn w conjunto dato namiento d x d salido vector pesos w actualizado forma separar linealmente dato d while converge do tomar ejemplo x d if wt x thir w w x end if if wt x thir w w x end if ejemplo d clasificar correctamente algoritmo converge end while capaz clasificar correctamente ocurrir ejemplo sitivo suma vector peso provocar ngulo separacin tos reducir caso ejemplo negativo aumentar momento ngulo permitir ejemplo clasificar correctamente goritmo converge figura representar ejemplo cmo podra evolucin vector pesos w perceptrn da iteración t algoritmo entrenamiento llegada adaline perceptrn pas nicamente combinacin lineal entrada dejar funcin activacin aparte podar ximar función real algoritmo original rosenblatt incapaz converger dato linealmente separabl autor pasar enfoque ms prctico abordar problema perspectiva puramente matemtico problema optimizacin definir funcin funcin ecm utilizado originalmente modificar algoritmo entrenamiento perceptrn peso ser ajustar forma proporcional error cometar modificar regla actualizacin isaac prez borrero manuel emilio gegndez aria figura evolucin vector peso algoritmo entrenamiento perceptrn peso introducir regla conocido regla delta hacar gradiente funcin prdida gradiente funcin vector agrupan derivado parcial parmetro funcin funcin prdida lw parametrizado n pesos w red neuronal gradiente definir lw lw lw wn caso particular funcin nico parmetro hablar directamente derivada funcin f x lugar gradiente deep learning fundamento teora aplicacin utilidad gradiente derivada radicar ofrecer informacin efecto valor salida funcin cambio peso sentido proporcionar informacin encontrar mnimo figura base valor derivada f x reducir x acercar él mnimo f x aumentar x acercar él mnimo f x aportar informacin cmo modificar x ptimo local mximo mnimo figurar representacin grfica funcin derivada superficie funcin prdida definir base ms parmetro adems caso anterior aadir caso conocido punto silla punto crtico mnimo isaac prez borrero manuel emilio gegndez aria nes mximo figura representar ejemplo función punto crtico aparecer figurar posible punto crtico funcin forma utilizar gradiente regla delto ms caso particular algoritmo descenso gradiente detallar algoritmo popular derivado seccin cipal problema entrenamiento basado gradiente analizar cin ventaja algoritmo optimizacin basado gradiente regla delta algoritmos iterativo abordar problema forma serar intratable equipo cmputo actual ventaja dato ser linealmente separabl podrar asignar peso perceptrn valor minimizar funcin prdida valer cero algoritmo capaz converger mnimo funcin garantizado algoritmo entrenamiento original independientemente algoritmo optimizacin escogido soler distinguir tipo optimizacin base nmero ejemplo conjunto dato d x d utilizar clculo gradiente optimizacin lote batch ingls caso ejemplo disponible paso principal problema estrategia caso inviable cantidad ejemplo disponible ralentizar entrenamiento imposible limitación memoria almacenar él instante problema plantear deep learning fundamento teora aplicacin arg mn w lw d optimizacin estocstico stochastic ingls caso opuesto nico ejemplo actualizar modelo cin forma entrenar soler utilizar dificultad guir rendimiento general modelo utilizar nico ejemplo planteamiento problema arg mn w optimizacin ingls estrategia presentar compromiso modelo rendimiento general dad entrenamiento utilizar n ejemplo optimizar modelo estrategia ms utilizado problema optimizacin arg mn w habitual utilizar optimizacin tamao lote batch size ingls definir forma equipo utilizar alojar memoria ejemplo calcular gradiente optimizacin ms popular efecto producir superficie funcin prdida utilizar ms ejemplo utilizar ejemplo disponible adems inviable imposibilidad almacenar dato memoria superficie funcin prdida iteración algoritmo caer mnimo local podr salir caso optimizacin estocstico iteracin superficie funcin prdida cambiar definir base error cometer red clasificar dato ejemplo dato iteracin actual seguramente superficie vare suficiente mnimo local est detenido dejar ser él escapar menor nmero ejemplo calcular gradiente menor probabilidad mnimo funcin prdida coincido mnimo funcin real intentar evitar optimizacin estocstico isaac prez borrero manuel emilio gegndez aria mnimo ms parecido real definir ejemplo namiento problema calcular gradiente tomar dato algoritmo optimizacin quedar do algn mnimo local optimizacin ofrecer compromiso utilizar mnimo referencia evitar mnir local figura comparar superficie cin prdida definirar estrategia optimizacin problema ejemplo efecto entrenamiento minsky papert estudiar perceptrn qued demostrado necesidad utilizar red neuronal capa oculto funcin activacin lineal tratar problema separabl linealmente problema red neuronal capa oculto conocer salida esperado neurona capa utilizar algoritmo entrenamiento basado gradiente funcin prdida conocer error cometer neurona limitacin provoc invierno inteligencia artificial abandono red neuronal gada algoritmo backpropagation dcada encontr ajustar peso capa interno propici ro inters red neuronal estudio algoritmo abordar seccin acabar seccin cabrar plantear él gradiente ma definir peso red tcnica basada gradiente derivada obtener informacin tura funcin prdida cmo rpido bajo prdida curva plano curvado abajo alternativa mtodo newton lugar utilizar derivada alcanzar punto est mnimo mtodo llegar mnimo forma ms rpida problema mtodo basado derivado elevado coste computacional procesamiento almacenamiento adems basar él clculo raz vado dar él caso existir s existir saber mximo mnimo mtodo acercar ptimo tipo acabar mximo objetivo encontrar mnimo viceversa fecha algoritmo optimizacin iterativo basado gradiente ms utilizado deep learning fundamento teora aplicacin figura efecto definicin superficie error estrategia optimizacin isaac prez borrero manuel emilio gegndez aria inicializacin peso algoritmo entrenamiento necesitar punto partida modificar peso red inicializacin peso marcar medida resultado podrn alcanzar presente segn inicialicir peso red situarn punto determinado superficie funcin prdida objetivo inicializacin site cerca mnimo global funcin figura indicar región superficie error interesar comenzar entrenamiento región prxima mnimo global alejado mnimo global repletar mnir local algoritmo podra detener él priori conocer superficie funcin prdida indicio qu valor mejor inicializar peso cin ms simple inicializacin peso puramente aleatorio dar alto dimensionalidad funcin prdida acabar regin prxima mnimo inicializacin aleatorio alternativa podra inicializar peso cero algn valor principal inconveniente tcnica depender gradiente error actualizar peso valor peso actualizar forma quedar valor resultado actualizacin solucin problema requerir peso neurona podr utilizar inicializacin necesario peso inicializar valor romper simetra adems romper simetra inicializar peso red forma ejemplo utilizar valor pequeo red capa podrar acabar generar valor prxir igual cero capa final red sucesivo multiplicacin entrada valor harar desvaneceír rrir contrario inicializar peso valor alto generar valor salida entrada similar tengan salida dificultar medida deep learning fundamento teora aplicacin figura región superficie error ideal comenzar entrenamiento namiento poder llegar producir él error desbordamiento superar él valor mximo tipo dato almacenar peso evitar problema anterior desarrollar tcnica cializacin aleatorio ms elaborada continuacin detallar inicializacin xavier tcnica surgir solucin problema ce inicializacin puramente aleatorio segn funcin activacin utilizar saturar llegar valor mo mnimo salida inicializar peso forma aleatorio provocar situacin salida estn saturada exigir ms iteración algoritmo entrenamiento ajustar peso isaac prez borrero manuel emilio gegndez aria autor sugerir inicializar peso forma varianza peso neurona unidad forma reducir probabilidad situar él zona provocar saturacin funcin activacin considerar neurona j n pesos varianza neurona v v wj nxn nv i i n inicializacin consistir valor peso neurona distribucin normal wj i n n inicializacin segn autor tcnica alternativa inicializar red utilizar función activacin relu permitir convergencia modelo extremadamente profundo capa inicializacin xavier eficaz consistir asignar valor aleatorio tomado distribucin uniforme wj i n n nmero entrada neurona bia neurona soler inicializar cero caso riesgo simetrir romper cualquiera inicialización aleatorio anterior resto peso neurona alternativa consistir red imagine utilizar red resultado problema complejo genrico algn investigador litar red peso ajustado lugar empezar entrenamiento cero problema inicializacin aleatorio podrar utilizar red entrenar as aprovechar red deep learning fundamento teora aplicacin capaz aprender acelerar entrenamiento tcnica inicializacin conocer transferencia aprendizaje transfer learning ingls transferir aprendido problema principio ms sencillo nal idea comenzar zona ms prxima mnimo inicializacin aleatorio gracias aprender red problema principal algoritmo optimizacin seccin estudiar algoritmo optimizacin basado gradiente ms utilizado entrenamiento red neuronal partir idea bsico algoritmo descenso gradiente aportar mejora convergencia ms rpida alcanzar mnimo diferencia algoritmo entrenamiento perceptrn ritmo criterio convergencia definir nmero iteración modificar peso base regla zacin especfico algoritmo detallar estructura comn algoritmo basado gradiente caso optimizacin algoritmo algoritmo basado gradiente entrada matriz peso red w conjunto dato entrenamiento d x d funcin prdida l salido matriz pesos w finalizar entrenamiento inicializar w for k numiteración do tomar ejemplo d end for importante recordar disponer salida esperado capa oculto isaac prez borrero manuel emilio gegndez aria red presentar algoritmo backpropagation podr utilizar algoritmo red ms capa algoritmo descenso gradiente algoritmo descenso gradiente atribuir matemtico francs augustin louis cauchy present ao algoritmo diente actualizar paso valor peso red sentido contrario signo derivada ver seccin permitir car él mnimo figurar forma proceso iterativo peso acabar sitar mnimo funcin prdida indicar gradiente punto inicial superficie funcin prdida venir inicializacin realizado peso punto actualizar peso w cantidad paso t indicar regla actualizacin algoritmo wt w lwt caso funcin prdida ecm regla actualizacin conocer regla delto introducir autor adaline estudiar problema utilizar directamente gradiente podr canzar mnimo cantidad modificar peso iteracin nocido paso venir marcado valor gradiente estn lejos mnimo valor pequeo gradiente permitir alcanzar él lizar entrenamiento estn cerca valor alto comenzarn pasar mnimo llegar bajar problema entrenamiento basado gradiente analizar detalle cin controlar efecto gradiente actualizacin peso introducir parmetro conocido tasa aprendizaje learning ratar ingls permitir controlar paso algoritmo entrenamiento deep learning fundamento teora aplicacin figura modificacin variable acercar él mnimo funcin cin peso regla actualizacin peso parmetro quedar wt w lwt estudiar seccin eleccin learning ratar esencial lograr convergencia mnimo isaac prez borrero manuel emilio gegndez aria figura mostrar ejemplo modificacin peso realizado algoritmo superficie funcin prdida figurar modificacin peso algoritmo descenso gradiente efectivo algoritmo problema necesitar ción llegar mnimo lugar actualizar peso base gradiente actual gradiente iteración anterior imagine nmero iteración gradiente venir apuntar direccin lugar actualizar iteracin paso determinado paso ms indicar direccin seguir algoritmo tiempo pedir seguir direccin as perder ms tiempo idea detr momento momentum ingls lugar utilizar gradiente actual media mvil ponencial gradient anterior aadar parmetro regla actualizacin controlar cunto gradiente actual cunto deep learning fundamento teora aplicacin anterior utilizar imaginar peso mover bola superficie funcin prdida conforme bajar ficie acumular momento descender ms rpido regla actualizacin descenso gradiente momento vt w lwt wt vt normalmente soler valor similar figura mostrar comparativo algoritmo descenso gradiente momento observar utilizar momento llegar mnimo iteración figurar comparativo algoritmo descenso gradiente momento isaac prez borrero manuel emilio gegndez aria problema corregir momento descenso zona curva superficie ms abruptar dimensin caso descenso gradiente momento oscilar travs zona estrecho gradiente apuntar lado ms superficie entender problema representar superficie funcin prdida plano curva nivel curva nivel funcin variable utilizado ejemplo funcin prdida familia curva representar punto imagen altura figura mostrar funcin variable curva nivel figurar funcin variable plano curva nivel momento gradiente aumentar dimensión cuyo diente apuntar direccin producir convergencia ms rpida reduccin oscilacin figurar deep learning fundamento teora aplicacin figurar reduccin oscilacin momento ejemplo mostrar ejemplo funcionamiento algoritmo censo gradiente ejemplo interesar encontrar mnimo funcin f variable definido algoritmo descenso gradiente utilizar regla actualizacin xt establecer valor learning ratar gradiente funcin directamente derivado x variable f x x regla actualizacin quedar xt aplicar algoritmo descenso gradiente inicializar ble ejemplo x base primero iteración algoritmo transcurriran mostrar continuacin isaac prez borrero manuel emilio gegndez aria iteracin variable calcular valor iteracin aplicar regla actualizacin iteracin volver aplicar regla actualizacin iteracin volver aplicar regla actualizacin iteracin volver aplicar regla actualizacin iteracin volver aplicar regla actualizacin figura mostrar avance algoritmo inicializacin ltimo iteracin ejemplo funcin prdida definir valor entrenar red neuronal funcin prdida quedar definido base salida esperado salida dado red entrada lote dato ejemplo actual salida red depender peso entrada mantener deep learning fundamento teora aplicacin figura ejemplo iteración algoritmo descenso gradiente peso cambiar entrada valor prdida cambiara tambin ocurrir cambiar peso mantener entrada iteracin estudiar funcin prdida cambiar forma isaac prez borrero manuel emilio gegndez aria adagrad algoritmo buscar mejorar descenso gradiente learning ratar adaptativo peso red lugar utilizar valor peso autor adagrad proponer peso utilizar gradiente acumulado actualizar learning ratar forma learning ratar ms alto peso oscilar valor peso oscilar permitir proceso optimizacin centrar utilizar peso realmente til ignorar aportar informacin seguro gradiente regla actualizacin algoritmo dividir learning ratar raz cuadrado suma acumulado cuadrado gradient iteración anterior mostrar continuacin st w wt st w lwt s soler inicializar cero valor pequeo evitar divisin ro ventaja tcnica eliminar necesidad ajustar mente learning ratar contrapartida acumulacin gradiente provocar entrenamiento acabar detenindo él learning ratar pequeo rmsprop objetivo corregir problema adagrad rmsprop adapta learning ratar peso usar medio mvil exponencial gradiente diferencia simple acumulacin gradient adagrad medio mvil exponencial permitir gradient ms antiguo acabar olvidndo él prevenir parada prematura entrenamiento algoritmo publicar travs apunte clase lnea profesor geoffrey hinton regla actualizacin deep learning fundamento teora aplicacin vt w wt vt w lwt tasa decay rate normalmente representar valor pequeo evitar divisin cero adam adaptive moment estimation adam algoritmo buscar combinar mejora aportar momento learning ratar adaptativo adam learning ratar adaptativo utilizar medio mvil exponencial gradiente algoritmo rmsprop adems medio mvil exponencial gradiente lugar gradiente iteracin actual descenso dientir momento adam utilizar estimador calcular momento estimador inicializar cero autor observar gado especialmente primero iteración medida contrarrestar efecto calcular estimador forma corrigierar sesgo mt lwt vt w mt mt t vt vt t regla actualizacin parmetro wt vt mt isaac prez borrero manuel emilio gegndez aria autor proponer valor defecto representar valor pequeo evitar divisin cero backpropagation estudio limitación perceptrn minsky pert red neuronal caer olvido dcada volver ganar protagonismo gracias trabajo david hart geoffrey hinton ronald williams mostrar forma prctico cmo utilizar algoritmo backpropagation propagacin atrs espaol entrenar red neuronal ms capa ginalmente algoritmo backpropagation desarroll dcada investigador independiente demostracin prctico empez utilizar entrenar red neuronal idea subyacer algoritmo regla cadena utilizado clculo derivada composicin función bsicamente red neuronal clculo gradiente funcin prdida funcin peso capa oculto conocer error peso capa oculto salida esperado s cmo afectar peso error cometer red salida puesto calcular composicin operación capa calcular gradiente funcin prdida peso neurona capa salida peso red backpropagation ofrecer solucin problema clculo gradiente capa oculto cmo utilizar él forma utilizar gradiente actualizar peso seguir propuesta algoritmo seccin backpropagation aplicar él red ms capa caso red neuronal objetivo algoritmo entrenamiento minimizar funcin prdida l par ejemplo x entrenamiento concreto dato entrada x valor funcin prdida lw calcular base distancia salida esperado salida obtenido red peso salida deep learning fundamento teora aplicacin red definir ecuacin composicin función intervenir peso red base minimizar funcin prdida calcular rivado peso red lograr él comenzar derivar funcin prdida aplicar regla cadena observacin travs composicin función generar lido red función activacin incluido llegar peso est ajustar observacin segn regla cadena derivada funcin puesto z g calcular z g x notacin leibniz z dz dx dz df df x algoritmo backpropagation etapa forward pass ingls obtener valor prdida resultado operación intermedio red etapa backward pass ingls valor prdida obtenido forward pass resultado operación intermedio calcular gradiente funcin prdida backward pass peso calcular derivada capa salida calcular derivada peso descomponer funcin prdida llegar saber funcin prdida lw calcular base salida red obtener ecuacin ecuacin aparecer funcin activacin capa salida multiplicacin capa entrada peso derivar proceso continuo llegar capa obtener derivada funcin prdida peso red isaac prez borrero manuel emilio gegndez aria red neuronal l capas derivada peso j neurona i capa salida l l wi l j derivado calcular directamente funcin dido depender peso salida red salida ltimo capa yl seguir derivar aplicar regla cadena l wi l j l yl yl wi l j preciso peso est relacionado salida red neurona i capa salida l wi l j l yl i yl i wi l j salida red obtener funcin activacin capa salida f l peso volver aplicar regla cadena derivar funcin activacin peso l wi l j l yl i yl i wi l j l yl i yl i f l f l wi l j ltimo funcin activacin calcular producto peso neurona i entrada capa xl gi wi lt xl requerir volver aplicar regla cadena ltimo derivar producto peso l wi l j l yl i yl i wi l j l yl i yl i f l f l wi l j l yl i yl i f l f l gi l gi l wi l j deep learning fundamento teora aplicacin expresin calcular derivado l wi l j puesto conocer valor trmino l yl i yl i f l f l gi l gi l wi l j gi l wi l j xl j peso j neurona i capa l derivar wi l j est conectado entrada j ltimo capa xl j obtener previamente forward pass pasar calcular peso capa observar expresin obtener calcular derivada peso ltimo capa trmino común peso neurona depender ndice j necesario calcular ms evitar calcular trmino forma redundante peso definir concepto seal error neurona recoger clculo derivada independiente peso derivar seal error denotar l i indicar seal error relativo neurona i capa l propagar atrs peso neurona despu peso capa anterior ah nombre algoritmo backpropagation capa salida seal error l i l yl i yl i f l f l gi l evitar clculo repetido definir derivada peso ltimo capa l wi l j l i gi l wi l j l i calcular previamente capa anterior salida proceso idntico calcular derivada peso funcin prdida aprovechar isaac prez borrero manuel emilio gegndez aria clculo realizado retropropagar seal error capa retropropagacin soler interpretar neurona responsable error cometer neurona enir salida devolver seal error cmo modificar peso corregir salida presente normalmente trabajar capa fully connected neurona capa estn conectado capa implicar neurona capa salida recibir entrada salida neurona capa calcular derivada peso capa inmediatamente salida capa peso contribuir salida neurona contribuir salida red deber calcular derivado peso j neurona i capa l derivada l wi j l yl yl wi j l ylnl yl nl wi j nl nmero neurona capa salida trmino desarrollar regla cadena encontrar expresin contener peso est derivar utilizar expresin seal error obtener l wi j l wi j l nl gnl l wi j expresin gk l referencia multiplicacin peso neurona k capa l entrada xl peso est capa capa l seguir expandeir expresin llegar xl ms salida capa sesgo concretamente salida neurona peso est derivar componente i xl seguir aplicar regla cadena derivar salida capa deep learning fundamento teora aplicacin l wi j l xl i xl i i i j l nl gnl l xl i xl i i i j derivar salida red equivalente neurona capa salida ndiz relativo capa actualizado l wi j l xl i xl i i i f f gi gi wi j l xl i xl i i i f f gi gi wi j l nl gnl l xl i xl i i i f f gi gi wi j seal error neurona capa l trmino derivada ndice peso neurona i capa l seal error i nl l k gk l xl i xl i i i f f gi utilizar seal error derivado quedara l wi j i gi wi j capa salida gi multiplicacin peso neurona entrada capa derivar peso isaac prez borrero manuel emilio gegndez aria gi wi j j procedimiento calcular derivada peso resto capa red generalizar expresin l wi r j r i gi r wi r j r i seal error relativo neurona i capa r j peso calcular derivada peso actualizar valor bia nico cambiar valor multiplicar lugar entrada x venir salida capa directamente valor bia normalmente reemplazar expresin derivada ejemplo peso j neurona i capa r modificar valor bia derivar él gi r gi r wi r j xr j complejidad algoritmo radicar multitud ndiz implicado clculo dificultar comprensin olvidar nico backpropagation calcular derivada funcin prdida peso ahorrar clculo redundant utilizar concepto seal error propagar poder reutilizar principio algoritmo est limitado trabajar ningn tipo funcin activacin operacin concreto utilizar combinacin operación red derivab él ajuste peso requerir derivar operación generar salida red llegar peso est actualizar comn representar clculo backpropagation grafo ción computational graph ingls ms representar serie operación grafo acclico dirigido nodo representar deep learning fundamento teora aplicacin operacin realizar salida nodo conectar figura mostrar ejemplo grafo operación operacin x figurar ejemplo grafo operación grafo mostrar forma simblico operación entrada x grafo computacional representar tambin operación implicado clculo valor prdida incluir evidentemente funcin prdida red neuronal nodo grafo principio realizar forward pass dato ejemplo tener salida nodo llegar obtener valor prdida dato entrada actual comenzar backward pass lar derivado nodo adems calcular derivado calcular envan trmino común seal error nodo anterior actual llegar nodo utilizar peso calcular vada seal error recibido actualizar valor base regla actualizacin algoritmo optimizacin elegido valor derivada finalizar seccin resumen figura representar red problema ajustar funcin xor capa isaac prez borrero manuel emilio gegndez aria neuronar respectivamente función activacin capa conjunto dato valor inicial funcin prdida utilizar detallar figura figurar ejemplo completo red neuronal problema xor figura generar grafo operación equivalente aplicar algoritmo backpropagation calcular derivada peso actualizarlo algoritmo descenso gradiente utilizar funcin prdida grafo representar figura deep learning fundamento teora aplicacin grafo figura calcular forward pass color verde backward pass color rojo iteracin algoritmo observar figura error cometer red ejemplo valor inicial peso figura utilizar derivada calcular peso actualizar él segn regla actualizacin learning ratir establecer wt w valor peso utilizar peso volver calcular prdida dato entrada obtener valor prdida utilizar algoritmo backpropagation calcular derivada peso actualizndo él base regla descenso gradiente conseguir pasar valor prdida reducir valor prdida prctica mayora librera utilizar forma representar ma optimizacin calcular simblico derivado despus utilizar clculo gradiente entrenar red conjunto dato indicar backpropagation podra trabajar isaac prez borrero manuel emilio gegndez aria grafo operación representen red neuronal tipo operacin nodo derivabl deep learning fundamento teora aplicacin figura grafo operación red neuronal figura color azul dato entrada salida morado peso red isaac prez borrero manuel emilio gegndez aria figura clculo iteracin algoritmo descenso gradiente grafo operación red deep learning fundamento teora aplicacin problema aprendizaje basado gradiente algoritmo basado gradiente actualizar peso red ampliamente utilizado resultado presente forma actualizar peso presentar venient estudiar continuacin problema utilizar derivada actualizar peso forma calcular aplicar regla cadena aadir trmino expresin clculo derivada tiplicar anterior calcular derivada peso primero capa red llegar multitud multiplicación producir explosin gradiente exploding gradient ingls desvanecimiento vanish gradient ingls explosin gradiente ocurrir cadena multiplicación trmino implicado mayor unidad ocurrir valor derivado crecer sucesivo multiplicacin llegar valor acabar infinito carecer sentido utilizar él actualizar peso trmino multiplicacin menor unidad secuencia multiplicación reducir valor derivado tiplicacin llegar valer cero valor cercano imposible mover valor peso derivado prcticamente nulo trmino implicado clculo derivado peso derivado funcin activacin segn funcin activacin elijar facilitar dificultar entrenamiento red ra representar forma grfico derivada principal función activacin observar figura utilizar funcin activacin sigmoide valor cero cerca origen valor alejado unidad provocar derivar funcin reducir valor derivada est calcular funcin tanh alcanzar valor ms alto origen poder llegar unidad origen valor cercano cero contrario anterior funcin isaac prez borrero manuel emilio gegndez aria figurar grfica derivada principal función activacin relu modifico gradiente derivado valor positivo multiplicar unidad contrapartida valor negativo valor derivada cero informacin modificar peso caso gradiente mantener rango valor aceptable dr ajustar peso red sufrir problema desvanecimiento explosin independientemente padezcar blema iteracin entrenamiento podrar alcanzar mnimo local gradiente valdr cero obstante difcil tuacin presente superficie funcin prdida cambiar lote ejemplo utilizar esperar cabo nmero iteración mnimo marcar peso actual superficie dejar ser él poder mover él as learning ratar utilizar hacer peso mueir llegar lote ejemplo volver quedar parado mnimo ducindo él especie oscilacin mnimo llegar deep learning fundamento teora aplicacin mover él presente normalmente red neuronal mil millón parmetro definir superficie funcin prdida significar mnimo encontrar mnimo global s punto silla llón derivada apuntar coincidir definir punto peso red mnimo pensar as apuntar estudio mnimo local espacio tanto dimensión ms probable mnimo local parecer mnimo global caer mnimo local problema deber preocupar problema relacionado gradiente conocido olvido catastrfico catastrophic forgetting ingls problema dificultad red mantener desempeo conseguido problema entrenar humano ejemplo aprender montar bici despu conducir coche olvidar cmo montar bici red neuronal contrario entrenar tarea olvidar cmo resolver entrenar forma actualizar peso actualizacin venir marcado gradiente superficie funcin prdida superficie definir dato ejemplo tarea valor prdida derivar penalizar error cometer red tarea dato red intentar aproximar dato ejemplo consecuencia incluir prdida informacin previo peso permitir resolver tarea entrenar ver seccin red neuronal tratado caja gra porqu funcionamiento puesto defini nicamente objetivo minimizar error cometer red tos ejemplo solucin encontrado minimizar error dato conjunto entrenamiento porqu funcionar dato ver citado seccin red neuronal fallar entrada visto realizar leve modificación dato conjunto entrenamiento caso sobreajuste overfitting ingls red problema inherente modelo machine learning producir sobreajuste modelo capaz extrapolar isaac prez borrero manuel emilio gegndez aria resultado dato conjunto entrenamiento caso contrario sobreajuste infraajuste underfitting ingls modelo capaz resultado conjunto dato entrenamiento entender gravedad problema red neuronal estudio mo demostrar red neuronal capaz clasificar correctamente conjunto imgén cuyo clase generar aleatorio idea presente problema sobreajuste entrenar red correr riesgo aprendar dato problema memoria fallen estrepitosamente dato visto problema sobreajuste infraajuste soler relacionado dad expresivo modelo nmero neurona capa jidad podr funcin aproximar red contrario nmero inferior neurona capa necesario aproximar funcin descrito dato conjunto entrenamiento red lograr sultado ejemplo ms evidente ltimo caso intentar aproximar funcin xor nico perceptrn modelo sufrir infraajuste capacidad suficiente aproximar funcin objetivo modelo sufrir sobreajuste significar capacidad ms suficiente aproximar funcin objetivo ms fcil resolver problema tar peso aproximar dato conjunto entrenamiento intentar buscar distribucin subyacer dato aproximar provocar red construir funcin ms complejo intentar aproximar ajustar él nicamente dato recibir comprender problema sobreajuste infraajuste figura mostrar ejemplo modelo observar funcin aproximar color rojo funcin aproximado modelo color azul f x base punto ejemplo suministrar entrenamiento color morado observar modelo central ajustar curva describir dato modelo ajustar mente dato ejemplo describir curva complejo distir curva describir dato ltimo modelo capacidad suficiente aproximar punto ejemplo modelo lograr comportamiento modelo central deep learning fundamento teora aplicacin figura ejemplo sobreajuste izquierda ajuste centro infraajuste derecha base conjunto dato ejemplo color morado color rojo indicar funcin objetivo color azul funcin descrito modelo figurar dato visto capaz generalizar dato ejemplo dato muestra lograr obtener modelo permitir resolver problema dato entrenamiento conseguir red generalice capacidad representativo conjunto dato entrenar falta generalizacin modelo deber cometer error aproximar funcin intentar aproximar funcin f red neuronal f lograr aproximar exactamente funcin objetivo cometer error forma f f asumir error error irreducible problema inherente conjunto dato outlier ruido hacer representar correctamente punto funcin original resto error cometer modelo soler sentar distribucin normal medio desviacin tpico error entrada x definir valor esperado diferencia cuadrado salida red salida esperado x calcular sesgo varianza descomponer funcin error llegar explicar él base estimador x error cometer modelo falta capacidad infraajuste modelo sesgo alto importar dato entrada isaac prez borrero manuel emilio gegndez aria error base evitar problema modelo complejo demasiado capacidad error variar dato entrada puesto sern parecido dato sobreajustar modelo error distir aprendido modelo producirn error alto caso modelo varianza alto comprender concepto sesgo varianza figura mostrar problema ejemplo representar predicción tro diana hecho modelo sufrir posible combinación tipo error sesgo alto lograr acertar centro diana error base varianza alto punto acertar alejar centro ideal error bajo parejo normalmente error incompatible intentar reducir sesgo incrementar complejidad modelo aumentar varianza viceversa dilema conocer compromiso sesgo varianza intentar alcanzar hora disear arquitectura red problema figura mostrar forma grfico efecto complejidad red error cometer sesgo varianzar as punto equilibrio ideal punto buscar situar complejidad modelo definir arquitectura reducir error aparecer problema comportamiento modelo observar figura definicin complejidad modelo arquitectura red adecuado problema est abordar padecer sesgo varianzar alto siguiente sección intentar pauta lograr namiento exitoso evitar problema mencionado seccin deep learning fundamento teora aplicacin figura posible escenario modelo sesgo varianza isaac prez borrero manuel emilio gegndez aria figura bsqueda compromiso sesgo varianzar definir complejidad modelo deep learning fundamento teora aplicacin parmetro entrenamiento entrenamiento red neuronal conllevar eleccin multitud metro comenzar entrenamiento arquitectura red learning ratar nmero ejemplo utilizar lote entrenamiento ejemplo parmetro elegir mano idea claro efecto resultado entrenamiento valor escogido parmetro llamar hiperparmetro puesto parmetro definir entrenamiento permitir encontrar valor parmetro pesos red hiperparmetro soler elegir él base criterio cia previo estudio arte problema similar tratar resolver pequea prueba realizar conjunto dato namiento mejor resultado mostrar sern har entrenamiento ms extenso red obstante nico entrenamiento soler suficiente mejor resultado soler desarrollar ferent experimento forma cclico idea mejorar resultado experimento probar hiperparmetro centrar él recoleccin ms dato mejorar calidad desarrollo red neuronal soler ciclo vida representado figura definido problema tratar obtener preparar dato definir hiperparmetro entrenamiento finalizado obtener peso despliegue produccin red monitorizar comportamiento base monitorizacin podr definir experimento mejorar resultado observar figura desarrollo red neuronal proceso iterativo eleccin hiperparmetro as dato jugar papel esencial entrenamiento red adems divisin experimento prueba hiperparmetro vo dato hacer etapa entrenamiento soler dividir nmero consistir utilizar dato conjunto namiento entrenar red venir definido nmero ración necesitar algoritmo optimizacin utilizar dato conjunto entrenamiento iteracin componer tanto ejemplo isaac prez borrero manuel emilio gegndez aria figurar ciclo vida desarrollo red neuronal definir tamao lote algoritmo entrenamiento figura mostrar relacin concepto finalizar evalar red obtener mtrica conjunto validacin tambin soler aprovechar comprobar mtrica mejorar mejor resultado obtenido momento entrenamiento caso mejora almacenar valor peso actual mejor valor peso red entrenamiento base mtrica mejorar deep learning fundamento teora aplicacin figura relacin dato conjunto dato tamao lote iteracin entrenamiento isaac prez borrero manuel emilio gegndez aria validacin costoso tambin optar almacenar peso despus acabar entrenamiento elegir red base resultado conseguir conjunto validacin almacenar peso tamao ocupar almacenar nmero lugar continuacin detallar principal hiperparmetro involucrado entrenamiento red recomendación hora finirlo arquitectura red eleccin arquitectura red definir pacidad deber acorde problema est resolver eleccin fcil definir nmero capa neurona funcin activacin utilizar opcin ms sencillo consistir utilizar arquitectura red emplear problema similar resultado contrastado garanta red funcionar problema similar venir teorema conocido freir lunch segn teorema modelo mejor resultado dems problema intuir utilizar arquitectura red diseado problema complejo aplicar él problema sencillo sobreajustar go resultado arquitectura ms simple animar buscar modificar arquitectura base red aplicar él problema ajustar complejidad requerido ejemplo sentido utilizar red neuronal ciento capa neurona abordar problema funcin xor resolver neurona organizado capa tcnicar inicializacin estudi seccin alternativa inicializar peso red parmetro ms elegir intentar utilizar tcnica transfer learning inicializar red peso obtenido algn namiento previo forma ms probabilidad comenzar zona cercano mnimo objetivo entrenamiento ms rpido inicializacin puramente aleatorio nmero entrenamiento entrenamiento componer determinado nmero entrenamiento nmero deep learning fundamento teora aplicacin llegar reducir valor prdida suficiente empezar obtener resultado depender complejidad problema necesitarn ms entrenamiento par soler suficiente algoritmo acercar mnimo tamao lote ejemplo nmero ejemplo calcular valor prdida influir forma superficie funcin prdida dificultad tendr algoritmo optimizacin alcanzar mnimo soler emplear potencia definir tamao lote valor ms habitual eleccin depender potencia equipo utilizar entrenamiento tamao lote ver limitado memoria disponible soler motivo elegir valor almacenamiento peso red experimento realizar buscar obtener mejor peso red problema decidir cundo qu criterio guardar valor peso derlo utilizar etapa inferencia coment anteriormente ideal almacenar peso mejorar valor prdida mtrica conjunto validacin algoritmo optimizacin backpropagation nico opcin calcular derivada peso capa oculto red forma utilizar derivado actualizar peso venir indicado algoritmo optimizacin elegido seccin estudiar pal algoritmo optimizacin ventaja adems atender bondad algoritmo presente mos descenso gradiente ms rpido calcular adam ejemplo ventaja algoritmo descenso soler optar algoritmo ms simple algoritmo detener llegar mnimo ms probable mnimo aplanado pronunciado ventaja nimos aplanado frente pronunciado ms probabilidad coincidir mnimo real est buscar presente mnimo definir lote ejemplo actual conjunto dato problema real mnimo aplanado significar modificar levemente valor peso seguirn situado isaac prez borrero manuel emilio gegndez aria mnimo mnimo pronunciado dejar ser él mover lor peso figura mostrar ejemplo tipo mnimo figurar diferencia mnimo pronunciado aplanado funcin prdida definido lote ejemplo definido dato parmetro algoritmo optimizacin algoritmo optimizacin cogido soler requerir definir parmetro learning ratar parmetro ms importante escoger algoritmo entrenamiento l depender medida lograr alcanzar mnimo mayora parmetro algoritmo optimizacin soler dejar valor defecto proponer autor eleccin learning ratar requerir cuidado especial valor relacionado tamao lote nmero ejemplo lote ms probabilidad gradiente indicar mnimo real funcin paso ms iteracin usar learning deep learning fundamento teora aplicacin alto contrario disponer ejemplo gradiente contendr ruido marcar mnimo real zona cercano paso menor tamao fiar direccin marcado iteracin sern necesario ms iteración llegar posicin precio pagar utilizar ejemplo ruido gradiente ayudar escapar mnir local estudiar norma general utilizar learning ratar tamao lote ejemplo dividir dividir tamao lote ejemplo usar learning ratar ejemplo valor learning ratar pequeo algoritmo alcanzar mnimo acabar divergeir paso alejar él mnimo sern necesario prueba garantizar entrenamiento llevar él cabo valor escogido mtrica multitud mtrica evaluacin fica determinado problema base objetivo perseguir elegir mtrica evaluar red sern indicar cundo guardar peso utilizar etapa inferencia ejemplo est desarrollar sistema diagnstico des quizs interesen mtrica tener ms sistema falso negativo paciente enfermedad clasificado sano falso sitivo paciente enfermedad clasificado enfermo puesto mdico determinar despus paciente est realmente enfermo har descartar antemano clasificar él sano estar enfermo funcin prdida seccin habl función prdida ms utilizado error cuadrtico nico opción disponer ecm ms indicado problema regresin ce clasificacin obstante modificar función adaptar él casustica problema est abordar ejemplo problema outlier conjunto dato funcin ecm generar valor elevado error elevar diferencia cuadrado podrar utilizar valor exponente funcin evitar valor alto tambin valor contar valor prdida menor isaac prez borrero manuel emilio gegndez aria simple diferencia elevar él cuadrado depender problema necesitar utilizar tipo funcin prdida modificacin anterior limitacin tipo funcin utilizar derivable algoritmo backpropagation calcular derivada peso parmetro tcnica control entrenamiento seccin estudiarn tcnica controlar principal problema gir entrenamiento red adems elegir cul mayora tcnica soler algn parmetro habr definir empezar entrenamiento definido valor hiperparmetro podr rimento base mtrica alcanzado decidir cul prueba seccin intentar pauta seguir lograr namiento exitoso detectar problema poder aparecer namiento ayudar elegir hiperparmetro cul modificar cmo control seguimiento namiento entrenamiento red neuronal aparecer cualquiera problema estudiar control seguimiento entrenamiento tendrn garanta transcurrir normalidad seguimiento entrenamiento detectar error producir normal soler registro valor prdida mtrica estudiar evolucin entrenamiento concreto soler almacenar valor prdida iteracin entrenamiento mtrica calcular iteracin as media finalizar valor medio prdida mtrica conjunto deep learning fundamento teora aplicacin validacin esperar avance entrenamiento valor prdida comiencir reducir él mtrica aumentar valor obtenido conjunto entrenamiento parejo conjunto validacin anomala comportamiento descrito anteriormente soler sntoma algn problema tendr corregir seguimiento valor prdida mtrica soler representacin grfica eje x representar iteración namiento eje valor prdida mtrica segn est sentar figura mostrar grfica tpica entrenamiento valor prdida mtrica conjunto entrenamiento cin grfica figura representar valor prdida iteración entrenamiento valor mtrica conjunto validacin observar cmo valor prdida mienza alto cabo nmero reducir forma notorio caso mtrica ver comenzar tener valor acabar subir conforme baja valor prdida tamiento principio entrenamiento peso red obtenido inicializacin escogido soler generar valor alto prdida avance entrenamiento algoritmo mizacin capaz bajar rpidamente valor prdida aumentar valor mtrica tambin esperar reduccin valor prdida est correlacionado mejora mtrica estudio grfica prdida mtrica podrn detectar mayora problema conocer sobreajuste infraajuste observar valor prdida conjunto entrenamiento alto conjunto validacin significar modelo est sobreajustar dato conjunto entrenamiento capaz generalizar comete error conjunto validacin error par quedar valor alto sntoma infraajuste modelo capacidad suficiente resolver problema caso aumentar conjunto dato fcil aprendrselo reducir complejidad modelo eliminar capa reducir nmero neurona caso aumentar nmero capa neurona isaac prez borrero manuel emilio gegndez aria figura grfica habitual entrenamiento red neuronal capaz seguir reducir error quedar él valor alto disponer mtrica conjunto entrenamiento tambin detectar problema analizar mtrica suficiente observar valor prdida figura mostrar cmo detectar problema grfica prdida adems problema inherente entrenamiento modelo estudio grfica permitir detectar error haber cometer macin ejemplo aparecer valor nar not number valor deep learning fundamento teora aplicacin figura deteccin sobreajuste infraajuste grfica prdida prdida significar intentar algn clculo ble divisin cero inf infinito resultado valor infinito multiplicar nmero caso soler indicar algn error cdigo eleccin hiperparmetro ocurrir observar valor caracterstico valor prdida var entrenamiento mtrica lector entrenar red neuronal problema clasificacin binario funcin algn error cdigo isaac prez borrero manuel emilio gegndez aria visto aparecer valor valor prdida valor generar vector entrada funcin prdida componente mo valor normalmente aplicar él funcin softmax componente valor probabilidad valor prdida problema soler algn momento entrada capa red completamente cero analizar mtrica trabajar problema clasificacin clase distribucin conjunto validacin clase respectivamente observar accuracy est soler significar red est clasificar entrada clase grfica prdida tambin ayuda elegir valor learning ratar funcin valor escogido descencer aumentar valor prdida efecto learning ratar grfica prdida mostrar figura figurar efecto valor learning ratar grfica prdida observar figura valor learning ratir alto acabar vocar valor prdida comience crecer descontrolado paso ms necesario acabar zona est indicar deep learning fundamento teora aplicacin gradiente valor alto observar cmo  valor prdida bajo rpidamente llegado punto capaz descender ms paso necesario valor necesitar ms iteración habitual alcanzar mnimo bajar valor prdida eleccin valor learning ratir adecuado har valor prdida bajar paulatinamente necesario esperar entrenamiento detectar problema lanzar experimento tomar conjunto reducido dato nico lote ejemplo modelo capaz breajustar él capaz ajustar él nico lote ejemplo aumentar complejidad modelo abordar problema hiperparmetro entrenamiento valor adecuado entrenamiento nico lote hacer correctamente ca sobreajuste inmediato podr pasar entrenar conjunto entrenamiento completo entrenamiento habitual elegir ejemplo lote aleatoriamente conjunto dato repetir comienzo entrenamiento ordenar forma aleatorio seguro elegir ayudar superficie prdida cambiar an ms lograr escapar mnir local registro valor prdida red ejemplo lote tener él repetir ejemplo probabilidad segn valor prdida forma obligar red reducir valor prdida ejemplo difcil detectar algn problema podrn tomar medida continuacin detallar tcnica ms utilizado corregir problema convergencia entrenamiento equilibrar nmero ejemplo clase producir breajustar modelo problema deber él simple clase problema ms minante conjunto dato imagine problema clase dato clase caso modelo podra optar clasificar entrada perteneciente clase cometera error evitar problema soler utilizar isaac prez borrero manuel emilio gegndez aria nmero ejemplo clase validacin as seguro mtrica representativo evitar modelo breajustar intentar equilibrar nmero ejemplo clase lote entrenamiento muestreo clase predominante obtener nmero ejemplo clase entrenar poltica actualizacin learning ratar algoritmo utilizar learning ratar adaptativo descenso gradiente comn utilizar poltica actualizacin learning ratar valor comienzo valor alto learning ratar garantizar descender rpidamente mnimo llegar regin valor alto hacer imposible continuar descender figurar instante interesar modificar valor learning ratar distinto alternativa actualizacin ms sencillo consistir disminuir valor cabo nmero detectar valor prdida aplanar valor similar figurar diferencia descenso gradiente valor learning ratar fijo variable normalizacin dato dato presentar media cero desviacin tpico unidad entrenamiento red neuronal ms rpido caso contrario dimensión entrada mover rango dems provocar superficie prdida alargar dimensin ms dems provocar aparicin oscilación entrenamiento figura mostrar efecto normalizacin dato deep learning fundamento teora aplicacin superficie funcin prdida dimensión mantener r rango reducido gradiente tendr valor alto entrenamiento ms rpido adems dividir desviacin tpico restar media valor estn rango tambin normalizar dividir mximo dejar valor rango figurar superficie funcin prdida dato normalizado normalizar regularizacin tcnica consistir aadir penalizacin clculo error base valor peso fundamentar modelo complejo ms propenso sobreajuste soler contar valor alto peso utilizar él operación cambio pequeo entrada traducir cambio brusco salida aqu gularizacin lograr reducir complejidad modelo obligar él utilizar peso valor bajo funcin describir peso modelo forma complejo figurar normalmente soler emplear norma trmino aadar problema optimizacin calculado peso w modelo isaac prez borrero manuel emilio gegndez aria arg mn w lw caso parmetro regularizacin hiperparmetro ms ajustar figurar efecto regularizacin complejidad modelo batch normalization adems normalizar dato entrada red bin normalizar entrada distinto capa red batch normalization travs capa red dato sufrir modificación acabar valor alto definitiva problema conjunto dato lizado batch normalization proponer normalizacin dividir desviacin tpico lote actual ejemplo restar media dato adems parmetro transformacin neal dato normalizado parmetro ajustar red resto peso neurona permitir capa ajustar dato rango valor ayudar reducir error traducir aprendizaje ms rpido etapa entrenamiento mantener medio mvil exponencial medio desviacin tpico lote emplear etapa inferencia concreto batch normalization recibir entrada lote ejemplo b xn calcular media desviacin tpico deep learning fundamento teora aplicacin b n n xi b n n xi valor utilizar normalizar dato x i xi b b valor pequeo evitar divisin cero ltimo lote ejemplo salida b yn obtener realizar transformacin lineal dato normalizar x segn valor aprendido entrenamiento yi x i b yn dropout tcnica buscar evitar sobreajuste convertir red nal conjunto red ms simple aprender resolver problema promediar salida lograr él aleatoriamente desactivar red neurona capa aplicar tcnica propagacin figurar implicar n nas capa desactivar aleatoriamente obtener red ms pequea iteracin entrenamiento escoger sibl red entrenar forma entrenar red tcnica ver él entrenar coleccin red ms pequea etapa inferencia utilizar dropout utilizar red completo provocar obtener resultado red ponderacin subred entrenar gracias controlar breajustar provocar existir redundancia red desactivacin neurona controlar parmetro p indicar probabilidad neurona acabar desactivado desactivacin neurona sistar salida cero neurona aplicar dropout isaac prez borrero manuel emilio gegndez aria salida escalar forma suma entrada permanezca inalterado figurar red dropout aumento dato alternativa modificar arquitectura red utilizar tcnica controlar sobreajuste aumento conjunto tos imagine disponer conjunto imgén mdica perteneciente radiografa conjunto limitado paciente ms probable red acabar sobreajustar él dato suficiente generalizar caso optar aumentar artificialmente deep learning fundamento teora aplicacin conjunto dato ejemplo poner bastar girar cir ruido recortar cambiar brillo contraste tcnica aumento dato sern especfica problema est tratar habr consultar literatura encontrar ejemplo tcnica resultado contrastado objetivo generar dato dato original seguir identificar clase asociado originalmente limitacin gradiente idea utilizar valor gradiente ningn tipo control función prdida ecm generar gradient aparecer outlier dato estn normalizar gradient valor utilizar situacin intentar actualizar peso gradient acabarn situado zona alejado mnimo valor peso prdida llegar suficiente valer infinito limitacin gradiente consistir valor mximo mnimo gradiente forma superar él seguro desplazar ms cantidad nico iteracin entrenamiento figura mostrar efecto limitar gradiente superficie prdida producir oscilación lancir entrenamiento trabajo consistir seguimiento registro grfica haber programar detectar posible problema cambio deberan experimento lograr as mejor resultado isaac prez borrero manuel emilio gegndez aria figurar efecto limitacin gradiente entrenamiento captulo red neuronal recurrente introduccin tipo dato componente temporal inherente leer texto est presente orden temporal dato cerebro procés imagen caso lectura sonido caso llegar detr permitir reconocer letra convertir enunciado caso dato individual carecer sentido imagen sonido aislado interesar sequencia completo frase javier comprar coche juan frase juan comprar coche javier descomponer frase componente palabra analizar individualmente quin coche quin comprar creacin secuencia dotar orden palabra frase responder pregunta problema precisar secuencia resolver necesitar contexto abordar él ejemplo dar imagen ra conocer cul posicin vagn fotograma disponer vdeo fotograma anterior actual dr predecir seguridad posicin figura nico forma resolver problema disponer secuencia imgén deep learning fundamento teora aplicacin figura informacin instante anterior predecir posicin vagn cerebro est constantemente analizar secuencia dato imgén sonido componente temporal jugar papel esencial forma cerebro procés informacin gracias memoria cerebro capaz almacenar informacin sensorial ayudar resolver problema presente versin digital red neuronal artificial contar mecanismo explcito permitir procesar secuencia dato red neuronal memoria problema relacionado procesamiento secuencia tratar problema tpico red neuronal ejemplo adivinar palabra frase red neuronal optar siguiente alternativa frase entrada utilizar frase entrada red establecer tamao entrada frase valor fijo podr analizar frase ms tamao fije adems tamao fijado entrada acabar isaac prez borrero manuel emilio gegndez aria figura base informacin predecir vagn mover derecha millón pesos neurona puesto peso componente vector entrada ventana tamao fijo alternativa opcin consistir dar él red entrada tamao fijo compuesto palabra final forma procesar frase larga reducir nmero parmetro alternativa obstante predicción dependencia plazo estn presente ventana entrada red ejemplo utilizar palabra anterior entrada red frase madrid ciudad bonito encantar volver entrada consistir vector codificacin ejemplo palabra volver comienzo frase est informacin necesitar predecir correctamente palabra independientemente alternativa utilizar desventaja red neuronal analizar secuencia entrada pendiente red definir peso componente entrada forma entrada gustar coche coche deep learning fundamento teora aplicacin gustar peso procesar frase coche coche aparecer ms comienzo peso procesar coche red neuronal invariante posicin bio orden palabra generar salida completamente ambos entrada significar reutilizar peso permitir detectar coche frase detectar él definir peso detectar coche posicin especfico ta explosin combinatorio palabra posición frase idea qu difcil imposible procesar secuencia red neuronal tradicional red neuronal encontrar limitado analizar secuencia modelo ms indicado propsito crear red neuronal tes red neuronal recurrente red neuronal estudiar captulo conocido red feedforward recibir entrada procesar red generar salida funcionar nico sentido componente red presentar ciclo red grafo acclico dirigido contrario red feedforward red neuronal recurrente red recurrente recurrent neural networks rnn ingls llamado as john hoppelfield venar usar red neuronal tradicional contar retroalimentacin mite memoria oculto hiddar statir ingls informacin contexto entrada actual retroalimentacin gracias conexin bucle transmitir oculto iteracin entrada iteracin actual dato entrada oculto iteracin utilizar calcular oculto actual lugar calcular salida red base dato entrada red recurrente calcular salida base oculto dato isaac prez borrero manuel emilio gegndez aria entrada oculto iteracin figura mostrar esquema red recurrente figurar esquema red neuronal recurrente observar figura facilitar comprensin rona capa representar nico neurona cuyo peso englor procesamiento realizar neurona capa neurona centrar calcular oculto h iteracin t dato entrada iteracin actual xt oculto iteracin salida actual red yt ms transformacin lineal oculto actual ht versin tradicional funcin activacin elegido neurona aplicar componente vector salida forma individual clculo realizar red recurrente similar red neuronal tradicional combinacin lineal oculto ht matriz pesos wy generar salida red combinacin deep learning fundamento teora aplicacin lineal entrada actual xt vector pesos wx cin lineal oculto iteracin matriz pesos wh permitir calcular oculto iteracin actual ht suma operación anterior aplicacin funcin activacin tanh eleccin funcin activacin adems introducir linealidad oculto controlar problema desvanecimiento gradiente habitual red función activacin salida rango probable aparezcar valor alto bajo puesto iteracin recibir entrada do oculto actual cuyo valor positivo negativo incrementar reducir respectivamente valor oculto permitir controlar crecimiento valor oculto dimensin entrada x as salida vendrn determinado dato problema dimensin oculto h dir él entrenar hiperparmetro ms habr ajustar entrenamiento red funcin problema est abordar aspecto importante red independientemente nmero iteración realizar utilizar peso iteración utilizar ms peso entrada ms ocurra red neuronal tradicional analizar frase carcter carcter procesar peso instante oculto cambiar funcin peso aprendido red problema permitir salida entrada base contexto actual entrada venir oculto inicializacin peso red tcnica ver red neuronal seccin adems peso dar él valor inicial oculto h utilizar oculto iteracin alternativa inicializar oculto ms sencillo consistir inicializar él valor fijo ejemplo opcin ms recomendable producir resultado llegar generar sobreajuste alternativa podra inicializar oculto algn valor aleatorio peso lugar utilizar peso entrenabl valor inicial oculto forma oculto inicial parmetro ms deber ajustar red isaac prez borrero manuel emilio gegndez aria base dato problema red aprender oculto partida permitir alcanzar mejor resultado simple inicializacin aleatorio constante obstante conjunto dato entrenamiento suficientemente red lograr aprender inicial simple inicializacin aleatorio logre mejor resultado formar red recurrente multicapo proceder red neurona recurrente formar capa s dir tanto neurona capa desear conectar salida entrada salida ltimo neurona salida red figurar figurar esquema red neuronal recurrente multicapo deep learning fundamento teora aplicacin neurona red oculto matriz pesos pendiente red neuronal interesar obtener salida red salida ltimo capa calcular procesar entrada neurona capa suministrar salida entrada llegar obtener salida ltimo neurona capa entrenamiento red neuronal currente entrenamiento red neuronal recurrente similar red neuronal tradicional entrenamiento red plantear blema optimizacin matemtico minimizar funcin dido minimizar funcin utilizar algoritmo zacin iterativo basado gradiente funcin prdida estudiar seccin red tradicional neurona capa oculto emplear backpropagation calcular derivada peso neurona capa conjunto dato entrenamiento d componer secuencia tos entrada x secuencia valor salida esperado ejemplo entrenamiento componer dato secuencia entrada instante t xt salida esperado instante t yt diferencia red neuronal cional separar dato forma crear conjunto entrenamiento validacin prueba puesto dato independiente seguir orden secuencia dato secuencia contexto venir definido dato secuencia t anterior red capturo oculto respetar orden dato red crear contexto entrada actual real ejemplo intentar predecir temperatura da base temperatura actual red trabajo correctamente recibir dato orden das anterior generar oculto alternativa hora trabajar secuencia dato isaac prez borrero manuel emilio gegndez aria entrenar red recurrente solucin ms simple consistir dividir secuencia original forma utilizar entrenar red validar prueba respectivamente desventaja solucin dato quedar utilizar entrenar contener situación aparecer conjunto entrenamiento alternativa utilizar ventana deslizante definido nmero instante tiempo dividir conjunto original ventana comenzar utilizar entrenar validar prueba instante entrenamiento desplazar unidad ventana volver entrenar figurar configuracin aprovechar dato secuencia entrenar listo divisin conjunto dato namiento red cambio importante entrenamiento red tradicional red recurrente componente temporal red instante t generar funcin entrada actual oculto red generar dato instante anterior instante backpropagation tendra calcular derivada peso tener situacin peso utilizar instante tiempo entrenar red recurrente utilizar versin modificado algoritmo backpropagation conocido backpropagation travs tiempo backpropagation through time bptt ingls definir nmero determinado instante backpropagation calcular forward pass despus calcular derivada peso backward pass permitir clculo derivado equipo actual limitar requerimiento procesamiento almacenamiento red neuronal tradicional red recurrente aplicar algoritmo bptt grafo operación red grafo red recurrente requerir desenrollar iteración red realizar iteracin crear grafo dirigido ciclo figura mostrar ejemplo cmo proceso iteración entrenamiento error consistir suma error iteracin obtener grafo operación desenrollado ciclo deep learning fundamento teora aplicacin figura divisin ventana secuencia dato iteracin entrenamiento entrenar validar v prueba p cular derivada forma calcular red neuronal dicional backpropagation isaac prez borrero manuel emilio gegndez aria figurar proceso desenrrollado grafo operación iteración red neuronal recurrente deep learning fundamento teora aplicacin aplicación aplicación red neuronal recurrente principalmente samiento secuencia dato entender secuencia dato conjunto dato orden asociado texto audio video serie temporal ejemplo secuencia dato forma general problema relacionado dato secuencial conocer modelado secuencia sequence modeling ingls modelado secuencia distingar problema funcin hacer entrada salida red tipo problema secuencia entrada salida secuencia caso problema duccin texto entrada secuencia texto idioma original salida texto idioma destino problema ignorar calcular salida instante anterior ltimo est interesado salida red ltimo instante tiempo procesar secuencia entrada problema tipo podra anlisis sentimiento texto entrada comentario cliente salida clasificacin secuencia entrada comentario positivo negativo ejemplo problema recibir entrada dato iteración base salida cambiar ejemplo tipo problema generacin descripcin imagen entrada red imagen salida palabra generar red iteracin problema considerar problema secuencia puesto problema abordar red neuronal tradicional dato entrada independiente nico salida depender dato entrada actual aplicación red neuronal recurrente destacar cin conversin voz texto prediccin serie temporal cesamiento lenguaje natural natural language processing npl ingls permitir crear chatbot red neuronal recurrente utilizar modelado isaac prez borrero manuel emilio gegndez aria dinmico control obstante presente red neuronal recurrente trabajar forma discreto continuo limitación red neuronal recurrente considerar mquina turing completa significar utilizar modelar algoritmo red neuronal disponer macin acerca arquitectura correcto valor peso problema obtener él proceso ensayo error limitación encontrar red entrenamiento limitar backpropagation nmero prefijado iteración entrenar secuencia completo est descartar informacin relevante salida problema aadir él vanecimiento gradiente producir calcular oculto iteracin funcin activacin tanh suponer mayor reto entrenamiento motivo red recurrente tambin acceder informacin ms lejana instante actual problema relevante captulo deep learning introduccin aprendizaje profundo deep learning ingls campo aprendizaje tomtico machine learning ingls englobar gencia artificial figurar figurar relacin inteligencia artificial machine learning deep learning trabajar tcnica machine learning utilizar conjunto dato ajustar modelo predefinido resolver determinado problema dato trabajar modelo soler dato original dato deep learning fundamento teora aplicacin bruto problema lugar soler definir serie caracterstica featur ingls obtener dato original utilizar modelo resolver problema enfoque requerir estudio previo problema serie experto ayudar definir mejor caracterstica modelo suponer ingente cantidad tiempo dinero forma tradicional trabajar modelo machine learning disponer caracterstica ms relevant solucionar problema elegir modelo machine learning red neuronal encargar aprender transformacin aplicar él caracterstica elegido obtener salida esperado contraste enfoque tradicional machine learning tcnica deep learning encarguir extraer s caracterstica ms relevant dato original resolver problema adems aprender transformacin aplicar caracterstica salida esperado figura mostrar comparativo desarrollo sistema basado tcnica machine learning deep learning perceptrn multicapo modelo deep learning ms bsico capa disponer aproximador universal función gracias popularizacin algoritmo backpropagation dcada comenzar aparecer red neuronal ms capa llamado red neuronal da deep neural networks ingls distinguir él red nico capa conocido red profundo shadow neural networks ingls principio capa suficiente aproximar cin prctica nmero neurona necesario resolver problema real nico capa oculto elevado lugar aumentar anchura capa oculta nmero neurona soler optar aumentar profundidad red aadeir ms capa forma general nmero neurona necesario resolver problema crecer lineal exponencial caso limitación red neuronal nico capa zo existir inters utilizar red neuronal profundo dotar ms capacidad red disponer algoritmo backpropagation entrenamiento red neuronal profundo difcil isaac prez borrero manuel emilio gegndez aria figurar comparativo desarrollo sistema basado tcnica machine learning deep learning limitación capacidad procesamiento to ao trabajo publicado geoffrey hinton simon osindero teh logr entrenar red nmero capa har lograr entrenar fecha motiv investigador comenzar entrenar red neuronal ms profundo bitual destacar comenz utilizar trmino deep learning deep learning fundamento teora aplicacin referir él entrenamiento red ms profundo entrenado momento caracterstica tcnica deep learning extraccin caracterstica realizar dato original soler generar representacin jerrquico distribuido extraer caracterstica menor complejidad forma independiente definindo él caracterstica nivel superior funcin caracterstica nivel inferior objetivo tcnica deep learning consistir encontrar caracterstica alto nivel forma modelo transformar caracterstica salida esperado ms facilidad emplear representacin original dato ejemplo caso clasificacin imgén tcnica deep learning procesar imagen entrada nivel extraer caracterstica nivel color borde nivel superior detectar caracterstica ms compleja ms alto nivel rueda coche cara persona caracterstica ms alto nivel sern utilizado generar salida modelo figura observar jerarqua caracterstica generarar modelo deep learning procesar imagen observar cmo nivel ms alto jerarqua capaz detectar objeto entrada base deteccin caracterstica ms nivel capa anterior forma general modelo deep learning distinguir etapa etapa extraccin caracterstica etapa transformacin rstica etapa consistir crear jerarqua caracterstica dato bruto problema etapa tomar caracterstica ms alto nivel jerarqua utilizar aplicarl transformacin permitir salida esperado sistema ejemplo caso red neuronal aplicado clasificacin imgén primero capa podrar extraccin caracterstica nivel aumentar profundidad red caracterstica extrada serar ms compleja definido terstica extrada capa ms nivel ltimo capa red utilizar él aprender transformacin caracterstica ms alto nivel salida esperado figura mostrar esquema completo modelo deep learning etapa principal desventaja utilizar red neuronal extraccin isaac prez borrero manuel emilio gegndez aria figura descomposicin pxel imagen representacin jerrquico caracterstica menor complejidad tico desempeo puesto invariant posicin persona imagen desplazado procesar peso totalmente procesar posicin original adems suponer ingente cantidad pesos ajustar posicin imagen neurona red limitar medida capacidad des neuronal trabajar extraer caracterstica definir caracterstica mano nivel brillo histograma trabajar rstica predefinida red encargar aprender transformacin caracterstica salida esperado reducir nmero pesos ajustar caracterstica extrada soler menor dimensin dato red neuronal obtenar mejor resultado utilizado perspectiva machine learning deep learning cambi deep learning fundamento teora aplicacin figura esquema completo modelo deep learning etapa menzar utilizar tipo red conocido red neuronal cional estudiar detalle seccin red neuronal nal s invariant posicin detectar caracterstica posicin entrada necesitar nmero pesos hito poner deep learning mapa aplicacin red neuronal convolucional alexnet creado geoffrey hinton alumno doctorado ao competicin clasificacin imgén net competicin exigir clasificacin imgén ms veinte mil clase ms millón imgén disponible trenar tcnica tradicional machine learning incapaz bajar error alexnet capaz bajar él capt isaac prez borrero manuel emilio gegndez aria ticamente inters comunidad tipo tcnica deep learning resultado novedad introducir alexnet gpu namiento red neuronal aprovechar paralelismo gpu entrenar red millón parmetro par da inaudito populariz red neuronal convolucional imagenet error seguir bajar ao ao llegar bajar considerado error humano clasificacin nes figura mostrar evolucin porcentaje error cometido modelo ganador imagenet ltimo ao figurar evolucin porcentaje error cometido modelo ganador imagenet ltimo aos principal ventaja tcnica deep learning machine ning facilitar desarrollo modelo necesario invertir elevado cantidad hora trabajo experto disear caracterstica adems sistema basado machine learning contar sesgo previo suponer deep learning fundamento teora aplicacin cota superior desempeo sistema experto elegir tico limitado cantidad informacin capaz manejar aadir ms dato contraproducente ms difcil discernir encontrar caracterstica ms relevant sistema deep learning carecer limitacin cambiar caracterstica recibir ms dato encontrar oportuno mejorar resultado proveer algoritmo ms dato ms fcil modelo ralice respondar dato tcnica tradicional garantizado radicar xito obstante problema ms sencillo disponer dato tcnica deep learning problema entrenar él correctamente dir comparado modelo ms sencillo sobreajustar él conjunto dato adems ms costoso entrenamiento explicar qu  prcticamente utilizar forma ms general tipo tcnica resolver problema abaratamiento almacenamiento disponibilidad dato gracias internet figurar aumento dad cmputo gpu entrenamiento modelo deep learning resultado propiciar auge campo figura mostrar comparativa rendimiento modelo machine learning deep learning base tamao conjunto dato disponible pasar desapercibido investigador trabajar comienzo deep learning contar reconocimiento merecer caber destacar figura yann lecun director laboratorio inteligencia artificial facebook creador red neuronal convolucional historia yoshua bengio investigador universidad montreal director instituto algoritmo aprendizaje montreal mila geoffrey hinton investigador google brain universidad toronto miembro royal society considerar padrino deep galardonar premio turing considerado premio nobel informtica avance campo figura imagen ganador auge tecnologa trar tejido empresarial dedor potenciar existir basado machine learning merecer isaac prez borrero manuel emilio gegndez aria figurar comparativo rendimiento modelo machine learning deep learning base tamao conjunto dato disponible mencin especial empresa deep mind dedicar investigacin tera deep learning tensorflow frameworks ms popular trabajar deep learning figura recoger panorama industrial actual campo ecosistema empresarial propiciar adopcin tcnica deep ning aplicación tal automatizacin industrial lucha cncer conduccin autnoma adems etctera momento ms relevant historia reciente deep learning producir alphago sistema basado tcnica deep learning desarrollado deep mind logr derrotar campen mundial go lee sedol juego complejidad rdén magnitud rior juego ajedrez considerar imposible inteligencia artificial capaz jugar correctamente juego hito recordar rrido dcada inteligencia artificial ibm deep blue logr vencer campen humano ajedrez garry kasprov deep learning trar democratizacin tcnica teligencia artificial permitir ordenador domstico dato recabado deep learning fundamento teora aplicacin figura coste almacenamiento dato frente disponibilidad fuente persona organizacin desarrollar sistema deep learning mejorar resultado tcnica tradicional an existir ferencia pequea empresa usuario empresa google facebook ltima contar mil r multitud prueba estrategia permitir encontrar mejor modelo problema realmente marcar diferencia permitir mejor resultado adems alto demanda procesamiento modelo estn producir serio problema huella carbono isaac prez borrero manuel emilio gegndez aria figurar izquierda derecha yann lecun geoffrey hinton yoshua bengio fuente deep learning fundamento teora aplicacin figura panorama industrial deep learning fuente isaac prez borrero manuel emilio gegndez aria red neuronal red neuronal convolucional convolutional neural networks cnn gls modelo red neuronal inspirado neocrtex cerebro encargado función ms compleja realizar humano concreto forma corteza visual procés informacin ao profesor david hubel torsten wiesel desarrollar trabajo experimental registrar activacin neurona corteza visual cerebro gato ver serie dibujo sorpresa encontrar har neurona activar lnea dibujar tena orientacin determinado llamar clula simple activar adems presentar orientacin determinado desplazar orientacin especfico ltima llamar clula compleja observar cerebro analizar informacin visual clula organizado estructura jerrquico clula estn prxima analizar regin cercano campo visual clula simple reaccionar caracterstica ms bsica borde lnea determinado orientacin clula compleja basar caracterstica detectado clula simple detectar caracterstica ms compleja concreto descubrir clula compleja ofrecer invarianza localizacin caracterstica gracias agrupacin realizar entrada recibido clula simple figura mostrar resumen experimento llevado cabo hubel wiesel humano corteza visual localizar posterior cerebro figura distinguir capa encargado procesar formacin visual capa extrae caracterstica basada extrada capa anterior aumentar campo receptivo regin espacio procesar neurona informacin enviado región cerebro seguir procesamiento informacin ms alto nivel ayuda toma decisión figura mostrar deep learning fundamento teora aplicacin figura experimento llevados cabo hubel wiesel fuente principal caracterstica extraer regin corteza visual procesamiento informacin recibir retina figurar localizacin corteza visual cerebro fuente trabajo hubel wiesel descrir cmo cerebro mamfero procés informacin visual extraccin caracterstica menor complejidad vali obtener premio nobel adems sentar base modelo deep learning modelo desarrollar trabajo neocognitron sirvi inspiracin red neuronal isaac prez borrero manuel emilio gegndez aria conocer kunihiko fukushima present modelo red neuronal llamado neocognitron modelo basado trabajo hubel wiesel capaz procesar imagen serie capa encargado extraer caracterstica dispuesto forma secuencial vo imitar extraccin caracterstica jerrquico cerebro capa neocognitron emplear tipo clula clula s clula clula s extraen caracterstica nivel entrada clula simple cerebro clula c extraen caracterstica alto nivel clula compleja cerebro capa clula organizar plano distribuir cuadrcula forma generar valor salida permitir mantener estructura dato forma matricial imagen entrada clula pertenecer plano extraen caracterstica entrada compartir peso analizar regin contrario red neuronal tradicional respetar estructura pacial dato imagen caso convertir entrada vector clula procés pequeo matriz pequea regin gen entrada operacin convolucin seccin permitir comparar regin actual plantilla pesos ca detectar clula figura clula c utilizar operacin agrupamiento pooling ingls normalmente media combinar salida clula s as ganar invarianza pequea traslación rotación caracterstica detectado clula figura mostrar estructura modelo principal aportación neocognitron procesado entrada operacin convolucin detectar caracterstica preservar estructura espacial dato tolerancia traslación caracterstica ltimo gracias operación pooling utilizar peso posición entrada reducir cantidad pesos ajustar principio basar red neuronal convolucional ao yann lecun equipo laboratorio bell presentar red neuronal convolucional historia llamado deep learning fundamento teora aplicacin trenado algoritmo backpropagation existir present neocognitron reconocimiento dgito escrito mano red neuronal diferencia red neuronal tradicional plear capa neurona tipo capa utilizar capa neurona extraer caracterstica dato bruto entrada primero capa normalmente convolucin pooling realizar formacin espacio dato entrada espacio caracterstica dimensin ms reducido utilizar red neuronal dicional salida red lugar utilizar directamente dato entrada distinguir etapa principal red neuronal nal etapa extraccin caracterstica etapa clasificacin etapa extraccin caracterstica capa convolucin pooling cin obtener representacin dato entrada basado creacin jerarqua caracterstica menor complejidad salida ltimo capa etapa representacin obtenido dato entrada entrada etapa clasificacin cin ms red neuronal tradicional caso red neuronal convolucional entrada directamente dato problema salida etapa ventaja enfoque entrada etapa clasificacin ms reducido caso tradicional disminuir nmero parmetro ajustar red adems representacin dato facilitar clasificacin red gracias convolucin detectar caracterstica permitir representar dato entrada combinacin presencia caracterstica dato entrada combinar red neuronal determinar clase entrada ambos etapa algoritmo backpropagation encargar ajustar peso caracterstica extraer primero capa definir forma capa neurona final etapa clasificacin poder generar salida correctamente entrada base estudio reciente observar forma emprico mero capa red neuronal convolucional extraen caracterstica nivel isaac prez borrero manuel emilio gegndez aria borde color servir entrada siguiente capa zar detectar caracterstica ms complejo coche cara guardar similitud observado primero capa corteza visual rebro avanzar capa red operacin convolucin extraer caracterstica ms compleja traducir tacin distribuido dato entrada clasificacin figura mostrar caracterstica detectar capa cin red neuronal convolucional observar cmo capa ms profundo red detectar caracterstica ms complejo rueda coche persona capa detectar caracterstica ms simple color borde esquina crculo peso red neuronal convolucional ajustar forma automtico backpropagation excelente resultado problema reconocimiento dgito mayora comunidad pens alcanzar resultado tarea ms compleja ms clase variabilidad imgén ao red neuronal convolucional alexnet utiliz competicin imagenet reducir considerablemente error modelo ao provoc popularizacin red neuronal convolucional definitiva tcnica deep learning red neuronal convolucional aplicar tud problema aplicación modificar arquitectura red neuronal convolucional aplicar adems clasificacin imgén mentacin deteccin objeto imgén transformacin dato representacin estructura espacial imgén permitir procesamiento audio texto red convolucional mejorar resultado conseguido tcnica tradicional campo aplicacin gracias definir rstica mano red definir deep learning fundamento teora aplicacin figura jerarqua caracterstica obtener cerebro informacin visual retina fuente isaac prez borrero manuel emilio gegndez aria figura clula s compartir peso extraer caracterstica región entrada fuente figurar estructura neocognitron fuente deep learning fundamento teora aplicacin figura caracterstica extrada capa red neuronal convolucional mostrar imagen producir activacin caracterstica ejemplo imgén real conjunto dato producir alto activacin fuente isaac prez borrero manuel emilio gegndez aria figura caracterstica extrada capa red neuronal convolucional izquierda mostrar imagen producir activacin caracterstica derecha ejemplo imgén real conjunto dato producir alto activacin fuente figurar caracterstica extrada capa red neuronal convolucional izquierda mostrar imagen producir activacin caracterstica derecha ejemplo imgén real conjunto dato producir alto activacin fuente deep learning fundamento teora aplicacin arquitectura arquitectura red neuronal convolucional diferir arquitectura red neuronal tradicional adems contar tipo capa estructura dato pasar papel relevante procesamiento realizar red estructura espacial dato preservar facilitar extraccin caracterstica generar salida red estructura esperado caso capa convolucin pooling aprovechar relacin espacial presentar dato extraer caracterstica caso depender objetivo red estructura dato modificar contar representacin deseado presentar novedad red neuronal entrada salida vector trabajar red neuronal tradicional neurona recibir mo entrada elemento vector entrada vector entrada dimensin nico dato entrada cero sión escalar dato dimensión caso imagen escala grís dato tacin matricial procesar red neuronal independientemente representacin original dato transformar vector caso dato dimensión imgén color ms sión procedimiento transformar dato vector dimensin ocurrir caso red neuronal convolucional respetar dimensión dato modificarla importante estructura dato momento aplicar operación correctamente forma general distinguir escalar vector matriz utilizar concepto tensor referencia dato forma escalar tensor orden cero vector tensor orden matriz tensor orden forma general dato dimensin n tensor orden figura mostrar ejemplo dato representacin denominacin base dimensin diseo arquitectura red tipo operacin tipo tensor procesar programar él correctamente isaac prez borrero manuel emilio gegndez aria figura ejemplo dato representacin denominacin base dimensin normalmente extraer caracterstica utilizar sucesión capa lucin pooling etapa transformacin utilizar capa red neuronal tradicional decidir tipo combinacin capa red neuronal convolucional requerir proceso prueba error encontrar combinacin tarea conveniente revisar literatura busca solución contrastado problema parecido resultado problema complejo analizar arquitectura ms popular deep learning fundamento teora aplicacin historia podr entender porqu estndar actualmente seguir disear arquitectura red neuronal convolucional caso disear arquitectura cero encontrar arquitectura red considerar adecuado problema literatura replicar arquitectura intentar modificar él mejorar resultado combinar tipo capa desear observar estudiar arquitectura popular soler disear él aspecto forma parecido ejemplo red neuronal intentar reducir dimensin dato evitar aumentar forma excesivo nmero parmetro red prctica estudiarn seccin siguiente sección estudiar principal capa red neuronal convolucional capa convolucin capa convolucin capa ms importante red neuronal convolucional capa permitir extraer caracterstica dato entrada capa convolucin aplicar serie convolución dato entrada poder entrada red salida capa capa convolucin buscar extraer caracterstica referencia salida entrada mapa caracterstica entrada salida segn operacin convolucin convolucin operador matemtico representar smbolo operador realizar transformacin función f g funcin f g integracin producto función invertido desplazado distancia x f f d isaac prez borrero manuel emilio gegndez aria funcin invertido desplazado denomina kernel cin intuicin detrs convolucin permitir obtener medida superposicin ambos función figura mostrar forma visual convolucin función ejemplo observar figura volucin rea curva producto función to definicin convolucin utilizado función continuo dato procesar red neuronal red neuronal convolucional discreto histrico precio bolsa activo texto nin red social imgén caso operacin convolucin discreto operacin sustituir integracin sumatorio calcular f i i figura mostrar convolucin función discreto ejemplo diferencia convolucin función continuo funcin discreto disponer valor punto muestreo funcin ejemplo ambos función dominio infinito caso trabajar red neuronal convolucional mapa caracterstica entrada kernel convolucin tamao finito elemento tambin referencia pieza informacin lugar función convolucin red neuronal convolucional representacin convolucin discreto pieza informacin tamao finito soler representar ver lugar representar forma similar representar imgén lizar disposicin cuadrcula sitar elemento mapa caracterstica kernel convolucin posicin tener respetar estructura dato figura mostrar ejemplo sentacin convolucin pieza informacin dimensin tamao limitado deep learning fundamento teora aplicacin convolucin conmutativo soler utilizar kernel convolucin pieza informacin ms pequea diferencia situación anterior convolucin aplicar pieza informacin tamao finito ocurrir resultado convolucin tamao función entrada figura resultado convolucin elemento función aplicar convolucin contar elemento respectivamente convolucin calcular posición ambos función estn definido kernel convolucin sitar completamente funcin provocar resultado convolucin tamao ms reducido funcin aplicar operacin necesario mantener tamao original pieza informacin desplazar kernel convolucin soler introducir relleno padding ingls pieza informacin resultado convolucin tamao deseado normalmente relleno consistir introducir cero extremo pieza informacin figura mostrar ejemplo convolucin discreto pieza informacin tamao finito aplicar relleno cero mantener tamao original pieza informacin salida convolucin adems modificar tamao funcin aplicar lucin ms adicin relleno tambin reducir tamao introducir concepto paso stridir ingls norma general convolucin aplicar posición funcin paso convolucin reducir tamao salida elegir paso tamao salida reducir proporcin paso elegido mapa caracterstica tamao n kernel convolucin tamao k paso convolucin p tamao salida n k p figura mostrar ejemplo convolucin discreto pieza isaac prez borrero manuel emilio gegndez aria informacin tamao finito utilizar paso caso resultado convolucin tamao n k p estudiar convolucin aplicado función mensin podrar aplicar nmero dimensión ejemplo convolucin discreto dimensión calcular f i j j i j ejemplo familiar convolucin dimensión filtrado nes caso tratar él dimensión imgén escala grís figura mostrar ejemplo convolucin dimensión sentar mapa caracterstica imagen escala grís normalizacin apreciar efecto filtro elegido entrada caso figura kernel convolucin capaz resaltar ojo figura dibujado observar mapa caracterstica resultado aplicar convolucin apreciar valor ms alto posición encontrar ojo figura dato ms dimensión caso imgén color dimensión efecto convolucin imgén ms fcil apreciar figura mostrar ejemplo resultado convolucionar kernels conocido campo procesamiento imgén imagen color observar figura segn kernel elegido desenfocar imagen detectar borde direccin direccin principal apreciación ejemplo figura tado convolucin imagen escala gris dimensión entrada kernel convolucin dimensión definicin operacin convolucin calcular posicin deep learning fundamento teora aplicacin suma elemento kernel porcin correspondiente imagen entrada resultar prdida ltima dimensin observar ejemplo anterior segn kernel elijar convolucin extraer determinado caracterstica dato entrada tivo obtener definicin kernel similar cmo obtener peso red neuronal tradicional utilizar algn algoritmo optimizacin campo deep learning denominar convolucin operacin empleado red neuronal convolucional correlacin cruzado correlation ingls operacin calcular forma similar convolucin reflejar kernel convolucin correlacin cruzado función discreto f g calcular f i i red neuronal convolucional encargada aprender valor kernels convolucin indiferente aprendar kernel reflejar indicar operacin convolucin despu reflejar caso correlacin cruzado red aprender directamente kernel reflejado multiplicar directamente entrada evitar reflejar él realmente operacin realizar capa convolucin correlacin cruzado lugar convolucin capa convolucin capa convolucin componer ms convolución caso neurona red neuronal convolución procesar entrada agrupan capa salida capa convolucin consistir concatenacin salida convolución capa tomar ejemplo figura capa convolucin componer convolución aplicar imagen entrada salida capa mapa caracterstica compuesto concatenacin imgén escala grís generar convolución capa formar nico imagen imagen canal isaac prez borrero manuel emilio gegndez aria figura mostrar informacin relativo capa cin importante notar valor relleno r referir relleno aadar mapa caracterstica entrada relleno soler dividir él mitad aplicar comienzo mitad dimensin aadir relleno relleno aplicar normalmente dimensión calcular convolucin h w principal hiperparmetro capa convolucin nmero convolución tamao kernel tamao paso cantidad relleno emplear ventaja convolucin red neuronal za traslacin caracterstica borde detectar posicin entrada emplear ms parmetro especfico kernel convolucin convolucin desplazar kernel trado calcular posicin salida permitir convolucin utilizar kernel posición red neuronal contrario posicin entrada multiplicar peso significar racterstico esquina superior izquierdo entrada procesar peso esquina inferior derecho detectar rstico ambos posición red neuronal habrar repetir valor peso ambos posición suponer ventaja red ronal convolucional red neuronal tradicional nmero parmetro entrada tamao h w c capa red ronal necesitar h w c parmetro neurona capa capa convolucin kernel tamao kh kw kc necesitar kh kw kc convolucin independientemente tamao entrada figura mostrar comparativo capa red neuronal capa red neuronal convolucional aspecto importante hora utilizar capa convolucin función activacin convolucin ver neurona tamao kernel coincidir tamao entrada caso elemento kernel multiplicar dato entrada rona tradicional tratar motivo red neuronal necesitar función activacin lineal mejorar capacidad red capa convolucin sern necesario funcin activacin deep learning fundamento teora aplicacin aplica red neuronal tradicional obtener mapa caracterstica salida elemento procesar funcin activacin valor actualizado mantener estructura dato red neuronal convolucional utilizar función activacin soler utilizar red neuronal tradicional estudiar seccin hora disear arquitectura red neuronal convolucional tendr optar red crecer profundidad aadeir ms capa anchura aadeir ms convolución capa comentar opcin habitual soler aumentar profundidad red enfoque venir motivado razón principal demostracin prctico mejora resultado red neuronal convolucional aumentar dad problema genrico competicin imagenet figurar nmero parmetro red utilizar capa tendr utilizar tamao kernel suficientemente procesar entrada ejemplo desear reconocer persona imagen pa imagen tamao kernel prcticamente imagen incremento nmero parmetro conllevar utilizar capa convolucin kernel ms pequeo cabo capa kernel recibir entrada cin proveniente imagen completo zona entrada capa convolucin procés conocer campo receptivo receptivir field ingls capa concuerdar tamao kernel interesar mantener compromiso nmero capa red campo receptivo forma controlar nmero parmetro ajustar red llegar procesar elemento inters entrada isaac prez borrero manuel emilio gegndez aria figura ejemplo grfico clculo convolucin función continuo deep learning fundamento teora aplicacin figura ejemplo grfico clculo convolucin función discreto isaac prez borrero manuel emilio gegndez aria figura representacin convolucin pieza informacin dimensin tamao limitado deep learning fundamento teora aplicacin figura representacin convolucin pieza informacin relleno isaac prez borrero manuel emilio gegndez aria figura representacin convolucin pieza informacin paso deep learning fundamento teora aplicacin figurar representacin convolucin pieza informacin dimensión isaac prez borrero manuel emilio gegndez aria figura convolución imagen dimensión canal rojo verde azul deep learning fundamento teora aplicacin figura informacin relativo capa convolucin isaac prez borrero manuel emilio gegndez aria figura comparativo capa red neuronal capa convolucin deep learning fundamento teora aplicacin figura mejora resultado aumento profundidad red error error cometido red clase esperado primero clase modelo isaac prez borrero manuel emilio gegndez aria capa pooling primero aplicación red neuronal convolucional problema clasificacin tipo problema capa convolucin permitir detectar elemento inters entrada importar posicin ocupar caso imgén ejemplo desear clasificar imgén dgito escrito mano necesitar posicin dgito imagen presencia dgito convolucin detectar caracterstica posicin caracterstica detectar variar ligeramente plo borde completamente recto pretender seguir dolo pequea variacin capa pooling permitir dotar invarianza pequea deformación caracterstica reduccin tamao dato tomar algn valor representativo regin ms contar parmetro ajustabl reducir tamao dato reducir nmero parmetro ajustar red aumentar campo receptivo capa convolucin posterior operacin pooling parecer convolucin realizar determinado operacin forma local posicin entrada tipo parmetro capa convolucin parmetro tamao ventana paso relleno tamao ventana equivalente tamao kernel convolucin capa convolucin indicar regin entrada procesar posicin paso permitir reducir tamao entrada normal utilizar paso ltimo relleno utilizar mantener tamao proporcional dato entrada salida clculo salida capa pooling depender tipo operacin elegido normalmente soler tratar él valor representativo regin analizar operación ms utilizado asigna salida posicin actual mximo valor regin analizado average pooling asigna salida posicin actual media regin analizado figura mostrar ejemplo operacin average deep learning fundamento teora aplicacin pooling entrada ejemplo observar figura temente tipo operacin utilizar posicin salida valor obtenido var mover posicin dato ventana trado permitir ganar ventana invarianza estructura espacial dato figurar ejemplo clculo average pooling caso red neuronal convolucional capa pooling procés canal mapa caracterstica individualmente concatenar finalizar salida conexión primero red neuronal convolucional seguan patrn capa consistar repetir secuencia capa convolucin pooling mapa caracterstica ltimo capa procesar red neuronal forma representacin deseado salida patrn repetitivo mapa caracterstica procesar forma isaac prez borrero manuel emilio gegndez aria cuencial capa primero mejora conseguir red ronal convolucional relacionado aumento profundidad nmero capa etapa extraccin caracterstica red centrar seguir repetir combinacin capa lucin pooling eleccin ms refinado parmetro forma ms profundo arquitectura provocar aumento nmero parmetro capa introducir adems empleo ms capa convolucin nmero convolución realizar capa soler aumentar profundidad red buscar extraer ms caracterstica dar él ms libertad modelo provocar nmero parmetro capa procesar dato capa tendr ms canal entrada procesar objetivo mejorar resultado red necesidad aumentar forma notorio nmero parmetro comenz explorar alternativa consecucin capa convolucin pooling buscar combinar ción forma mejorar desempeo red necesidad introducir ms parmetro utilizndola forma ms inteligente simple ticin operación capa capa continuacin detallar principal alternativa conexión capa red neuronal convolucional secuencial tipo conexin tradicional emplear capa lucin seguido capa pooling reducir tamao dato ganar invarianza pequea deformación procesar regin ms red aumentar nmero parmetro inception block enfoque nal mapa caracterstica entrada emplear binación operación parmetro operación distinto tamao kernel convolucin figura mostrar plo bloque inception observar cmo mapa ca procesar operación despu combinar concatenacin resultado operación nico mapa caracterstica salida nico capa convolucin tratar deep learning fundamento teora aplicacin figura ejemplo bloque inception skip connection tipo conexión surgir estudio lizado autor red resnet autor percatar aadir ms capas lograr bajar error red cometa namiento contraintuitivo red capa error esperar red usar ms capas logre error podra ocurrir llegado nmero capa necesario representacin ms compleja dato esperar red capa innecesario transformacin identidad modificar representacin obtenido capa anterior investigador comprobar red difcil aprender macin identidad optar dar él forma ms sencilla poder él necesitar skip connections proponer solucin problema figura conexin consistir sumar salida capa entrada pa x entrada f transformacin realizar capa salida capa caso necesitar transformacin isaac prez borrero manuel emilio gegndez aria identidad necesitar peso capa cero salida entrada figurar ejemplo skip connection adems corregir problema entrenar red profundo aadir entrada operacin adicin lograr contrarrestar blema vanish gradient gradiente propagar primero capa multiplicar ningn valor dense block principal inconveniente skip connections informacin capa actual est disponible forma individual mantener informacin perder dense blocks concatenar mapa caracterstica capa anterior nico mapa caracterstica figurar utilizar dense blocks red aadir informacin perder informacin estrategia desventaja deep learning fundamento teora aplicacin figura ejemplo dense block convolución capa ms parmetro ms canal entrada isaac prez borrero manuel emilio gegndez aria aplicación resultado conseguido red neuronal convolucional propiciar aplicacin multitud campo surgimiento ecosistema empresarial tcnica deep learning ecosistema sarial propiciar adopcin tcnica deep learning aplicación tal bsqueda frmaco deteccin enfermedad cin autnoma etctera figura encontrar caso ms común aplicación red neuronal convolucional campo aplicacin permitir superar resultado tcnica arte procesamiento imgén pasar procesamiento audio texto red neuronal convolucional demostrar potencialidad ejemplo aplicacin medicina agricultura permitir desarrollo multitud empresa buscar automatizar tarea campo realizar aplicar campo diverso necesario desarrollar delo capaz tratar peculiaridad problema replantear problema red neuronal convolucional abordar él resultado necesario modificar arquitectura mayora modificación basar aadir operación red aadir él componente temporal facilitar tratamiento tipo dato campo procesamiento imgén referir red neuronal convolucional convertir tcnica referencia abordar clasificacin segmentacin deteccin imgén gracias resultado figura empresa tesla red desarrollar sistema conduccin autnoma red neuronal convolucional cesar imgén tomar cmara coche detectar elemento inters persona seal trfico entorno ltimo instancia conducir vehculo figura mostrar informacin red tesla hydranet problema procesamiento imgén requerir red neuronal recurrente ejemplo describir imagen modelo deep learning fundamento teora aplicacin figurar principal caso deep learning fuente isaac prez borrero manuel emilio gegndez aria figura izquierda derecha clasificacin deteccin segmentacin fuente figurar red conduccin autnoma tesla hydranet fuente referencia red recurrente long short term memory lstm combinado red neuronal convolucional extraer rstica capaz descripcin contenido imagen mostrar figura necesidad combinar modelo lstm lograr desarrollar modelo derivado directamente red neuronal aplicar él problema elevado complejidad caso autoencoders generativir adversarial networks autoencoders encarguir transformar dato entrada tacin ms reducido permitir recuperacin representar entrada dimensin menor poder robustez representacin dato lograr invarianza multitud deep learning fundamento teora aplicacin figura red neuronal convolucional lstm usado describir imagen fuente formación ruido capaz agrupar objeto similar tacin generativir adversarial networks gan red neuronal convolucional cuyo modelo ajustar cumplir objetivo opuesto red intentar discernir entrada venir red dato conjunto dato red intentar engaar homloga gracias forma combinar red llamar red generador capaz entrada aleatorio crear dato presente conjunto dato entrenamiento modelo aplicación sorprendente mostrar figura limitar trabajar problema esttico desempeo extraer caracterstica forma automtico red neuronal emplear problema aprendizaje refuerzo entorno lado videojuego ejemplo modelo creado jugar juegos atari figura juego mesa go considerado juego inabordable travs tcnica inteligencia artificial modelo creado deepmind consigui batir campen dial go figura tambin aplicar entorno real conduccin autnoma figura manipulacin objeto brazo robtico figurar aplicación posible gracias combinacin algoritmo isaac prez borrero manuel emilio gegndez aria figura reescalado imgén fuente figurar reconstruccin coloreado imgén fuente tent modificación modelo deep learning original dato poder tratar tcnica capacidad extraer caracterstica forma automtico tcnica trasladar procesamiento lenguaje figura procesamiento audio figura combinacin ción utilizar él construir sistema complejo traduccin texto audio combinación transformación mayora caso modificar salida modelo capaz tratar problema actual ejemplo red neuronal convolucional desear clasificacin deep learning fundamento teora aplicacin figura representacin invariante objeto fuente figura transferencia estilo imagen fuente imgén modificar tarea tal segmentacin instancia segmentacin instancia permitir sificar pxel imagen base clase individuo pertenecer isaac prez borrero manuel emilio gegndez aria figura estimacin profundidad fuente figurar estimacin po él fuente enfoque adaptar red neuronal convolucional segmentacin instancia enfoque ms utilizado consistir proceso deteccin posterior etapa segmentacin individuo principal detección ejemplo ms representativo tcnica red mask presentado kaiming equipo laboratorio inteligencia artificial facebook figura encontrar deep learning fundamento teora aplicacin figura modelo basado deep learning entrenado aprendizaje refuerzo jugar juego atari fuente ejemplo segmentación instancia red isaac prez borrero manuel emilio gegndez aria figura modelo basado deep learning entrenado aprendizaje refuerzo jugar juego go fuente figurar conduccin autnomar modelo deep learning entrenado aprendizaje refuerzo fuente deep learning fundamento teora aplicacin figura manipulacin objeto modelo deep learning entrenado aprendizaje refuerzo fuente figurar procesamiento lenguaje red neuronal convolucional fuente isaac prez borrero manuel emilio gegndez aria figurar reconocimiento audio red neuronal convolucional combinado red neuronal recurrente fuente deep learning fundamento teora aplicacin figurar ejemplo segmentación instancia realizado red mask fuente isaac prez borrero manuel emilio gegndez aria arquitectura popular abordar problema perspectiva red neuronal volucional ahorrar él tiempo lugar disear arquitectura travs proceso prueba error utilizar arquitectura diseado arquitectura desear originalmente competición exigente gracias resultado conseguido competición arquitectura quedar referencia hora abordar problema red neuronal convolucional red neuronal estrategia transfer learning igualmente empleado red neuronal convolucional utilizar tectura entrenado entrenar nicamente red neuronal aprovechar extraccin caracterstica aprendido red problema utilizar él punto partida entrenamiento continuacin detallar principal arquitectura experto deep learning repercusin histrico resultado novedad introducir librera deep learning soler incluir arquitectura implementada lista red considerar red neuronal convolucional historia presentar yann lecun equipo investigacin laboratorio bell red desarrollar objetivo reconocer dgito escrito mano servicio postal estadounidense figura mostrar arquitectura red componer capa convolucin pooling encontrar intercalado salida ltimo capa pooling servir entrada red neuronal tradicional capa deep learning fundamento teora aplicacin figura arquitectura red fuente alexnet alexnet nombre red neuronal convolucional ganar competicin imagenet superar amplio margen dologa suponer comienzo popularizacin tcnica deep learning abordar mayora problema relacionado procesamiento gen red presentar alex krizhevsky tutela geoffrey ton arquitectura figura ms predecesora capa convolucin capa pooling red neuronal capa aumentar tamao red tambin aumentar nmero parmetro ajustar caso alexnet nmero parmetro ascender ms millón novedad introducir alexnet gpu namiento red reducir tiempo entrenamiento rable tambin populariz funcin activacin relu capa convolucin isaac prez borrero manuel emilio gegndez aria figura arquitectura red alexnet fuente googlenet red ganador competicin imagenet red googlenet red introducir inceptions blocks adentr aumento profundidad red neuronal convolucional mejorar figura mostrar arquitectura observar aumento nmero capa red anterior alexnet novedad introducir googlenet convolución tamao kernel reducir dimensin dato utilizar parmetro convolucin acta cuello botella obligar red aprender comprimir informacin googlenet componer capa gracias mejora introducido nmero parmetro reducir millón deep learning fundamento teora aplicacin figura arquitectura red googlenet fuente isaac prez borrero manuel emilio gegndez aria resnet red resnet populariz raz resultado guido competicin imagenet convirti ganadora ao principal aportacin skip connections adems proponer average pooling global tamao ventana tenar dato procesar ltimo mapa caracterstica as transformar él vector recoger valor canal mapa caracterstica reducir nmero pesos utilizar red neuronal generar salida tambin primero red batch normalization salida capa convolucin red present variante profundidad red mente presentar red referencia nmero bloque figura mostrar arquitectura red contar bloque red millón parmetro bajar considerado error humano torno competicin imagenet deep learning fundamento teora aplicacin figura arquitectura red fuente isaac prez borrero manuel emilio gegndez aria densenet despus llegada skip connections red densenet ganadora competicin imagenet present dense blocks preserva formacin entrada anterior red consigui mejorar resultado predecesora gracias diseo arquitectura aumentar considerablemente fundidad red versin ms red utilizar millón parmetro capa figura mostrar arquitectura red contrapartida red requerir ms capacidad procesamiento lizar ms operación arquitectura alternativo manejar mapa caracterstica tamao deep learning fundamento teora aplicacin figura arquitectura red fuente bibliografa hart nilsson and raphael formal basis for the heuristic termination of minimum cost paths ieee transactions on systems science and cybernetics pp mcculloch and pitts logical calculus of the ideas immanent in nervous activity the bulletin of mathematical biophysics pp rosenblatt  the perceptron probabilistic model for information storage and organization in the brain psychological review widrow and hoff adaptive switching circuits tech rep stanford univ ca stanford electronics labs hornik approximation capabilities of multilayer feedforward networks neural networks pp vargas and sakurai onir pixel attack for fooling deep neural networks ieee transactions on evolutionary computation pp hebb the organization of behavior neuropsychological theory wiley chapman hall glorot and bengio understanding the difficulty of training deep feedforward neural networks in proceedings of the thirteenth international conference on artificial intelligence and statistics pp zhang ren and sun delving deep into rectifiers surpassing performance on imagenet classification in proceedings of the ieee international conference on computer vision pp deep learning fundamento teora aplicacin duchi hazan and singer adaptive subgradient methods for line learning and stochastic optimization mach learn r july kingma and ba adam method for stochastic optimization arxiv preprint rumelhart hinton and williams learning tions by errors nature pp kawaguchi huang and kaelbling every local minimum value is the global minimum value of induced model in nonconvex machine ning neural computation pp choromanska henaff mathieu arous and lecun the loss surfaz of multilayer networks arxiv arxiv preprint zhang bengio hardt recht and vinyals tanding deep learning requir rethinking generalization arxiv preprint wolpert the lack of priori distinctions betwear learning rithms neural computation pp hopfield neural networks and physical systems with emergent ver computational abiliti proceedings of the national academy of scienz pp siegelmann and sontag on the computational power of neural nets journal of computer and system scienz pp hinton osindero and teh fast learning algorithm for deep belief nets neural computation pp krizhevsky sutskever and hinton imagenet classification with deep convolutional neural networks in advanz in neural information processing systems pp isaac prez borrero manuel emilio gegndez aria deng dong socher li li and imagenet hierarchical image database in ieee conference on computer vision and pattern recognition pp ieee metz turing award won by pioneer in artificial intelligence the new york times mar artificial intelligence googl alphago beats go master lee bbc mar mont and goertzel distributed decentralized and tized artificial intelligence technological forecasting and social change pp strubell ganesh and mccallum energy and policy considerations for deep learning in nlp arxiv preprint hubel and wiesel receptive fields of singlar neurón in the cats striate cortex the journal of physiology fukushimo neocognitron neural network model for mechanism of pattern recognition unaffected by shift in position biological cybernetics pp lecun bottou bengio and haffner learning applied to document recognition proceedings of the ieee pp zeiler and fergus visualizing and understanding convolutional networks in european conference on computer vision pp ger kheradpisheh ghodrati ganjtabesh and masquelier deep networks can resemble human vision in invariant object nition scientific reports zhang ren and sun deep residual learning for image recognition in proceedings of the ieee conference on computer vision and pattern recognition pp hochreiter and schmidhuber long memory neural computation pp deep learning fundamento teora aplicacin vinyals toshev bengio and erhaber show and tell ral image caption generator in proceedings of the ieee conference on computer vision and pattern recognition pp baldi autoencoders unsupervised learning and deep architectur in proceedings of icml workshop on unsupervised and transfer learning pp goodfellow mirza xu ozair courville and bengio generativir adversarial nets in advanz in neural information processing systems pp mnih kavukcuoglu silver grave antonoglou wierstra and riedmiller playing atari with deep reinforcement learning arxiv preprint silver hubert schrittwieser antonoglou lai guez lanctot sifre kumaran graepel lillicrap simonyan and hassabis general reinforcement learning algorithm that masters chess shogi and go through science pp chen seff kornhauser and xiao deepdriving learning fordance for direct perception in autonomous driving in proceedings of the ieee international conference on computer vision pp gu holly lillicrap and levine deep reinforcement ning for robotic manipulation with asynchronous updat in ieee international conference on robotics and automation icra pp ieee chen bolton and manning thorough examination of the n daily mail reading comprehension task arxiv preprint deng hinton and kingsbury new typ of deep neural work learning for speech recognition and related applications an overview in ieee international conference on acoustics speech and signal processing pp ieee isaac prez borrero manuel emilio gegndez aria weiss chorowski jaitly wu and chen models can directly translate foreign speech arxiv preprint gkioxari dollr and girshick mask in proceedings of the ieee international conference on computer vision pp szegedy liu jia sermanet reed anguelov vanhoucke and rabinovich going deeper with convolutions in proceedings of the ieee conference on computer vision and pattern recognition pp huang liu der maatar and weinberger denselyir  connected convolutional networks in proceedings of the ieee conference on computer vision and pattern recognition pp termin tar libro deep learning abril estar cuidado edicin servicio cación versidad huelva