representación aprendizaje reutilizar gradient retropropagacin roberto reyes-ochoa departamento fsica tecnolgico monterrey monterrey nl a00344331@exatec.tec.mx servar lpez-aguayo departamento fsica tecnolgico monterrey monterrey nl servando@tec.mx abstract trabajo proponer algoritmo gradient aprendizaje encontrar signicado entrada red neuronal adems proponer evaluarla orden importancia representar proceso aprendizaje travs etapa entrenamiento resultado obtenido utilizar referencia conjunto dato acerca tumor maligno benigno wisconsin referencia sirvi detectar patrn variable ms importante modelo gracias as evolucin temporal 1 introduccin red neuronal articial anns sigla ingls modelo computacional tratar emular forma cerebro aprender identicar patrón diferencia programa computacional tpico anns explcitamente programado travs serie comando utilizar conjunto parmetro cuyo valor aprendido tipo tarea estn desempear idea provino originalmente arthur samuel padre aprendizaje automtico 1 habl idea ensayo articial intelligence frontier of automation diagrama fig. 1 adaptacin idea arthur samuel forma utilizar actualmente modelo modelo computacional resultar sumamente ecaz resolver problema naturaleza lineal procesamiento imgén lenguaje sonido ejemplo red neuronal convolucional cnns arquitectura particularmente clasicar identicar patrón imgén grado reemplazar humano tarea lectura caracter escrito mano red neuronal recurrente rnns conseguir resultado impresionante momento generar texto original traducir idioma ejemplo actual ms impresionant modelo gpt-3 nvida utilizar cerca billn parmetro producir cadena texto original tema conversacin modelo ms sosticado gan generativir adversarial networks aprovechar capacidad discriminativo cnns construir red neuronal generar rostro articial realista 1.1 cmo funcionar ann unidad estructural red neuronal neurona concepto neurona provenir naturaleza biolgico cerebro naturaleza neurona recibir serie seal 1cabe recalcar tcnica aprendizaje automtico tal red bayesiana rbol decisin svms algoritmo clustering etc. preprint under review arxiv:2012.03188v1   cs.lg   6 dec 2020   figure 1 modelo red neuronal profundo dnn arquitectura formado capa neurona interconectado recibir dato entrenamiento variable independiente parmetro posteriormente utilizar parmetro calcular prediccin conjunto dato predicción comparar resultado objetivo variable dependiente actualizar funcin prdida finalmente minimizar funcin actualizar parmetro red iterativamente entrada producir seal salida funcin suma intensidad seal entrante comportamiento emular neurona articial conocer funcin activacin buscar funcin activacin cumplir caracterstica no-lineal continuamente diferenciable monotnico rango jo tradicionalmente funcin activacin excelencia har funcin sigmoidal eq 1 f(z = 1 1 + ez 1 prctica funcin relu eq 2 arrojar mejor resultado utilizar capa intermedio red neuronal profundo simplicidad funcin podra llevar yo dudar utilidad funcin cumplir no-linealiadad ms fcil procesar prctica modelo utilizar funcin convergir ms rpido solucin f(z = max 0 z 2 as activacin neurona marcar pauta activacin subsecuente neurona recibir entrada valor fig. 2 proceso repetir iterativo red neuronal capa entrada activación ms entrada red pasar capa intermedio tambin conocido capa oculto neurona recibir activación neurona capa anterior nalmente llegar capa salida activación representar predicción modelo interconexión neuronal estn acompaada parmetro peso umbral ingls conocer bia peso est asignado par neurona funcin priorizar activación capa umbral neurona capa intermedio capa salida funcin establecer punto activacin neurona fig. 3 mostrar conocer ann totalmente conectado neurona capa conexin neurona capa mencionar descripcin gura representacin grca acompaar él representacin matemtico neurona n capa salida l ec 3 xk representar conjunto entrada ejemplo entrenamiento w(l jk combinación pesos capa l j ndice recorrer neurona capa actual k capa b(l j umbral neurona j capa l f l k conjunto activación neurona k capa l. ecuacin construir aplicar f recursivo ltimo capa pasar capa l 1 l 2 llegar capa entrada 2   modelo biolgico b modelo matemtico figure 2 analogar modelo biolgico matemtico neurona b observar cmo funcin activacin f activar suma impulso p i wixi superar umbral activacin b. normalmente trmino p i wixi + b asignar variable z simplicar notacin activacin escribir f(z figure 3 ejemplo arquitectura ann totalmente conexa capa ocul- tas representacin matemtico neurona est capa salida = f(p k wkfk(p j wjfj(p i wixi + bj + bk + b i corresponder nmero neurona capa entrada j capa oculto k capa oculto yn(xk w(l jk b(l j = f l n   x k w(l1 jk f l1 k   x k w(l2 jk + b(l1 j + b(l j 3 objetivo ann modicar tipo parmetro dar entrada xk producir respuesta yn capa salida ms parecido salida esperado forma objetivo utilizar ejemplo entrenamiento optimizar funcin costo ejemplo entrenamiento serie observación x(m k y(m n y(m n representar salida observado vector entrada x(m n m indicar ejemplo entrenamiento 2 funcin costo funcin heurstico compar resultado red y(m n y(m n observado indicar modelo fig. 1 servir heurstica qu est desempear red ejemplo funcin mse mean squared error mostrar eq 4 c costo promedio evaluar m ejemplo entrenamiento n componente salida sumar m ejemplo entrenamiento promediar 2uno ventaja red neuronal mejorar considerablemente conforme incrementar nmero ejemplo entrenamiento disponible 3   c(y(m n = 1 2 m x m x n y(m n y(m n 2 4 contexto red neuronal proceso buscar parmetro optimizar funcin costo conocer entrenamiento etapa fundamental proceso propagacin retro-propagacin involucrar tomar entrada x(m k ejemplo entrenamiento x(m k y(m n propagar él travs red capa capa entrada activar neurona capa sta activar as llegar capa salida producir vector salida y(m n funcin costo recibir salida compara salida observado y(m n qu lejos est red alcanzar convergencia etapa retropropagacin backprop consistir calcular w b funcin costo forma intuitivo pensar proceso imaginar red capa iniciar capa salida proceder reversa obtenemos cmbio buscar funcin costo pasar cambio funcin activacin cambio necesario umbral terminar cambio peso concluir proceso actualizar parmetro gradient calculado ms capas simplemente reutilizar gradient llegar parmetro capa entrada proceso repetir agotar ejemplo entrenamiento fig. 1 representar aplicacin algoritmo ejemplo entrenamiento k entrada j salida acabar ejemplo entrenamiento concluir etapa entrenamiento nmero etapa entrenamiento as nmero ejemplo entrenamiento utilizado paso algoritmo retropropagacin mini-batches hiperparmetro red alterar proceso global minimizacin llamado sgd stochastic gradient descent algorithm 1 obtener gradient retropropagacin actualizar parmetro datar ejemplo entrenamiento xk yj conjunto pesos w(l jk umbral b(l j result gradient w(l jk b(l j actualizar parmetro w(l jk b(l j ejemplo xk yj yi z(l i a(l i feedforward(xi w(l jk b(l j cj yj yj)f   j(zj w(l jk cja(l1 k b(l j cj l l 1 while l > = 2 do c(l k f   k(z(l k p j c(l+1 j w(l+1 jk w(l jk c(l j a(l1 k b(l j c(l j l l 1 end hiperparmetro parmetro intrnseco modelo optimizar entrenamiento determinar potencial inicial red resolver problema optimizacin de- terminado ejemplo nmero etapa entrenamiento tamao mini-batch inicializacin parmetro preprocesamiento entrada tema dej intencionalmente momento formar conjunto hiper- parmetro arquitectura red neuronal empezar arquitectura red neuronal articial est restringido capa entrada capa salida capa intermedia ofrecer n variante 4   1.2 propuesta investigacin generalmente gradient retropropagacin slo utilizar optimizar parmetro red neuronal trabajo proponer utilizar él destacar propiedad entrada error salida propsito alcanzar entendimiento cualitativo proceso aprendizaje tipo modelo qu objetivo especco 1 simplicacin entrada identicar importancia variable modelo considerar utilidad simplicacin modelo complejo variable 2 representacin aprendizaje entender qu est aprender red neuronal conforme pasar etapa entrenamiento cmo evolucionar entendimiento entrada conforme actualizar parmetro modicar hiperparmetro 2 algoritmo gradient aprendizaje paso clave algoritmo gradient aprendizaje sumamente sencillo almacenar gradient retropropagado capa entrada entrada ser tambin parmetro red observar fig. 2 paso nal almacena gradient formar vector dimensin entrada permitir entrada mapear gradiente aprendizaje algorithm 2 obtener gradient aprendizaje retropropagacin datar ejemplo entrenamiento xk yj conjunto pesos w(l jk umbral b(l j result representación aprendizaje red neuronal conjunto entrada xi yi z(l i a(l i feedforward(xi w(l jk b(l j cj yj yj)f   j(zj w(l jk cja(l1 k b(l j cj l l 1 while l > = 2 do c(l k f   k(z(l k p j c(l+1 j w(l+1 jk w(l jk c(l j a(l1 k b(l j c(l j l l 1 end gradientek normk(absk(p j c(2 j w(2 jk gradient aprendizaje relacionar entrada red prestar atencin par detalle importante gradient estn expresado magnitud normalizar objetivo clasicar cualitativamente importancia variable responder pregunta investigacin riguroso gradient corresponder modicacin entrada mejorar desempeo red ejemplo entrenamiento tambin mini-batch ejemplo dar parmetro actual identicar lista factor ms inuyent concluir etapa entrenamiento slo posible aplicación gradient aprendizaje tambin identicar evolucin aprendizaje procedimiento simplemente comparar gradient correspondiente variable conforme transcurrir etapa entrenamiento anlisis ms granular considerar gradient correspondiente mini-batch ejemplo entrenamiento agregar granularidad observar prdida generalizacin describir 1 5   2.1 generalizacin arquitectura algoritmo gradient aprendizaje generalizar arquitectura cnn autor reportar apa saliencia 13 red deconvolucional generalizar propagacin gradient travs capa convolucional pooling 4 5 3 resultado estar usar conjunto dato resultado cancer wisconsin importado librera sklearn dataset sucientemente sencillo experimentar forma representacin aprendizaje usar retropropagacin necesidad introducir concepto red convolcional hiptesis red totalmente conexa 1 3 capa oculto podr alcanzar precisin aceptable tarea clasicacin binario modelo entrenado permitir examinar representación aprendizaje posteriormente modelo representacin encontrado podrn generalizar él obervar él ms claro red convolucional usar imgén proceso anlogo mapa saliencia 3.1 caso cancer wisconsin descripcin detallado conjunto dato proporcionado sklearn br east cancer wisconsin d i g n s t i c d t s t   datar set c h r c t r i s t i c s number of i n s t n c s 569 number of t t r i b t s 30 numeric p r d i c t i v t t r i b t s and the c l s s t t r i b t i n f r m t i n r d i s mean of d i s t n c s from c n t r to p i n t s on the p r i m t r t x t r s t n d r d d v i t i n of gray s c l valu p r i m t r area smoothness l c l v r i t i n in r d i s l n g t h s compactness p r i m t r ^2 area 1 0 c n c v i t s v r i t of concavir p r t i n s of the contour concave p i n t s number of concavir p r t i n s of the contour symmetry   f r c t l dimension c s t l i n approximation 1 the mear s t n d r d r r r and worst or l r g s t mean of the t h r l r g s t valu of t h s f t r s were computed f r each image r s l t i n g in 30 f t r s for i n s t n c f i l d 3 i s mean radius f i l d 13 i s radius f i l d 23 i s worst radius c l s s wdbcmalignant wdbcbenign 6   punto referencia considerar 0 representar tumor maligno 1 signico tumor benigno fig. 4 mostrar cuantitativo cmo relacionar variable conjunto dato figure 4 variable conjunto dato cancer wisconsin coecient correlacin evidente grupo variable correlacionar fuertemente principalmente variable estadstico media error estn evaluado variable ejemplo radio tumor acto seguido proceder construir dataset entrenamiento prueba formato facilitar entrenamiento red usar librera pytorch importante mencionar arquitectura simple red neuronal tipo feedforward requerir variable entrada estandarizar evitar variable escala adqueír importancia desproporcionado procedimiento realizar estandarizacin simplemente restar promedio variable dividir él desviacin estndar posteriormente proceder construir modelo red neuronal pensar inicializacin parmetro peso umbral capa función propagacin capa correspondiente mapeo lineal optimizacin hiperparmetro opt arquitectura completamente conexir capa oculto m   3   t m = 30 nmero entrada t = 1 nmero variable respuesta arquitectura tipo perceptrn capa oculto 3 neurona finalmente implement algoritmo 2 embebido proceso optimizacin sgd despu pasar modelo 40 etapa entrenamiento obtener precisin 100% conjunto prueba posteriormente realiz experimento utilizar gradient obtenido 7   resultado observar fig. 5 grca corresponder etapa entrenamiento impar barra corresponder componente distribucin generado gradient asignar porcentaje relevancia variable entrada 4 discusin previamente hablar acerca representación aprendizaje principalmente entorno red convolucional 4 5 2 avance traer herramienta tecnolgica utilizar intuición interpretar importancia entrada red neuronal general ejemplo captum.ai trabajo proponer cambio utilizar gradient retropropagacin objetivo principal representar aprendizaje codicado parmetro hiperparmetro red resultado obtenido mostrar convergencia gradient retropropagacin obser- vado conforme pasar etapa entrenamiento considerar limitación red neuronal articial implicar ptimo local red variable ms importante momento calcular funcin error fig. 5 logr objetivo representar evolucin representacin aprendizaje trabajo futuro abordar detalle evolucin aprendizaje red convolucional posibilidad identicar cmo aprender sección imagen conforme transcurrir periodo entrenamiento 5 cdigo cdigo python construccin modelo entrenamiento resultado so- bre dataset cncer wisconsin https://colab.research.google.com/drive/ 1btg3ih6k3cbbwvv1u84n_quckz0d3lbh?usp = sharing referenz 1 ahmed alqaraawi martin schuessler philipp wei enrico costanza and nadia berthouze evaluating saliency map explanations for convolutional neural networks user study arxiv:2002.00772 c february 2020 arxiv 2002.00772 2 karen simonyan andrea vedaldi and andrew zisserman deep insidir convolutional networks visualising image classication models and saliency maps arxiv:1312.6034 cs april 2014 arxiv 1312.6034 3 t. nathaber mundhenk barry y. chen and gerald friedland efcient saliency maps for explainable ai arxiv:1911.11293 c march 2020 arxiv 1911.11293 4 matthew d. zeiler and rob fergus visualizing and understanding convolutional networks arxiv:1311.2901 cs november 2013 arxiv 1311.2901 5 jost tobias springenberg alexey dosovitskiy thoma brox and martin riedmiller striving for simplicity the all convolutional net arxiv:1412.6806 c april 2015 arxiv 1412.6806 8   figure 5 modelo ajuste 100% conjunto prueba despus 40 etapa entre- namiento 5 variable ms importante despu periodo entrenamiento 1 worst areo 2 worst texture 3 worst radius 4 mear perimeter 5 mear texture 9