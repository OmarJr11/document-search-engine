representación aprendizaje reutilizar gradient retropropagacin roberto departamento fsica tecnolgico monterrey monterrey nl servar departamento fsica tecnolgico monterrey monterrey nl abstract trabajo proponer algoritmo gradient aprendizaje encontrar signicado entrada red neuronal adems proponer evaluarla orden importancia representar proceso aprendizaje travs etapa entrenamiento resultado obtenido utilizar referencia conjunto dato acerca tumor maligno benigno wisconsin referencia sirvi detectar patrn variable ms importante modelo gracias as evolucin temporal introduccin red neuronal articial anns sigla ingls modelo computacional tratar emular forma cerebro aprender identicar patrón diferencia programa computacional tpico anns explcitamente programado travs serie comando utilizar conjunto parmetro cuyo valor aprendido tipo tarea estn desempear idea provino originalmente arthur samuel padre aprendizaje automtico habl idea ensayo articial intelligence frontier of automation diagrama adaptacin idea arthur samuel forma utilizar actualmente modelo modelo computacional resultar sumamente ecaz resolver problema naturaleza lineal procesamiento imgén lenguaje sonido ejemplo red neuronal convolucional cnns arquitectura particularmente clasicar identicar patrón imgén grado reemplazar humano tarea lectura caracter escrito mano red neuronal recurrente rnns conseguir resultado impresionante momento generar texto original traducir idioma ejemplo actual ms impresionant modelo nvida utilizar cerca billn parmetro producir cadena texto original tema conversacin modelo ms sosticado gan generativir adversarial networks aprovechar capacidad discriminativo cnns construir red neuronal generar rostro articial realista cmo funcionar ann unidad estructural red neuronal neurona concepto neurona provenir naturaleza biolgico cerebro naturaleza neurona recibir serie seal recalcar tcnica aprendizaje automtico tal red bayesiana rbol decisin svms algoritmo clustering preprint under review dec figure modelo red neuronal profundo dnn arquitectura formado capa neurona interconectado recibir dato entrenamiento variable independiente parmetro posteriormente utilizar parmetro calcular prediccin conjunto dato predicción comparar resultado objetivo variable dependiente actualizar funcin prdida finalmente minimizar funcin actualizar parmetro red iterativamente entrada producir seal salida funcin suma intensidad seal entrante comportamiento emular neurona articial conocer funcin activacin buscar funcin activacin cumplir caracterstica continuamente diferenciable monotnico rango jo tradicionalmente funcin activacin excelencia har funcin sigmoidal eq ez prctica funcin relu eq arrojar mejor resultado utilizar capa intermedio red neuronal profundo simplicidad funcin podra llevar yo dudar utilidad funcin cumplir ms fcil procesar prctica modelo utilizar funcin convergir ms rpido solucin max z as activacin neurona marcar pauta activacin subsecuente neurona recibir entrada valor proceso repetir iterativo red neuronal capa entrada activación ms entrada red pasar capa intermedio tambin conocido capa oculto neurona recibir activación neurona capa anterior nalmente llegar capa salida activación representar predicción modelo interconexión neuronal estn acompaada parmetro peso umbral ingls conocer bia peso est asignado par neurona funcin priorizar activación capa umbral neurona capa intermedio capa salida funcin establecer punto activacin neurona mostrar conocer ann totalmente conectado neurona capa conexin neurona capa mencionar descripcin gura representacin grca acompaar él representacin matemtico neurona n capa salida l ec xk representar conjunto entrada ejemplo entrenamiento jk combinación pesos capa l j ndice recorrer neurona capa actual k capa j umbral neurona j capa l f l k conjunto activación neurona k capa ecuacin construir aplicar f recursivo ltimo capa pasar capa l l llegar capa entrada modelo biolgico b modelo matemtico figure analogar modelo biolgico matemtico neurona b observar cmo funcin activacin f activar suma impulso p i wixi superar umbral activacin normalmente trmino p i wixi b asignar variable z simplicar notacin activacin escribir figure ejemplo arquitectura ann totalmente conexa capa tas representacin matemtico neurona est capa salida k j i wixi bj bk b i corresponder nmero neurona capa entrada j capa oculto k capa oculto jk j f l n x k jk f k x k jk j j objetivo ann modicar tipo parmetro dar entrada xk producir respuesta yn capa salida ms parecido salida esperado forma objetivo utilizar ejemplo entrenamiento optimizar funcin costo ejemplo entrenamiento serie observación k n n representar salida observado vector entrada n m indicar ejemplo entrenamiento funcin costo funcin heurstico compar resultado red n n observado indicar modelo servir heurstica qu est desempear red ejemplo funcin mse mean squared error mostrar eq c costo promedio evaluar m ejemplo entrenamiento n componente salida sumar m ejemplo entrenamiento promediar ventaja red neuronal mejorar considerablemente conforme incrementar nmero ejemplo entrenamiento disponible n m x m x n n n contexto red neuronal proceso buscar parmetro optimizar funcin costo conocer entrenamiento etapa fundamental proceso propagacin involucrar tomar entrada k ejemplo entrenamiento k n propagar él travs red capa capa entrada activar neurona capa sta activar as llegar capa salida producir vector salida n funcin costo recibir salida compara salida observado n qu lejos est red alcanzar convergencia etapa retropropagacin backprop consistir calcular w b funcin costo forma intuitivo pensar proceso imaginar red capa iniciar capa salida proceder reversa obtenemos cmbio buscar funcin costo pasar cambio funcin activacin cambio necesario umbral terminar cambio peso concluir proceso actualizar parmetro gradient calculado ms capas simplemente reutilizar gradient llegar parmetro capa entrada proceso repetir agotar ejemplo entrenamiento representar aplicacin algoritmo ejemplo entrenamiento k entrada j salida acabar ejemplo entrenamiento concluir etapa entrenamiento nmero etapa entrenamiento as nmero ejemplo entrenamiento utilizado paso algoritmo retropropagacin hiperparmetro red alterar proceso global minimizacin llamado sgd stochastic gradient descent algorithm obtener gradient retropropagacin actualizar parmetro datar ejemplo entrenamiento xk yj conjunto pesos jk umbral j result gradient jk j actualizar parmetro jk j ejemplo xk yj yi i i jk j cj yj jk k j cj l l while l do k f k p j j jk jk j k j j l l end hiperparmetro parmetro intrnseco modelo optimizar entrenamiento determinar potencial inicial red resolver problema optimizacin terminado ejemplo nmero etapa entrenamiento tamao inicializacin parmetro preprocesamiento entrada tema dej intencionalmente momento formar conjunto parmetro arquitectura red neuronal empezar arquitectura red neuronal articial est restringido capa entrada capa salida capa intermedia ofrecer n variante propuesta investigacin generalmente gradient retropropagacin slo utilizar optimizar parmetro red neuronal trabajo proponer utilizar él destacar propiedad entrada error salida propsito alcanzar entendimiento cualitativo proceso aprendizaje tipo modelo qu objetivo especco simplicacin entrada identicar importancia variable modelo considerar utilidad simplicacin modelo complejo variable representacin aprendizaje entender qu est aprender red neuronal conforme pasar etapa entrenamiento cmo evolucionar entendimiento entrada conforme actualizar parmetro modicar hiperparmetro algoritmo gradient aprendizaje paso clave algoritmo gradient aprendizaje sumamente sencillo almacenar gradient retropropagado capa entrada entrada ser tambin parmetro red observar paso nal almacena gradient formar vector dimensin entrada permitir entrada mapear gradiente aprendizaje algorithm obtener gradient aprendizaje retropropagacin datar ejemplo entrenamiento xk yj conjunto pesos jk umbral j result representación aprendizaje red neuronal conjunto entrada xi yi i i jk j cj yj jk k j cj l l while l do k f k p j j jk jk j k j j l l end gradientek j j jk gradient aprendizaje relacionar entrada red prestar atencin par detalle importante gradient estn expresado magnitud normalizar objetivo clasicar cualitativamente importancia variable responder pregunta investigacin riguroso gradient corresponder modicacin entrada mejorar desempeo red ejemplo entrenamiento tambin ejemplo dar parmetro actual identicar lista factor ms inuyent concluir etapa entrenamiento slo posible aplicación gradient aprendizaje tambin identicar evolucin aprendizaje procedimiento simplemente comparar gradient correspondiente variable conforme transcurrir etapa entrenamiento anlisis ms granular considerar gradient correspondiente ejemplo entrenamiento agregar granularidad observar prdida generalizacin describir generalizacin arquitectura algoritmo gradient aprendizaje generalizar arquitectura cnn autor reportar apa saliencia red deconvolucional generalizar propagacin gradient travs capa convolucional pooling resultado estar usar conjunto dato resultado cancer wisconsin importado librera sklearn dataset sucientemente sencillo experimentar forma representacin aprendizaje usar retropropagacin necesidad introducir concepto red convolcional hiptesis red totalmente conexa capa oculto podr alcanzar precisin aceptable tarea clasicacin binario modelo entrenado permitir examinar representación aprendizaje posteriormente modelo representacin encontrado podrn generalizar él obervar él ms claro red convolucional usar imgén proceso anlogo mapa saliencia caso cancer wisconsin descripcin detallado conjunto dato proporcionado sklearn br east cancer wisconsin d i g n s t i c d t s t datar set c h r c t r i s t i c s number of i n s t n c s number of t t r i b t s numeric p r d i c t i v t t r i b t s and the c l s s t t r i b t i n f r m t i n r d i s mean of d i s t n c s from c n t r to p i n t s on the p r i m t r t x t r s t n d r d d v i t i n of gray s c l valu p r i m t r area smoothness l c l v r i t i n in r d i s l n g t h s compactness p r i m t r area c n c v i t s v r i t of concavir p r t i n s of the contour concave p i n t s number of concavir p r t i n s of the contour symmetry f r c t l dimension c s t l i n approximation the mear s t n d r d r r r and worst or l r g s t mean of the t h r l r g s t valu of t h s f t r s were computed f r each image r s l t i n g in f t r s for i n s t n c f i l d i s mean radius f i l d i s radius f i l d i s worst radius c l s s wdbcmalignant wdbcbenign punto referencia considerar representar tumor maligno signico tumor benigno mostrar cuantitativo cmo relacionar variable conjunto dato figure variable conjunto dato cancer wisconsin coecient correlacin evidente grupo variable correlacionar fuertemente principalmente variable estadstico media error estn evaluado variable ejemplo radio tumor acto seguido proceder construir dataset entrenamiento prueba formato facilitar entrenamiento red usar librera pytorch importante mencionar arquitectura simple red neuronal tipo feedforward requerir variable entrada estandarizar evitar variable escala adqueír importancia desproporcionado procedimiento realizar estandarizacin simplemente restar promedio variable dividir él desviacin estndar posteriormente proceder construir modelo red neuronal pensar inicializacin parmetro peso umbral capa función propagacin capa correspondiente mapeo lineal optimizacin hiperparmetro opt arquitectura completamente conexir capa oculto m t m nmero entrada t nmero variable respuesta arquitectura tipo perceptrn capa oculto neurona finalmente implement algoritmo embebido proceso optimizacin sgd despu pasar modelo etapa entrenamiento obtener precisin conjunto prueba posteriormente realiz experimento utilizar gradient obtenido resultado observar grca corresponder etapa entrenamiento impar barra corresponder componente distribucin generado gradient asignar porcentaje relevancia variable entrada discusin previamente hablar acerca representación aprendizaje principalmente entorno red convolucional avance traer herramienta tecnolgica utilizar intuición interpretar importancia entrada red neuronal general ejemplo trabajo proponer cambio utilizar gradient retropropagacin objetivo principal representar aprendizaje codicado parmetro hiperparmetro red resultado obtenido mostrar convergencia gradient retropropagacin vado conforme pasar etapa entrenamiento considerar limitación red neuronal articial implicar ptimo local red variable ms importante momento calcular funcin error logr objetivo representar evolucin representacin aprendizaje trabajo futuro abordar detalle evolucin aprendizaje red convolucional posibilidad identicar cmo aprender sección imagen conforme transcurrir periodo entrenamiento cdigo cdigo python construccin modelo entrenamiento resultado bre dataset cncer wisconsin sharing referenz ahmed alqaraawi martin schuessler philipp wei enrico costanza and nadia berthouze evaluating saliency map explanations for convolutional neural networks user study c february arxiv karen simonyan andrea vedaldi and andrew zisserman deep insidir convolutional networks visualising image classication models and saliency maps cs april arxiv nathaber mundhenk barry chen and gerald friedland efcient saliency maps for explainable ai c march arxiv matthew zeiler and rob fergus visualizing and understanding convolutional networks cs november arxiv jost tobias springenberg alexey dosovitskiy thoma brox and martin riedmiller striving for simplicity the all convolutional net c april arxiv figure modelo ajuste conjunto prueba despus etapa namiento variable ms importante despu periodo entrenamiento worst areo worst texture worst radius mear perimeter mear texture